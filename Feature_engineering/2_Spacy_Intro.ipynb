{"cells":[{"cell_type":"markdown","metadata":{"id":"Czf2260KKAvK"},"source":["#  <font color = 'dodgerblue'> **Objective**\n","The first step in NLP projects is to clean the text. For example we might want to remove punctuations, white spaces etc. Futher we want to break our strings into tokens. This step is required as we want to lean the vector (number) representaion of the tokens that we can use in our models. Spacy is a very useful library which can help us in text cleaning and tokeinzation. In this notebook, you will understand the basics of the spacy library.\n","\n","After completing this notebook, you will be able to\n","- Clean text using spacy\n","- Create tokens using spacy\n","- Extract Part of Speech Tags\n","- Extract Named Entities"]},{"cell_type":"markdown","metadata":{"id":"e3XVtBZC3Z5W"},"source":["#  <font color = 'dodgerblue'>**Install latest version of spaCy**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8211,"status":"ok","timestamp":1661691945196,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Ohxr8GRscIpK","outputId":"7ebe2bd6-9691-4fa6-b547-78c0508a3ab1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"]}],"source":["!pip install -U spacy"]},{"cell_type":"markdown","metadata":{"id":"vgZHQ4OwB3S4"},"source":["#  <font color = 'dodgerblue'>**Import Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzyN8ym07DCt"},"outputs":[],"source":["import spacy\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105,"status":"ok","timestamp":1661691957690,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"lDgJ4PeEU-EE","outputId":"98e06aba-da46-4125-a8d1-72cc78f20292"},"outputs":[{"name":"stdout","output_type":"stream","text":["3.4.1\n"]}],"source":["# check spaCy Verion\n","print(spacy.__version__)"]},{"cell_type":"markdown","metadata":{"id":"i6o-N-d5B-d_"},"source":["#  <font color = 'dodgerblue'>**Sample Strings**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtF6PxC6i7Kw"},"outputs":[],"source":["# Sample String - Create a sample String\n","\n","text1 = \"\"\"China's capital is Beijing. \\n\\nBeijing is where we'll go. \\n\\nLet's travel to Hong Kong from Beijing. \\\n","          \\n\\nA friend is pursuing his M.S from Beijing. \\n\\nBeijing is a cool place!!! :-P <3 #Awesome \\\n","          \\n\\nA Rolex watch costs in the range of $3000.0 - $8000.0 in USA and China. \\n\\n@tompeter I'm \\ \n","          \\n\\ngoing to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3 \\\n","          for more info see: http://www.example_beijing.com! Ten is different from 10\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hL3VOCmBZ8fe"},"outputs":[],"source":["text2  = \"\"\"China's capital is Beijing. \\n\\nA Rolex watch costs in the range of $3000.0 - $8000.0 in USA\"\"\""]},{"cell_type":"markdown","metadata":{"id":"z3cSytbc3z7p"},"source":["#  <font color = 'dodgerblue'>**White Space Tokenizers**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1661691966437,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"zI-hr_Lti7K6","outputId":"03ea9ee9-c33c-4bcf-f13f-7521f89392fa"},"outputs":[{"data":{"text/plain":["[\"China's\",\n"," 'capital',\n"," 'is',\n"," 'Beijing.',\n"," 'A',\n"," 'Rolex',\n"," 'watch',\n"," 'costs',\n"," 'in',\n"," 'the',\n"," 'range',\n"," 'of',\n"," '$3000.0',\n"," '-',\n"," '$8000.0',\n"," 'in',\n"," 'USA']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Whitespace Tokenizer splits text across whitespaces\n","text2.split()"]},{"cell_type":"markdown","metadata":{"id":"q8tGWjno5f04"},"source":["#  <font color = 'dodgerblue'>**spaCy Basics**\n","\n","**spaCy** (https://spacy.io/) is an open-source Python library that parses and \"understands\" large volumes of text. Separate models are available that cater to specific languages (English, French, German, etc.).\n","\n","Models in spaCy for English Language as of release 3.0.0\n","- **en_core_web_sm:** 11MB\n","- **en_core_web_md:** 48MB\n","- **en_core_web_lg:** 746MB\n","<br><br>\n","![picture](https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg)\n","\n","Picture Source : https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg\n","\n","The first step in spaCy is to create an `nlp` object. The `nlp` object is a instance of a model and consists of various operations like tokenizaton, tagger, parser, ner etc (see figure above). When a text is passed through the object, it goes throught these operations. When creating an object ,we can disable the operations that we do not need.\n"]},{"cell_type":"markdown","metadata":{"id":"wwzpMf35CQGz"},"source":[" ## <font color = 'dodgerblue'>**Download Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7850,"status":"ok","timestamp":1661692015097,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"V7WDFOLUCO0T","outputId":"597b6ed2-6868-4dc2-bfb2-a122709b96ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["2022-08-28 13:06:49.949860: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 25.5 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["!python -m spacy download en_core_web_sm"]},{"cell_type":"markdown","metadata":{"id":"_rbWF4oTCZW1"},"source":[" ## <font color = 'dodgerblue'>**Load Model**\n","\n","When we load the model, we can use the 'disable=' arguments to disable the components we do not need.\n","\n","In this notebook, we will work only with tokenizer and hence we will disable `tagger, ner and parser`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePemPus6CblL"},"outputs":[],"source":["nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90,"status":"ok","timestamp":1661692037880,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"bUW3_NUDnO5U","outputId":"6aff1dc7-5deb-49f0-d092-da3733994141"},"outputs":[{"name":"stdout","output_type":"stream","text":["['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"]}],"source":["print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEXllpsOnY49"},"outputs":[],"source":["disabled = nlp.select_pipes(disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91,"status":"ok","timestamp":1661692043610,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"hlOCQxxtnrw4","outputId":"4105b1de-16fe-4b5b-88f2-a8c1f13cfef1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[]\n"]}],"source":["print(nlp.pipe_names)"]},{"cell_type":"markdown","metadata":{"id":"L17v9uKVwo91"},"source":["# <font color = 'dodgerblue'>**Tokenization in spaCy**\n","A Text is tokenized in spaCy when creating the Language processing pipeline nlp() object. "]},{"cell_type":"markdown","metadata":{"id":"NSBinQlyxpAf"},"source":["![picture](https://spacy.io/tokenization-9b27c0f6fe98dcb26239eba4d3ba1f3d.svg)"]},{"cell_type":"markdown","metadata":{"id":"pRRhFhW_2Trr"},"source":["Picture Source: https://spacy.io/tokenization-9b27c0f6fe98dcb26239eba4d3ba1f3d.svg"]},{"cell_type":"markdown","metadata":{"id":"RyWTJcf6Ltg3"},"source":["The algorithm below is taken from Tokenization part from spaCy's Documentation.  \n","https://spacy.io/usage/linguistic-features#tokenization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zeLLMMNxg2bi"},"source":["- Iterate over space-separated substrings.\n","- Look for a token match. If there is a match, stop processing and keep this token.\n","- Check whether we have an explicitly defined special case for this substring. If we do, use it.\n","- Otherwise, try to consume one prefix. If we consumed a prefix, go back to #2, so that the token match and special cases always get priority.\n","- If we didn’t consume a prefix, try to consume a suffix and then go back to #2.\n","- If we can’t consume a prefix or a suffix, look for a URL match.\n","- If there’s no URL match, then look for a special case.\n","- Look for “infixes” – stuff like hyphens etc. and split the substring into tokens on all infixes.\n","- Once we can’t consume any more of the string, handle it as a single token.\n","- Make a final pass over the text to check for special cases that include spaces or that were missed due to the incremental processing of affixes.\n"]},{"cell_type":"markdown","metadata":{"id":"zcq_x_mMiNPx"},"source":[" ## <font color = 'dodgerblue'>**Create Doc Object**\n","\n","When we call nlp on a string, spaCy first tokenizes the text and creates a document object."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_V6pOrri7LV"},"outputs":[],"source":["# creating a Doc object\n","doc2 = nlp(text2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110,"status":"ok","timestamp":1661692572755,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"ZfhUAFNxi7Ld","outputId":"1d4cc3e2-9134-4607-f002-f63f53eab428"},"outputs":[{"data":{"text/plain":["China's capital is Beijing. \n","\n","A Rolex watch costs in the range of $3000.0 - $8000.0 in USA"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["doc2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91,"status":"ok","timestamp":1661692575103,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"is99sNori7Ll","outputId":"06e655ee-30f0-4496-ac81-67742443cacc"},"outputs":[{"data":{"text/plain":["spacy.tokens.doc.Doc"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# check the type of doc\n","type(doc2)"]},{"cell_type":"markdown","metadata":{"id":"farHJxJ_Dhel"},"source":["  ## <font color = 'dodgerblue'>**Accessing text of the tokens**\n","token is an object. We can acccess the text of the token using text attribute."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92,"status":"ok","timestamp":1661692576367,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"WDDbkA7wi7L9","outputId":"757b072d-4235-4921-a8c5-5a1d5613fa73","scrolled":true},"outputs":[{"data":{"text/plain":["['China',\n"," \"'s\",\n"," 'capital',\n"," 'is',\n"," 'Beijing',\n"," '.',\n"," '\\n\\n',\n"," 'A',\n"," 'Rolex',\n"," 'watch',\n"," 'costs',\n"," 'in',\n"," 'the',\n"," 'range',\n"," 'of',\n"," '$',\n"," '3000.0',\n"," '-',\n"," '$',\n"," '8000.0',\n"," 'in',\n"," 'USA']"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["[token.text for token in doc2]"]},{"cell_type":"markdown","metadata":{"id":"vljZl9sfDZfp"},"source":["  ## <font color = 'dodgerblue'>**Compare spacy tokenizer with white space tokenizer**\n","We can create tokenizer using Python split() function. In the last lecture we created tokenizer by splitting on non-alpha numeric characters. That gave us tokens separated by non-alphanumeric caharacters i.e. our tokens only have alpha numeric characters (words, numbers and underscores). We will now craete a white space tokenizer. i.e. it will split the string based on white spaces and create tokens.\n","\n","We wil compare this tokenizer with spacy's tokenizer."]},{"cell_type":"markdown","metadata":{"id":"x2wqquhMFXTD"},"source":["  ## <font color = 'dodgerblue'>**Example 1 (more and better tokens)**\n","\n","The Whitespace Tokenizer splits the words from whitespaces.\n","\n","The spaCy tokenizer splits the text into meaningful segments dependent on the language model that is used. \n","\n","It is apparent that spaCy is a better tokenizer, as it's tokens contain more than just words separated from whitespaces."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90,"status":"ok","timestamp":1661692562283,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"zwLhE_pBi7MF","outputId":"6a17222e-dcd0-4067-a466-f397697c66ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["[\"China's\", 'capital', 'is', 'Beijing.', 'Beijing', 'is', 'where', \"we'll\", 'go.', \"Let's\", 'travel', 'to', 'Hong', 'Kong', 'from', 'Beijing.', 'A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing.', 'Beijing', 'is', 'a', 'cool', 'place!!!', ':-P', '<3', '#Awesome', 'A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$3000.0', '-', '$8000.0', 'in', 'USA', 'and', 'China.', '@tompeter', \"I'm\", '\\\\', 'going', 'to', 'buy', 'a', 'Rolexxxxxxxx', 'watch!!!', ':-D', '#happiness', '#rolex', '<3', 'for', 'more', 'info', 'see:', 'http://www.example_beijing.com!', 'Ten', 'is', 'different', 'from', '10']\n","['China', \"'s\", 'capital', 'is', 'Beijing', '.', '\\n\\n', 'Beijing', 'is', 'where', 'we', \"'ll\", 'go', '.', '\\n\\n', 'Let', \"'s\", 'travel', 'to', 'Hong', 'Kong', 'from', 'Beijing', '.', '          \\n\\n', 'A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing', '.', '\\n\\n', 'Beijing', 'is', 'a', 'cool', 'place', '!', '!', '!', ':-P', '<3', '#', 'Awesome', '          \\n\\n', 'A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$', '3000.0', '-', '$', '8000.0', 'in', 'USA', 'and', 'China', '.', '\\n\\n', '@tompeter', 'I', \"'m\", '\\\\', '\\n          \\n\\n', 'going', 'to', 'buy', 'a', 'Rolexxxxxxxx', 'watch', '!', '!', '!', ':-D', '#', 'happiness', '#', 'rolex', '<3', '          ', 'for', 'more', 'info', 'see', ':', 'http://www.example_beijing.com', '!', 'Ten', 'is', 'different', 'from', '10']\n"]}],"source":["doc1 = nlp(text1)\n","# Whitespace Tokenizer\n","print(text1.split())\n","\n","# spaCy Tokenizer\n","print([token.text for token in doc1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":107,"status":"ok","timestamp":1661692197763,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"i1Wi4Ewfi7MM","outputId":"7a4c5920-7bc1-466f-9ac9-6fa7cb6ff843"},"outputs":[{"name":"stdout","output_type":"stream","text":["70\n","100\n"]}],"source":["# No. of Tokens in Whitespace Tokenizer\n","print(len(text1.split()))\n","# No. of Tokens in spaCy's Tokenizer\n","print(len([token.text for token in doc1]))"]},{"cell_type":"markdown","metadata":{"id":"t9-WzMA-FoG4"},"source":["  ## <font color = 'dodgerblue'>**Example 2**\n","You can see that spacy recognizes % sumbol and create a separate token for it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1643187849072,"user":{"displayName":"Shaannoor Mann","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02520257695567980696"},"user_tz":360},"id":"KvLyXUp7i7MU","outputId":"4aec38bf-52e9-4f80-9454-140fb225c762"},"outputs":[{"name":"stdout","output_type":"stream","text":["['There', 'is', '20%', 'probaility', 'of', 'winning', 'a', 'lottery.']\n","['There', 'is', '20', '%', 'probaility', 'of', 'winning', 'a', 'lottery', '.']\n"]}],"source":["text3 = \"There is 20% probaility of winning a lottery.\"\n","doc3 = nlp(text3)\n","# Whitespace Tokenizer\n","print(text3.split())\n","\n","# spaCy Tokenizer\n","print([token.text for token in doc3])"]},{"cell_type":"markdown","metadata":{"id":"8wNEEIhPF2wb"},"source":["  ## <font color = 'dodgerblue'>**Example 3**\n","Spacy's tokenizer recognizes that m is the unit of distance (based on sentence and creates a separate token for it. It will not split random combination of numbers and alphabets.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1643187850386,"user":{"displayName":"Shaannoor Mann","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02520257695567980696"},"user_tz":360},"id":"jsNGne55i7Mb","outputId":"19268315-b7dd-419d-fabe-a045cf4828eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["['I', 'walk', '10m', 'everyday.']\n","['I', 'walk', '10', 'm', 'everyday', '.']\n","[' ', 'What', 'is', '10o8iu']\n"]}],"source":["# Another example measuring difference between Whitespace and spaCy tokenizers\n","text4=\"I walk 10m everyday.\"\n","doc4 = nlp(text4)\n","# Whitespace Tokenizer\n","print(text4.split())\n","\n","# spaCy Tokenizer\n","print([token.text for token in doc4])\n","\n","text5 = \" What is 10o8iu\"\n","doc5 = nlp(text5)\n","# spaCy Tokenizer\n","print([token.text for token in doc5])"]},{"cell_type":"markdown","metadata":{"id":"Q3shEsBmHDsf"},"source":["  ## <font color = 'dodgerblue'>**Example 4**\n","It takes into acount special cases like C++, U.S.A"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116,"status":"ok","timestamp":1643187852036,"user":{"displayName":"Shaannoor Mann","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02520257695567980696"},"user_tz":360},"id":"xzwC7j9Si7Mi","outputId":"64e5c894-1046-4e5a-e238-8d5cd5199b67"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Some', 'good', 'programming', 'languages', 'to', 'know', 'HTML,', 'CSS,', 'JavaScript,', 'C++,', 'and', 'Node.js.']\n","['Some', 'good', 'programming', 'languages', 'to', 'know', 'HTML', ',', 'CSS', ',', 'JavaScript', ',', 'C++', ',', 'and', 'Node.js', '.']\n"]}],"source":["text5=\"Some good programming languages to know HTML, CSS, JavaScript, C++, and Node.js.\"\n","doc5 = nlp(text5)\n","# Whitespace Tokenizer\n","print(text5.split())\n","\n","# spaCy Tokenizer\n","print([token.text for token in doc5])\n"]},{"cell_type":"markdown","metadata":{"id":"8PtmgOHyHZZa"},"source":["  ## <font color = 'dodgerblue'>**Text Processing/Cleaning**\n","Spacy's tokens have attributes which can be very useful in text cleaning. https://spacy.io/api/token."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FJg3TnvY8Lw"},"outputs":[],"source":["# let us check other attributes of token class\n","att_doc1 ={'token': [token for token in doc1],\n","          'token.idx': [token.idx for token in doc1],\n","          'token.text': [token.text for token in doc1],\n","          'token.is_alpha': [token.is_alpha for token in doc1],\n","          'token.is_punct': [token.is_punct for token in doc1],\n","          'token.is_space': [token.is_space for token in doc1],\n","          'token.is_stop': [token.is_stop for token in doc1],\n","          'token.like_num': [token.like_num for token in doc1],\n","          'token.is_digit': [token.is_digit for token in doc1],\n","          'token.like_url': [token.like_url for token in doc1],\n","           'token.like_email': [token.like_url for token in doc1],\n","          \n","          }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"elapsed":107,"status":"ok","timestamp":1661692596516,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"yJwfSHax9Oc-","outputId":"7139f197-5bd7-40a3-f781-e32bb2857192"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c9cc07b2-3aad-4064-94e2-d2d9f46dbd8e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>token.idx</th>\n","      <th>token.text</th>\n","      <th>token.is_alpha</th>\n","      <th>token.is_punct</th>\n","      <th>token.is_space</th>\n","      <th>token.is_stop</th>\n","      <th>token.like_num</th>\n","      <th>token.is_digit</th>\n","      <th>token.like_url</th>\n","      <th>token.like_email</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>China</td>\n","      <td>0</td>\n","      <td>China</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>'s</td>\n","      <td>5</td>\n","      <td>'s</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>capital</td>\n","      <td>8</td>\n","      <td>capital</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>is</td>\n","      <td>16</td>\n","      <td>is</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Beijing</td>\n","      <td>19</td>\n","      <td>Beijing</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>Ten</td>\n","      <td>437</td>\n","      <td>Ten</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>is</td>\n","      <td>441</td>\n","      <td>is</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>different</td>\n","      <td>444</td>\n","      <td>different</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>from</td>\n","      <td>454</td>\n","      <td>from</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>10</td>\n","      <td>459</td>\n","      <td>10</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 11 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9cc07b2-3aad-4064-94e2-d2d9f46dbd8e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c9cc07b2-3aad-4064-94e2-d2d9f46dbd8e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c9cc07b2-3aad-4064-94e2-d2d9f46dbd8e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["        token  token.idx token.text  token.is_alpha  token.is_punct  \\\n","0       China          0      China            True           False   \n","1          's          5         's           False           False   \n","2     capital          8    capital            True           False   \n","3          is         16         is            True           False   \n","4     Beijing         19    Beijing            True           False   \n","..        ...        ...        ...             ...             ...   \n","95        Ten        437        Ten            True           False   \n","96         is        441         is            True           False   \n","97  different        444  different            True           False   \n","98       from        454       from            True           False   \n","99         10        459         10           False           False   \n","\n","    token.is_space  token.is_stop  token.like_num  token.is_digit  \\\n","0            False          False           False           False   \n","1            False           True           False           False   \n","2            False          False           False           False   \n","3            False           True           False           False   \n","4            False          False           False           False   \n","..             ...            ...             ...             ...   \n","95           False           True            True           False   \n","96           False           True           False           False   \n","97           False          False           False           False   \n","98           False           True           False           False   \n","99           False          False            True            True   \n","\n","    token.like_url  token.like_email  \n","0            False             False  \n","1            False             False  \n","2            False             False  \n","3            False             False  \n","4            False             False  \n","..             ...               ...  \n","95           False             False  \n","96           False             False  \n","97           False             False  \n","98           False             False  \n","99           False             False  \n","\n","[100 rows x 11 columns]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(att_doc1)\n"]},{"cell_type":"markdown","metadata":{"id":"-aWp36ssIEp_"},"source":["  ## <font color = 'dodgerblue'>**Extract only numbers and alphabets**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121,"status":"ok","timestamp":1661692644496,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"v2-4NG0KXZJY","outputId":"70c971c4-e04c-4da9-c30e-afc4cb92f49f"},"outputs":[{"data":{"text/plain":["['China',\n"," 'capital',\n"," 'is',\n"," 'Beijing',\n"," 'Beijing',\n"," 'is',\n"," 'where',\n"," 'we',\n"," 'go',\n"," 'Let',\n"," 'travel',\n"," 'to',\n"," 'Hong',\n"," 'Kong',\n"," 'from',\n"," 'Beijing',\n"," 'A',\n"," 'friend',\n"," 'is',\n"," 'pursuing',\n"," 'his',\n"," 'from',\n"," 'Beijing',\n"," 'Beijing',\n"," 'is',\n"," 'a',\n"," 'cool',\n"," 'place',\n"," 'Awesome',\n"," 'A',\n"," 'Rolex',\n"," 'watch',\n"," 'costs',\n"," 'in',\n"," 'the',\n"," 'range',\n"," 'of',\n"," '3000.0',\n"," '8000.0',\n"," 'in',\n"," 'USA',\n"," 'and',\n"," 'China',\n"," 'I',\n"," 'going',\n"," 'to',\n"," 'buy',\n"," 'a',\n"," 'Rolexxxxxxxx',\n"," 'watch',\n"," 'happiness',\n"," 'rolex',\n"," 'for',\n"," 'more',\n"," 'info',\n"," 'see',\n"," 'Ten',\n"," 'is',\n"," 'different',\n"," 'from',\n"," '10']"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# extract only alphabets and numbers\n","[token.text for token in doc1 if  (token.is_alpha or token.like_num)]"]},{"cell_type":"markdown","metadata":{"id":"opiBlWneISd3"},"source":["  ## <font color = 'dodgerblue'>**Remove punctuations**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109,"status":"ok","timestamp":1661692650034,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"cTTDTLQDJkJM","outputId":"2b8d8f3a-582f-4178-bb77-69f3dceb839d"},"outputs":[{"name":"stdout","output_type":"stream","text":["China's capital is Beijing. \n","\n","A Rolex watch costs in the range of $3000.0 - $8000.0 in USA\n"]}],"source":["print(text2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1661692656334,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"XXxZO6IYaEKb","outputId":"ea66d8d9-3496-4840-c463-72d071815a4e"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"China 's capital is Beijing \\n\\n A Rolex watch costs in the range of $ 3000.0 $ 8000.0 in USA\""]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["# remove punctuation\n","\" \".join([token.text for token in doc2 if  not token.is_punct])"]},{"cell_type":"markdown","metadata":{"id":"I-rO3UDQIYSB"},"source":["  ## <font color = 'dodgerblue'>**Extract/Remove URLs**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152,"status":"ok","timestamp":1661692685123,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"MBCfP-UEZLdx","outputId":"e17130f9-148c-4f0c-8dfa-a6942b89862d"},"outputs":[{"data":{"text/plain":["['https://colab.research.google.com/', 'utdallas.edu']"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["# extract urls\n","text7 = 'my urls are https://colab.research.google.com/ and utdallas.edu '\n","doc7 = nlp(text7)\n","[token.text for token in doc7 if  token.like_url]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1661692692733,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"L3Dbeq6YIlsw","outputId":"e1c1e542-479a-4c5e-ef6a-e86718a3c688"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'my urls are and'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# remove urls\n","\" \".join([token.text for token in doc7 if not token.like_url])"]},{"cell_type":"markdown","metadata":{"id":"3neuQHgiIcqI"},"source":["  ## <font color = 'dodgerblue'>**Extract/Remove emails**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97,"status":"ok","timestamp":1661692709268,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"kTzBuT95ZWLR","outputId":"4d9ba09c-e65f-4cc6-ba21-bf6f85f8f882"},"outputs":[{"data":{"text/plain":["['xyz@utdallas.edu', 'xyz@gmail.com']"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# extract emails \n","text8 = 'my email is xyz@utdallas.edu or xyz@gmail.com'\n","doc8 = nlp(text8)\n","[token.text for token in doc8 if  token.like_email]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1661692710224,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"wiKcjI8RJMiF","outputId":"da17223f-273d-4456-fcdd-8641e74bbd89"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'my email is or'"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["\" \".join([token.text for token in doc8 if not token.like_email])"]},{"cell_type":"markdown","metadata":{"id":"BWvBlKxGL0DO"},"source":["  ## <font color = 'dodgerblue'>**Stopwords**\n","\n","# Stop words\n","- Stop words are basically a set of most commonly used words in a language, for exampe, 'the', 'a', 'in', 'an' etc.\n","- The stop words do not provide any contextual meaning to the text and are therefore sometimes removed. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kLFH7IIfG_a"},"outputs":[],"source":["#  The following paragraph has been taken from https://en.wikipedia.org/wiki/Regular_expression\n","text9 = \"\"\"A regular expression (shortened as regex or regexp;[1] also referred to as rational expression[2][3]) is a sequence of characters that define a search pattern. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation. It is a technique developed in theoretical computer science and formal language theory.\n","The concept arose in the 1950s when the American mathematician Stephen Cole Kleene formalized the description of a regular language. The concept came into common use with Unix text-processing utilities. Different syntaxes for writing regular expressions have existed since the 1980s, one being the POSIX standard and another, widely used, being the Perl syntax.\n","Regular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK and in lexical analysis. Many programming languages provide regex capabilities either built-in or via libraries.\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ekf20_b7fTyl"},"outputs":[],"source":["doc9 = nlp(text9)"]},{"cell_type":"markdown","metadata":{"id":"3tb44jB1fXDC"},"source":["  ## <font color = 'dodgerblue'>**Understanding Stopwords**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxECRXBPfhPV"},"outputs":[],"source":["# create tokens using spacy\n","tokens = [token.text for token in doc9 if not token.is_punct]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"co9ue8_Efo5I"},"outputs":[],"source":["from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3HXBSlOTfiAF"},"outputs":[],"source":["# create a counter object based on tokens obtained \n","# A Counter is a class containing dict objects that is used to count hashable objects\n","# Counter contains elements as keys in a dictionary and their counts as the values for the respective keys.\n","\n","counter = Counter(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1661692745074,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Bol2sqh6fwXz","outputId":"39ac89fd-3b4a-43ca-ed07-b3857ed89c16"},"outputs":[{"name":"stdout","output_type":"stream","text":["Counter({'and': 7, 'in': 6, 'the': 6, 'or': 4, 'a': 4, 'regular': 3, 'as': 3, 'of': 3, 'search': 3, 'used': 3, 'for': 3, 'text': 3, 'regex': 2, 'is': 2, 'such': 2, 'are': 2, 'find': 2, 'replace': 2, 'language': 2, '\\n': 2, 'The': 2, 'concept': 2, 'processing': 2, 'utilities': 2, 'expressions': 2, 'being': 2, 'A': 1, 'expression': 1, 'shortened': 1, 'regexp;[1': 1, 'also': 1, 'referred': 1, 'to': 1, 'rational': 1, 'expression[2][3': 1, 'sequence': 1, 'characters': 1, 'that': 1, 'define': 1, 'pattern': 1, 'Usually': 1, 'patterns': 1, 'by': 1, 'string': 1, 'searching': 1, 'algorithms': 1, 'operations': 1, 'on': 1, 'strings': 1, 'input': 1, 'validation': 1, 'It': 1, 'technique': 1, 'developed': 1, 'theoretical': 1, 'computer': 1, 'science': 1, 'formal': 1, 'theory': 1, 'arose': 1, '1950s': 1, 'when': 1, 'American': 1, 'mathematician': 1, 'Stephen': 1, 'Cole': 1, 'Kleene': 1, 'formalized': 1, 'description': 1, 'came': 1, 'into': 1, 'common': 1, 'use': 1, 'with': 1, 'Unix': 1, 'Different': 1, 'syntaxes': 1, 'writing': 1, 'have': 1, 'existed': 1, 'since': 1, '1980s': 1, 'one': 1, 'POSIX': 1, 'standard': 1, 'another': 1, 'widely': 1, 'Perl': 1, 'syntax': 1, 'Regular': 1, 'engines': 1, 'dialogs': 1, 'word': 1, 'processors': 1, 'editors': 1, 'sed': 1, 'AWK': 1, 'lexical': 1, 'analysis': 1, 'Many': 1, 'programming': 1, 'languages': 1, 'provide': 1, 'capabilities': 1, 'either': 1, 'built': 1, 'via': 1, 'libraries': 1})\n"]}],"source":["# print counter\n","print(counter)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1661692746552,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"JZzfJeXqfw2O","outputId":"e41257ed-00be-4869-ef81-318d48cbc2cf"},"outputs":[{"data":{"text/plain":["[('and', 7),\n"," ('in', 6),\n"," ('the', 6),\n"," ('or', 4),\n"," ('a', 4),\n"," ('regular', 3),\n"," ('as', 3),\n"," ('of', 3),\n"," ('search', 3),\n"," ('used', 3)]"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["# Counter class contains class methods that provide useful info using the count of elements.\n","counter.most_common(10)"]},{"cell_type":"markdown","metadata":{"id":"L8czdjt1gB0W"},"source":["  ## <font color = 'dodgerblue'>**Stop Words with Spacy**\n","- Each model in Spacy has default list of stopwords. \n","- You can check that using model.Defaults.stop_words. \n","- You can also check whether a particular word is a stopword. \n","- Further, you can modify the default list of stopwords. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1661692759872,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"U_HB0d-wgFu9","outputId":"bb250340-5ed1-4b0c-f780-1c20e30339bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'thru', 'part', 'indeed', 'out', 'alone', 'also', 'even', 'many', 'noone', 'thus', 'everywhere', 'most', 'became', 'cannot', 'whoever', 'there', 'take', 'therein', 'between', 'during', 'become', 'make', 'itself', 'unless', 'must', 'anyway', 'amongst', 'ten', 'again', '’re', 'since', 'nothing', 'hundred', 'of', 'wherein', '’s', 'by', 'would', 'still', 'something', 'among', 'how', 'doing', 'along', 'none', 'back', 'neither', 'why', 'just', '‘s', 'hers', 'which', 'whether', 'besides', 'whereas', 'rather', 'what', 'few', 'however', 'had', 'anyone', 'n’t', 'thereupon', 'though', 'both', 'afterwards', 'ever', 'but', 'him', 'fifteen', 'whole', \"'s\", 'enough', 'give', 'beforehand', 'wherever', 'hereafter', 'been', 'whose', 'eight', 'to', 'nor', 'when', 'formerly', 'yet', 'about', 'someone', 'such', 'my', 'empty', 'name', 'eleven', 'below', 'while', 'into', \"'d\", 'go', 'at', 'upon', 'one', 'his', 'five', 'either', 'will', 'yours', 'after', 'beside', 'really', 'for', 'hereupon', 'perhaps', 'without', 'several', 'serious', 'therefore', 'twelve', 'i', 'own', 'was', 'yourselves', 'anyhow', 'per', 'across', 'former', 'thereby', 'all', 'no', \"'m\", 'latter', '’ve', 'done', 'ca', 'nine', 'anything', 'nowhere', '‘ve', 'above', 'much', 'were', 'each', 'together', 'various', 'an', 'this', 'except', 'sometime', 'against', 'mostly', 'should', 'four', '’m', 'less', 'never', 'another', 'more', 'twenty', 'show', 'these', '’ll', 'same', '’d', 'often', 'least', 'forty', 'from', 'everything', 'latterly', 'thereafter', 'them', 'n‘t', 'elsewhere', 'side', 'is', 'nobody', 'thence', 'our', 'me', 'everyone', 'then', 'only', 'onto', 'first', 'becomes', 'being', 'and', 'could', 'move', 'those', 'may', 'in', 'ours', 'have', 'toward', 'not', 'moreover', 'seem', 'always', 'who', 'six', 'whereby', 'sixty', 'very', '‘d', 'full', 'other', 'hence', 'somewhere', 'anywhere', 'further', 're', 'down', 'she', 'last', 'did', 'herein', 'call', 'whereupon', 'using', 'under', 'are', 'keep', 'see', 'throughout', \"'ll\", 'on', 'off', 'where', \"n't\", 'he', 'others', 'although', 'her', 'behind', 'they', 'amount', 'regarding', 'two', 'here', 'seemed', 'over', 'until', 'every', 'it', 'might', '‘re', 'seeming', 'as', 'three', 'now', 'before', 'themselves', 'nevertheless', 'once', 'your', 'namely', 'sometimes', 'please', 'whenever', 'around', 'because', 'or', 'else', 'the', 'does', 'so', 'am', 'used', 'due', 'has', 'becoming', 'meanwhile', '‘ll', 'within', 'almost', 'say', 'any', 'myself', 'somehow', 'next', 'can', 'made', 'herself', 'whence', 'already', 'we', 'well', \"'ve\", 'whither', 'with', 'its', 'mine', 'that', 'be', 'a', 'otherwise', 'bottom', 'hereby', 'do', 'ourselves', 'quite', 'you', 'up', \"'re\", 'himself', 'whereafter', 'whatever', 'than', 'if', 'towards', 'their', 'front', 'top', 'via', 'fifty', 'some', 'put', 'beyond', 'get', 'seems', 'us', '‘m', 'third', 'whom', 'too', 'yourself', 'through'}\n"]}],"source":["# default stopwords from the loaded model in spaCy\n","# the stopwords will change with the librray we import\n","print(nlp.Defaults.stop_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1661692764575,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Z8aRa8yqgLVU","outputId":"c8832511-75da-4e0a-99ba-4855411743a8"},"outputs":[{"data":{"text/plain":["326"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["len(nlp.Defaults.stop_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114,"status":"ok","timestamp":1661692768755,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Sjd70KhJhmc4","outputId":"fb15f2df-47df-485e-ea46-d7f97ea4b79d"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["# To check whether word regular is in default stop words\n","'regular' in nlp.Defaults.stop_words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":130,"status":"ok","timestamp":1661692774253,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"7eXMQdHphnIY","outputId":"8f8a816d-a711-4bef-ab40-46581d01ccec"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["# modify spacy's default stop words; \n","# add regular as stopwords\n","nlp.Defaults.stop_words.add('regular')\n","'regular' in nlp.Defaults.stop_words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113,"status":"ok","timestamp":1661692781461,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Lv6xl-k9hqyp","outputId":"604cbd2a-559a-465b-b977-6856300e9fd8"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["# now let us modify the default words again \n","# remove regular from default stop words\n","nlp.Defaults.stop_words.remove('regular')\n","'regular' in nlp.Defaults.stop_words"]},{"cell_type":"markdown","metadata":{"id":"lXXolZuTiOKE"},"source":["  ## <font color = 'dodgerblue'>**Remove stop words from text**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVvehQKchvQK"},"outputs":[],"source":["tokens = [ token.text for token in doc9  if not (token.is_stop or token.is_punct)]\n","text9_clean = \" \".join(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1661692810297,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"GD_5FP5OieEH","outputId":"50b745a6-51ab-4a4c-f62f-c5f21f5339bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["regular expression shortened regex regexp;[1 referred rational expression[2][3 sequence characters define search pattern Usually patterns string searching algorithms find find replace operations strings input validation technique developed theoretical computer science formal language theory \n"," concept arose 1950s American mathematician Stephen Cole Kleene formalized description regular language concept came common use Unix text processing utilities Different syntaxes writing regular expressions existed 1980s POSIX standard widely Perl syntax \n"," Regular expressions search engines search replace dialogs word processors text editors text processing utilities sed AWK lexical analysis programming languages provide regex capabilities built libraries\n"]}],"source":["print(text9_clean)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhVaOmz0i6XI"},"outputs":[],"source":["counter = Counter(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95,"status":"ok","timestamp":1661692813418,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"TkhgekZljAAi","outputId":"4f4fc6fd-48cc-429f-c220-4c4356ab70bb"},"outputs":[{"data":{"text/plain":["[('regular', 3),\n"," ('search', 3),\n"," ('text', 3),\n"," ('regex', 2),\n"," ('find', 2),\n"," ('replace', 2),\n"," ('language', 2),\n"," ('\\n', 2),\n"," ('concept', 2),\n"," ('processing', 2)]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["counter.most_common(10)"]},{"cell_type":"markdown","metadata":{"id":"wBje2fG0egJW"},"source":["  ## <font color = 'dodgerblue'> **Lammetization**"]},{"cell_type":"markdown","metadata":{"id":"OHl8EWXue9EH"},"source":["<img src =\"https://drive.google.com/uc?export=view&id=1zk5L9vyg6LlTW8nCZh-YxBOyU-IinShN\" width = 500>\n","\n","image source: https://spacy.io/models"]},{"cell_type":"markdown","metadata":{"id":"HsxNLGqyemn8"},"source":["- For Lammetization we need POS and for POS we need `['tagger', 'attribute_ruler' , tok2vec]`\n","- Hence for lammetization we can disable  `['ner', 'parser']`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":150,"status":"ok","timestamp":1661693279110,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"QBOLsAXvfrHM","outputId":"dcd08897-8ce2-4d17-c864-7685a0519404"},"outputs":[{"name":"stdout","output_type":"stream","text":["[]\n"]}],"source":["print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NNey3IQflhz"},"outputs":[],"source":["disabled.restore()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":146,"status":"ok","timestamp":1661693282926,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"VTU_-dsYftEo","outputId":"7cb3d7df-d84d-49e0-f20b-486e4926216e"},"outputs":[{"name":"stdout","output_type":"stream","text":["['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"]}],"source":["print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xhbp76CUfvJv"},"outputs":[],"source":["disabled = nlp.select_pipes(disable= ['ner', 'parser'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":147,"status":"ok","timestamp":1661693289816,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"TZk_7xfEgI1C","outputId":"c722f4eb-438b-4c3a-b677-ff65141ff72b"},"outputs":[{"name":"stdout","output_type":"stream","text":["['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']\n"]}],"source":["print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0eNtqssgcAS"},"outputs":[],"source":["# We will look at lemmatizing on a small part of the text\n","text10 =\" A regular expression also referred to as rational expression is a sequence of characters that define a search pattern\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrfZ-1-TguB6"},"outputs":[],"source":["doc10=nlp(text10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obRnprZeg9px"},"outputs":[],"source":["# Lemmatizing the text\n","lemmas= [token.lemma_ for token in doc10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118,"status":"ok","timestamp":1661693340127,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"C0c64-VihCuo","outputId":"60c1c011-7a19-40d7-f8b6-1255993716f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["lemmatized :   a regular expression also refer to as rational expression be a sequence of character that define a search pattern\n","original   :  A regular expression also referred to as rational expression is a sequence of characters that define a search pattern\n"]}],"source":["print(f'lemmatized : {\" \".join(lemmas)}')\n","print(f'original   : {text10}')"]},{"cell_type":"markdown","metadata":{"id":"LPv6PbcB6n-H"},"source":["# <font color = 'dodgerblue'>**Sentence tokenization using spacy**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiReNVEPofkT"},"outputs":[],"source":["disabled.restore()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZ6vK4u5ojLb"},"outputs":[],"source":["doc2 = nlp(text2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1661693436727,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"9v2yerQx6lHW","outputId":"50874048-b77e-4bf6-fb40-1dc42f5be7b1"},"outputs":[{"data":{"text/plain":["[\"China's capital is Beijing.\",\n"," '\\n\\n',\n"," 'A Rolex watch costs in the range of $3000.0 - $8000.0 in USA']"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["# We use doc.sents to tokenize sentences\n","sentences = [sent.text for sent in doc2.sents]\n","sentences"]},{"cell_type":"markdown","metadata":{"id":"incarAQAzYZH"},"source":["# <font color = 'dodgerblue'>**Name Entity Recognition (NER)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpJACyiXzbYE"},"outputs":[],"source":["disabled = nlp.select_pipes(disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1661693456512,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"BSeRI_eFzpIk","outputId":"73ed610c-9e64-481c-d4de-4d6b0e0e3fdd"},"outputs":[{"name":"stdout","output_type":"stream","text":["['ner']\n"]}],"source":["print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96,"status":"ok","timestamp":1661693458191,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"n-O50Pijzstk","outputId":"f261aef8-34e0-46d7-9bf5-c2b00baea5c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Entity              : Tag\n","\n","China                : GPE\n","Beijing              : GPE\n","Beijing              : GPE\n","Hong Kong            : GPE\n","Beijing              : GPE\n","Beijing              : GPE\n","Beijing              : GPE\n","Rolex                : ORG\n","$3000.0 - $          : MONEY\n","8000.0               : MONEY\n","USA                  : GPE\n","China                : GPE\n","Rolexxxxxxxx         : PRODUCT\n","Ten                  : CARDINAL\n","10                   : CARDINAL\n"]}],"source":["print(f'{\"Entity\":<20}: Tag\\n')\n","for entity in doc1.ents:\n","  print(f'{entity.text:<20} : {entity.label_}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANqtbmjU05AL"},"outputs":[],"source":["# You can use displacy to visualize the named entities\n","from spacy import displacy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":577},"executionInfo":{"elapsed":109,"status":"ok","timestamp":1661693465932,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"-OEWQiPB1Dgl","outputId":"f32fbb82-80cd-4bf1-fb06-513f60896047"},"outputs":[{"data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    China\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n","'s capital is \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Beijing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",". </br></br>\n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Beijing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," is where we'll go. </br></br>Let's travel to \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Hong Kong\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," from \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Beijing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",".           </br></br>A friend is pursuing his M.S from \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Beijing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",". </br></br>\n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Beijing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," is a cool place!!! :-P &lt;3 #Awesome           </br></br>A \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Rolex\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," watch costs in the range of \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    $3000.0 - $\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n","\n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    8000.0\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n"," in \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    USA\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," and \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    China\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",". </br></br>@tompeter I'm \\ </br>          </br></br>going to buy a \n","<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Rolexxxxxxxx\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n","</mark>\n"," watch!!! :-D #happiness #rolex &lt;3           for more info see: http://www.example_beijing.com! \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Ten\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n"," is different from \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    10\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n","</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["displacy.render(doc1,style='ent',jupyter=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdMPbtzw05pQ"},"outputs":[],"source":["# text taken from https://oilprice.com/Energy/Oil-Prices/Oil-Rally-Continues-On-Bright-US-Economic-Data.html on June23 2021.\n","# Defining String\n","text11 = \"\"\"\n","Oil prices rose early on Wednesday, driven by brighter economic prospects for the United States and continued recovery in oil demand in America and elsewhere in the world.\n","As of 9:04 a.m. EDT on Wednesday, ahead of the weekly inventory report by the U.S. Energy Information Administration (EIA), WTI Crude was up 1.04 percent at $73.61, \n","and Brent Crude traded at $75.54, up by 0.99 percent on the day.Prices found support late on Tuesday after the American Petroleum Institute (API) \n","reported a draw in crude oil inventories of 7.199 million barrels for the week ending June 18. If the EIA confirms a draw today, it would be the fifth consecutive week of crude inventory draws in the United States, where demand for fuels continues to grow.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02vppqSI1N4L"},"outputs":[],"source":["# Creating doc object\n","doc11 = nlp(text11)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":165,"status":"ok","timestamp":1661693473170,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"JczOtRPA1Tj-","outputId":"e1a88cf0-540b-4fff-9dc2-36807d8f09ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Entity                                        : Tag\n","\n","Wednesday                                     : DATE\n","the United States                             : GPE\n","America                                       : GPE\n","9:04 a.m. EDT                                 : TIME\n","Wednesday                                     : DATE\n","weekly                                        : DATE\n","the U.S. Energy Information Administration    : ORG\n","1.04 percent                                  : PERCENT\n","73.61                                         : MONEY\n","Brent Crude                                   : ORG\n","75.54                                         : MONEY\n","0.99 percent                                  : PERCENT\n","the day                                       : DATE\n","Tuesday                                       : DATE\n","the American Petroleum Institute              : ORG\n","API                                           : ORG\n","7.199 million barrels                         : QUANTITY\n","the week ending June 18                       : DATE\n","EIA                                           : ORG\n","today                                         : DATE\n","the fifth consecutive week                    : DATE\n","the United States                             : GPE\n"]}],"source":["# doc.ents give us the named entities\n","# We can use entity.text and entity.label_ to get the entities and their tags\n","print(f'{\"Entity\":<45} : Tag\\n')\n","for entity in doc11.ents:\n","  print(f'{entity.text:<45} : {entity.label_}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"elapsed":99,"status":"ok","timestamp":1661693476535,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"vDH9OBGX1Z_y","outputId":"646a5bfd-fbac-477e-8bae-e0e69c4c4727"},"outputs":[{"data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>Oil prices rose early on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Wednesday\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",", driven by brighter economic prospects for \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the United States\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," and continued recovery in oil demand in \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    America\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," and elsewhere in the world.</br>As of \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    9:04 a.m. EDT\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n","</mark>\n"," on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Wednesday\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",", ahead of the \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    weekly\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," inventory report by \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the U.S. Energy Information Administration\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," (EIA), WTI Crude was up \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    1.04 percent\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n","</mark>\n"," at $\n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    73.61\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n",", </br>and \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Brent Crude\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," traded at $\n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    75.54\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n","</mark>\n",", up by \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    0.99 percent\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n","</mark>\n"," on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the day\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",".Prices found support late on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Tuesday\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," after \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the American Petroleum Institute\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," (\n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    API\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n",") </br>reported a draw in crude oil inventories of \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    7.199 million barrels\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n","</mark>\n"," for \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the week ending June 18\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",". If the \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    EIA\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," confirms a draw \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    today\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",", it would be \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the fifth consecutive week\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," of crude inventory draws in \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the United States\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n",", where demand for fuels continues to grow.</br></div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# You can use displacy to visualize the named entities\n","displacy.render(doc11,style='ent',jupyter=True)"]},{"cell_type":"markdown","metadata":{"id":"vt2gZ_Im3KZY"},"source":["# <font color = 'dodgerblue'>**Part of Speech Tagging**\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1zk5L9vyg6LlTW8nCZh-YxBOyU-IinShN\" width = 500>\n","\n","image source: https://spacy.io/models\n","\n","- For POS we need `['tagger', 'attribute_ruler' , tok2vec]`\n","-The POS tags come from rules that map token.tag to token.pos in (see mapping here https://spacy.io/api/annotation) the attribute_ruler component.\n","- If the dependency parse is available, there are more specific rules it can apply related to AUX and VERB. \n","- The mapping is hard to do perfectly because the token.tag (PTB tags) that come from the tagger don't make an aux/verb distinction at all.\n","- Hence for POS we can disable `['lemmatizer', 'ner']`\n","\n","source: https://stackoverflow.com/questions/69313960/does-spacys-version3-1-pos-tagger-depends-on-parser\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":233,"status":"ok","timestamp":1661694113689,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"ZM83lPHy3qPI","outputId":"7218fdd6-029d-41a3-cdb6-2e1a15e2b03c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"]}],"source":["disabled.restore()\n","print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1661694115580,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"CWAQyD7v3ttn","outputId":"aa3b2cf8-eca1-4e4e-9122-dbc6f835277c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['tok2vec', 'tagger', 'parser', 'attribute_ruler']\n"]}],"source":["disabled = nlp.select_pipes(disable= ['ner','lemmatizer'])\n","print(nlp.pipe_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":129,"status":"ok","timestamp":1661694143816,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"K1sqN0-Rjkvo","outputId":"446bafe8-821c-4a48-dcd1-043d99515fea"},"outputs":[{"name":"stdout","output_type":"stream","text":["A               -> DET        -> DT        \n","regular         -> ADJ        -> JJ        \n","expression      -> NOUN       -> NN        \n","(               -> PUNCT      -> -LRB-     \n","shortened       -> VERB       -> VBN       \n","as              -> ADP        -> IN        \n","regex           -> NOUN       -> NNS       \n","or              -> CCONJ      -> CC        \n","regexp;[1       -> NOUN       -> NN        \n","]               -> PUNCT      -> -RRB-     \n","also            -> ADV        -> RB        \n","referred        -> VERB       -> VBD       \n","to              -> ADP        -> IN        \n","as              -> ADV        -> RB        \n","rational        -> ADJ        -> JJ        \n","expression[2][3 -> NOUN       -> NN        \n","]               -> PUNCT      -> -RRB-     \n",")               -> PUNCT      -> -RRB-     \n","is              -> AUX        -> VBZ       \n","a               -> DET        -> DT        \n","sequence        -> NOUN       -> NN        \n","of              -> ADP        -> IN        \n","characters      -> NOUN       -> NNS       \n","that            -> PRON       -> WDT       \n","define          -> VERB       -> VBP       \n","a               -> DET        -> DT        \n","search          -> NOUN       -> NN        \n","pattern         -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","Usually         -> ADV        -> RB        \n","such            -> ADJ        -> JJ        \n","patterns        -> NOUN       -> NNS       \n","are             -> AUX        -> VBP       \n","used            -> VERB       -> VBN       \n","by              -> ADP        -> IN        \n","string          -> NOUN       -> NN        \n","-               -> PUNCT      -> HYPH      \n","searching       -> VERB       -> VBG       \n","algorithms      -> NOUN       -> NNS       \n","for             -> ADP        -> IN        \n","\"               -> PUNCT      -> ``        \n","find            -> VERB       -> VB        \n","\"               -> PUNCT      -> ''        \n","or              -> CCONJ      -> CC        \n","\"               -> PUNCT      -> ``        \n","find            -> VERB       -> VB        \n","and             -> CCONJ      -> CC        \n","replace         -> VERB       -> VB        \n","\"               -> PUNCT      -> ''        \n","operations      -> NOUN       -> NNS       \n","on              -> ADP        -> IN        \n","strings         -> NOUN       -> NNS       \n",",               -> PUNCT      -> ,         \n","or              -> CCONJ      -> CC        \n","for             -> ADP        -> IN        \n","input           -> NOUN       -> NN        \n","validation      -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","It              -> PRON       -> PRP       \n","is              -> AUX        -> VBZ       \n","a               -> DET        -> DT        \n","technique       -> NOUN       -> NN        \n","developed       -> VERB       -> VBN       \n","in              -> ADP        -> IN        \n","theoretical     -> ADJ        -> JJ        \n","computer        -> NOUN       -> NN        \n","science         -> NOUN       -> NN        \n","and             -> CCONJ      -> CC        \n","formal          -> ADJ        -> JJ        \n","language        -> NOUN       -> NN        \n","theory          -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","\n","               -> SPACE      -> _SP       \n","The             -> DET        -> DT        \n","concept         -> NOUN       -> NN        \n","arose           -> VERB       -> VBD       \n","in              -> ADP        -> IN        \n","the             -> DET        -> DT        \n","1950s           -> NOUN       -> NNS       \n","when            -> SCONJ      -> WRB       \n","the             -> DET        -> DT        \n","American        -> ADJ        -> JJ        \n","mathematician   -> NOUN       -> NN        \n","Stephen         -> PROPN      -> NNP       \n","Cole            -> PROPN      -> NNP       \n","Kleene          -> PROPN      -> NNP       \n","formalized      -> VERB       -> VBD       \n","the             -> DET        -> DT        \n","description     -> NOUN       -> NN        \n","of              -> ADP        -> IN        \n","a               -> DET        -> DT        \n","regular         -> ADJ        -> JJ        \n","language        -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","The             -> DET        -> DT        \n","concept         -> NOUN       -> NN        \n","came            -> VERB       -> VBD       \n","into            -> ADP        -> IN        \n","common          -> ADJ        -> JJ        \n","use             -> NOUN       -> NN        \n","with            -> ADP        -> IN        \n","Unix            -> PROPN      -> NNP       \n","text            -> NOUN       -> NN        \n","-               -> PUNCT      -> HYPH      \n","processing      -> VERB       -> VBG       \n","utilities       -> NOUN       -> NNS       \n",".               -> PUNCT      -> .         \n","Different       -> ADJ        -> JJ        \n","syntaxes        -> NOUN       -> NNS       \n","for             -> ADP        -> IN        \n","writing         -> VERB       -> VBG       \n","regular         -> ADJ        -> JJ        \n","expressions     -> NOUN       -> NNS       \n","have            -> AUX        -> VBP       \n","existed         -> VERB       -> VBN       \n","since           -> SCONJ      -> IN        \n","the             -> DET        -> DT        \n","1980s           -> NUM        -> CD        \n",",               -> PUNCT      -> ,         \n","one             -> NUM        -> CD        \n","being           -> AUX        -> VBG       \n","the             -> DET        -> DT        \n","POSIX           -> PROPN      -> NNP       \n","standard        -> NOUN       -> NN        \n","and             -> CCONJ      -> CC        \n","another         -> PRON       -> DT        \n",",               -> PUNCT      -> ,         \n","widely          -> ADV        -> RB        \n","used            -> VERB       -> VBN       \n",",               -> PUNCT      -> ,         \n","being           -> AUX        -> VBG       \n","the             -> DET        -> DT        \n","Perl            -> PROPN      -> NNP       \n","syntax          -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","\n","               -> SPACE      -> _SP       \n","Regular         -> ADJ        -> JJ        \n","expressions     -> NOUN       -> NNS       \n","are             -> AUX        -> VBP       \n","used            -> VERB       -> VBN       \n","in              -> ADP        -> IN        \n","search          -> NOUN       -> NN        \n","engines         -> NOUN       -> NNS       \n",",               -> PUNCT      -> ,         \n","search          -> VERB       -> VB        \n","and             -> CCONJ      -> CC        \n","replace         -> VERB       -> VB        \n","dialogs         -> NOUN       -> NNS       \n","of              -> ADP        -> IN        \n","word            -> NOUN       -> NN        \n","processors      -> NOUN       -> NNS       \n","and             -> CCONJ      -> CC        \n","text            -> NOUN       -> NN        \n","editors         -> NOUN       -> NNS       \n",",               -> PUNCT      -> ,         \n","in              -> ADP        -> IN        \n","text            -> NOUN       -> NN        \n","processing      -> NOUN       -> NN        \n","utilities       -> NOUN       -> NNS       \n","such            -> ADJ        -> JJ        \n","as              -> ADP        -> IN        \n","sed             -> ADJ        -> JJ        \n","and             -> CCONJ      -> CC        \n","AWK             -> PROPN      -> NNP       \n","and             -> CCONJ      -> CC        \n","in              -> ADP        -> IN        \n","lexical         -> ADJ        -> JJ        \n","analysis        -> NOUN       -> NN        \n",".               -> PUNCT      -> .         \n","Many            -> ADJ        -> JJ        \n","programming     -> NOUN       -> NN        \n","languages       -> NOUN       -> NNS       \n","provide         -> VERB       -> VBP       \n","regex           -> NOUN       -> NNS       \n","capabilities    -> NOUN       -> NNS       \n","either          -> CCONJ      -> CC        \n","built           -> VERB       -> VBN       \n","-               -> PUNCT      -> HYPH      \n","in              -> NOUN       -> NN        \n","or              -> CCONJ      -> CC        \n","via             -> ADP        -> IN        \n","libraries       -> NOUN       -> NNS       \n",".               -> PUNCT      -> .         \n"]}],"source":["# Get Part of Speech (POS) tags \n","# print token text, pos and tag\n","doc9 = nlp(text9)\n","for token in doc9:\n","    print(f'{token.text:<15} -> {token.pos_:<10} -> {token.tag_:<10}')"]},{"cell_type":"markdown","metadata":{"id":"q9mNHiGJt-Dh"},"source":["The list of pos_ attributes along with its meaning:\n","\n","* ADJ: adjective, e.g. old, green, first, etc.\n","* ADP: adposition, e.g. in, to, during, etc.\n","* ADV: adverb, e.g. very, tomorrow, down, where, there, etc.\n","* AUX: auxiliary, e.g. is, has (done), will (do), should (do), etc.\n","* CONJ: conjunction, e.g. and, or, but, etc.\n","* CCONJ: coordinating conjunction, e.g. and, or, but, etc.\n","* DET: determiner, e.g. a, an, the, etc.\n","* INTJ: interjection, e.g. psst, ouch, bravo, hello, etc.\n","* NOUN: noun, e.g. girl, cat, tree, air, etc.\n","* NUM: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV, etc.\n","* PART: particle, e.g. ’s, not, etc.\n","* PRON: pronoun, e.g I, you, he, she, myself, themselves, somebody, etc.\n","* PROPN: proper noun, e.g. Mary, John, Chucago, NATO, etc.\n","* PUNCT: punctuation, e.g. ., (, ), ?, etc.\n","* SCONJ: subordinating conjunction, e.g. if, while, that, etc.\n","* SYM: symbol, e.g. $, %, §, ©, +, −, ×, ÷, =, :), 😝, etc.\n","* VERB: verb, e.g. run, runs, running, ate, eating, etc.\n","* X: other, e.g. sfpksdpsxmsa(some random text).\n","* SPACE: space."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gz1Mr0MEjkwL"},"outputs":[],"source":["# We can get any of the above Part of Speech\n","# For a list of the Parts of Speech click the link here\n","# https://spacy.io/usage/linguistic-features\n","# Let us get Verbs , Nouns and Proper Nouns\n","\n","Verbs = [token.text for token in doc9 if(token.pos_=='VERB')]\n","Nouns = [token.text for token in doc9 if(token.pos_=='NOUN')]\n","Proper_Nouns = [token.text for token in doc9 if(token.pos_=='PROPN')]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114,"status":"ok","timestamp":1661694174258,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"gM6Foq6-jkwS","outputId":"eb44c40e-8440-4cd6-df15-315ddc03f7a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["shortened\n","referred\n","define\n","used\n","searching\n","find\n","find\n","replace\n","developed\n","arose\n"]}],"source":["# Print Verbs\n","for verb in Verbs[:10]:\n","  print(verb)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1661694176958,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"BDWs4ypDxwKq","outputId":"244599ca-9d3e-4adb-944d-7aec457bab1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["expression\n","regex\n","regexp;[1\n","expression[2][3\n","sequence\n","characters\n","search\n","pattern\n","patterns\n","string\n"]}],"source":["# Print Nouns\n","for noun in Nouns[:10]:\n","  print(noun)"]},{"cell_type":"markdown","metadata":{"id":"ummJZrvx0lKU"},"source":["# <font color = 'dodgerblue'>**Stemming**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3Ch1Yais_jr"},"outputs":[],"source":["!pip install -U nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-28T23:38:09.490944Z","iopub.status.busy":"2022-08-28T23:38:09.490415Z","iopub.status.idle":"2022-08-28T23:38:09.951854Z","shell.execute_reply":"2022-08-28T23:38:09.951303Z","shell.execute_reply.started":"2022-08-28T23:38:09.490902Z"},"id":"BjfG4wSds_jr"},"outputs":[],"source":["# Import PorterStemmer from nltk.stem\n","from nltk.stem import PorterStemmer"]},{"cell_type":"markdown","metadata":{"id":"6fYOHH7hVngD"},"source":["## <font color = 'dodgerblue'>**Example**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-08-28T23:39:24.130677Z","iopub.status.busy":"2022-08-28T23:39:24.130365Z","iopub.status.idle":"2022-08-28T23:39:24.134007Z","shell.execute_reply":"2022-08-28T23:39:24.133565Z","shell.execute_reply.started":"2022-08-28T23:39:24.130663Z"},"executionInfo":{"elapsed":131,"status":"ok","timestamp":1643766705557,"user":{"displayName":"Shaannoor Mann","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02520257695567980696"},"user_tz":360},"id":"juQm06FateFv","outputId":"ee86872d-2517-48b1-d1d0-85e8934acc13"},"outputs":[{"name":"stdout","output_type":"stream","text":["connection  :  connect\n","connected  :  connect\n","connnecter  :  connnect\n","connnecting  :  connnect\n","connect  :  connect\n"]}],"source":["# Create an object of class PorterStemmer\n","stemmer = PorterStemmer()\n","\n","words = ['connection', 'connected', 'connnecter', 'connnecting', 'connect']\n","\n","for w in words:\n","  print(w, \" : \", stemmer.stem(w))"]},{"cell_type":"markdown","metadata":{"id":"MOj1X0pks_jr"},"source":["# <font color = 'dodgerblue'>**Remove HTML Tags**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:26:02.857594Z","iopub.status.busy":"2022-08-29T02:26:02.857358Z","iopub.status.idle":"2022-08-29T02:26:02.860337Z","shell.execute_reply":"2022-08-29T02:26:02.859895Z","shell.execute_reply.started":"2022-08-29T02:26:02.857580Z"},"tags":[],"id":"FtOkdqESs_jr"},"outputs":[],"source":["text3=\"\"\"I just can't understand the negative comments about this film. Yes it is a typical\n","boy-meets-girl romance but it is done with such flair and polish that the time just flies by. \n","Henstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever \n","(who says the golden age of cinema is dead?) and Vartan holds his own.<br /><br />There is \n","simmering chemistry between the two leads; the film is most alive when they share a scene - \n","lots! It is done so well that you find yourself willing them to get together...<br /><br />Ignore \n","the negative comments - if you are feeling a bit blue, watch this flick, you will feel so much \n","better. If you are already happy, then you will be euphoric.<br /><br />(PS: I am 33, Male, \n","from the UK and a hopeless romantic still searching for his Princess...)\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:29:47.357070Z","iopub.status.busy":"2022-08-29T02:29:47.356872Z","iopub.status.idle":"2022-08-29T02:29:47.359439Z","shell.execute_reply":"2022-08-29T02:29:47.359124Z","shell.execute_reply.started":"2022-08-29T02:29:47.357057Z"},"tags":[],"id":"-PSmWoQ-s_js"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:26:44.406623Z","iopub.status.busy":"2022-08-29T02:26:44.406265Z","iopub.status.idle":"2022-08-29T02:26:44.409549Z","shell.execute_reply":"2022-08-29T02:26:44.409060Z","shell.execute_reply.started":"2022-08-29T02:26:44.406608Z"},"tags":[],"id":"4lZnq5Mus_js"},"outputs":[],"source":["soup = BeautifulSoup(text3, \"html.parser\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:27:06.185972Z","iopub.status.busy":"2022-08-29T02:27:06.185740Z","iopub.status.idle":"2022-08-29T02:27:06.188519Z","shell.execute_reply":"2022-08-29T02:27:06.188157Z","shell.execute_reply.started":"2022-08-29T02:27:06.185958Z"},"id":"_eIS5dLws_js"},"outputs":[],"source":["cleaned_text3 = soup.get_text()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:27:14.927358Z","iopub.status.busy":"2022-08-29T02:27:14.926810Z","iopub.status.idle":"2022-08-29T02:27:14.932890Z","shell.execute_reply":"2022-08-29T02:27:14.932563Z","shell.execute_reply.started":"2022-08-29T02:27:14.927343Z"},"tags":[],"id":"8AQKgKKEs_js","outputId":"deb20a2d-6934-4b6f-c216-42ff09f4451a"},"outputs":[{"data":{"text/plain":["\"I just can't understand the negative comments about this film. Yes it is a typical\\nboy-meets-girl romance but it is done with such flair and polish that the time just flies by. \\nHenstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever \\n(who says the golden age of cinema is dead?) and Vartan holds his own.There is \\nsimmering chemistry between the two leads; the film is most alive when they share a scene - \\nlots! It is done so well that you find yourself willing them to get together...Ignore \\nthe negative comments - if you are feeling a bit blue, watch this flick, you will feel so much \\nbetter. If you are already happy, then you will be euphoric.(PS: I am 33, Male, \\nfrom the UK and a hopeless romantic still searching for his Princess...)\""]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["cleaned_text3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:29:49.988862Z","iopub.status.busy":"2022-08-29T02:29:49.988652Z","iopub.status.idle":"2022-08-29T02:29:49.991577Z","shell.execute_reply":"2022-08-29T02:29:49.991242Z","shell.execute_reply.started":"2022-08-29T02:29:49.988849Z"},"tags":[],"id":"7JQoHQiVs_js"},"outputs":[],"source":["def basic_clean(text):\n","\n","    '''\n","    This fuction removes HTML tags from text\n","    '''\n","    if (bool(BeautifulSoup(text, \"html.parser\").find())==True):         \n","        soup = BeautifulSoup(text, \"html.parser\")\n","        text = soup.get_text()\n","    else:\n","        pass\n","    return re.sub(r'[\\n\\r]',' ', text) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:29:50.482046Z","iopub.status.busy":"2022-08-29T02:29:50.481790Z","iopub.status.idle":"2022-08-29T02:29:50.484993Z","shell.execute_reply":"2022-08-29T02:29:50.484508Z","shell.execute_reply.started":"2022-08-29T02:29:50.482031Z"},"tags":[],"id":"kYaWgMP-s_js"},"outputs":[],"source":["cleaned_text = basic_clean(text=text3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-29T02:29:56.702834Z","iopub.status.busy":"2022-08-29T02:29:56.702641Z","iopub.status.idle":"2022-08-29T02:29:56.706111Z","shell.execute_reply":"2022-08-29T02:29:56.705709Z","shell.execute_reply.started":"2022-08-29T02:29:56.702821Z"},"id":"TGyBFp_2s_js","outputId":"af7df510-d5f8-463d-cfb8-b42f76ad99cc"},"outputs":[{"data":{"text/plain":["\"I just can't understand the negative comments about this film. Yes it is a typical boy-meets-girl romance but it is done with such flair and polish that the time just flies by.  Henstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever  (who says the golden age of cinema is dead?) and Vartan holds his own.There is  simmering chemistry between the two leads; the film is most alive when they share a scene -  lots! It is done so well that you find yourself willing them to get together...Ignore  the negative comments - if you are feeling a bit blue, watch this flick, you will feel so much  better. If you are already happy, then you will be euphoric.(PS: I am 33, Male,  from the UK and a hopeless romantic still searching for his Princess...)\""]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["cleaned_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCqEgD-Xs_js"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"2_Spacy_Intro.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}