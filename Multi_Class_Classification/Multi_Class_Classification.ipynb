{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hl5QWUHjxFeq"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "else:\n",
        "  print('Not running on CoLab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRdBmO-WxKql",
        "outputId": "4778c74d-49df-4b1c-8190-96fae4b02330"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on CoLab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install wandb and update it to the latest version\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    !pip install wandb --upgrade -q"
      ],
      "metadata": {
        "id": "SOEPN3vJxKtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f686bf1-b56e-4b65-a6b3-5ce17180ea8b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 47.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 36.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 914 kB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 18.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 65.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 41.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 30.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 62.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 40.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 45.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 52.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 43.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 44.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 49.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 24.8 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nl1eoMvxKvs",
        "outputId": "aa0b3e6f-21cd-4421-8c15-2662073e45fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.vocab import  vocab\n",
        "\n",
        "import random\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from collections import Counter\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb"
      ],
      "metadata": {
        "id": "o-yeXYtZxKyW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder = Path('/content/drive/MyDrive/NLP_Fall22/HW5')\n",
        "data_folder = base_folder/'Data'\n",
        "model_folder = base_folder/'Models'\n",
        "custom_functions = base_folder/'Scripts/custom_function'"
      ],
      "metadata": {
        "id": "HNmrCGM9xK1t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(str(custom_functions))"
      ],
      "metadata": {
        "id": "5ZbAuC6ExK4N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCYV2NMrxK6o",
        "outputId": "1c13dce1-90a3-4254-93c8-d4eede8decb8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython',\n",
              " '/content/drive/MyDrive/NLP_Fall22/HW5/Scripts/custom_function']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to W&B\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "n929zVNuxK9i",
        "outputId": "446f67ea-6291-49f7-c5d4-29c57e92ca73"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import custom_preprocessor as cp"
      ],
      "metadata": {
        "id": "50Dttsn2xLAy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(data_folder/'multiclass_hw_cleaned.csv')"
      ],
      "metadata": {
        "id": "6zoqKcgbZOve"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fp4FNmumZiwv",
        "outputId": "5c5fe52c-f704-46a0-e9c0-6216dc690fe5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                              Title  \\\n",
              "0           0            detail disclosure indicator on UIButton   \n",
              "1           1           hello world fails to show up in emulator   \n",
              "2           2  Why is JSHint throwing a \"possible strict viol...   \n",
              "3           3       Programmatically Make Bound Column Invisible   \n",
              "4           4  More than one EditText - not getting focus, no...   \n",
              "\n",
              "                                                Body  \\\n",
              "0  <p>Is there a simple way to place a detail dis...   \n",
              "1  <p>I followed Hello World tutorial exactly.  E...   \n",
              "2  <p>Trying to validate some Javascript in JsHin...   \n",
              "3  <p>I'm trying to make a data bound column invi...   \n",
              "4  <p>The home screen of my Android application h...   \n",
              "\n",
              "                                        cleaned_text        Tags  \\\n",
              "0  detail disclosure indicator uibutton simple wa...      iphone   \n",
              "1  hello world fail emulator follow hello world t...     android   \n",
              "2  jshint throw possible strict violation line tr...  javascript   \n",
              "3  programmatically bound column invisible try da...     asp.net   \n",
              "4  edittext get focus soft keyboard android home ...     android   \n",
              "\n",
              "   Tag_Number_final  \n",
              "0                 8  \n",
              "1                 4  \n",
              "2                 3  \n",
              "3                 9  \n",
              "4                 4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f806e712-1150-4d37-ae5e-6dc66ae1b1df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Title</th>\n",
              "      <th>Body</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Tag_Number_final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>detail disclosure indicator on UIButton</td>\n",
              "      <td>&lt;p&gt;Is there a simple way to place a detail dis...</td>\n",
              "      <td>detail disclosure indicator uibutton simple wa...</td>\n",
              "      <td>iphone</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>hello world fails to show up in emulator</td>\n",
              "      <td>&lt;p&gt;I followed Hello World tutorial exactly.  E...</td>\n",
              "      <td>hello world fail emulator follow hello world t...</td>\n",
              "      <td>android</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why is JSHint throwing a \"possible strict viol...</td>\n",
              "      <td>&lt;p&gt;Trying to validate some Javascript in JsHin...</td>\n",
              "      <td>jshint throw possible strict violation line tr...</td>\n",
              "      <td>javascript</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Programmatically Make Bound Column Invisible</td>\n",
              "      <td>&lt;p&gt;I'm trying to make a data bound column invi...</td>\n",
              "      <td>programmatically bound column invisible try da...</td>\n",
              "      <td>asp.net</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>More than one EditText - not getting focus, no...</td>\n",
              "      <td>&lt;p&gt;The home screen of my Android application h...</td>\n",
              "      <td>edittext get focus soft keyboard android home ...</td>\n",
              "      <td>android</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f806e712-1150-4d37-ae5e-6dc66ae1b1df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f806e712-1150-4d37-ae5e-6dc66ae1b1df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f806e712-1150-4d37-ae5e-6dc66ae1b1df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['cleaned_text', 'Tag_Number_final']]"
      ],
      "metadata": {
        "id": "CfW8mNU2aOV1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "24YnOcx1aiZy",
        "outputId": "70044d1f-ec69-40f4-d939-5dd447f9307b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        cleaned_text  Tag_Number_final\n",
              "0  detail disclosure indicator uibutton simple wa...                 8\n",
              "1  hello world fail emulator follow hello world t...                 4\n",
              "2  jshint throw possible strict violation line tr...                 3\n",
              "3  programmatically bound column invisible try da...                 9\n",
              "4  edittext get focus soft keyboard android home ...                 4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59b9fac2-aa85-44fe-a147-ee802b94e326\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>Tag_Number_final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>detail disclosure indicator uibutton simple wa...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hello world fail emulator follow hello world t...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jshint throw possible strict violation line tr...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>programmatically bound column invisible try da...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>edittext get focus soft keyboard android home ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59b9fac2-aa85-44fe-a147-ee802b94e326')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59b9fac2-aa85-44fe-a147-ee802b94e326 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59b9fac2-aa85-44fe-a147-ee802b94e326');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = df['cleaned_text'].values, df['Tag_Number_final'].values\n",
        "X_train, X_t, y_train, y_t = train_test_split(X, y, test_size = 0.20, random_state=42)"
      ],
      "metadata": {
        "id": "_JOcF5VmxLD0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, X_valid, y_test, y_valid = train_test_split(X_t, y_t, test_size = 0.50, random_state=42)"
      ],
      "metadata": {
        "id": "g-IiPCzlxLGi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"IMDB dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = np.array(X)\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        text = self.X[idx]\n",
        "        labels = self.y[idx]\n",
        "        sample = (text, labels)\n",
        "        \n",
        "        return sample"
      ],
      "metadata": {
        "id": "oHyFhiZNxLJS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = CustomDataset(X_train,y_train)\n",
        "validset = CustomDataset(X_valid,y_valid)\n",
        "testset = CustomDataset(X_test,y_test)"
      ],
      "metadata": {
        "id": "dqrMD0VpxLL1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab(dataset, min_freq):\n",
        "  counter = Counter()\n",
        "  for (text, _) in dataset:\n",
        "    counter.update(str(text).split())\n",
        "  my_vocab = vocab(counter, min_freq=min_freq)\n",
        "  my_vocab.insert_token('<unk>', 0)\n",
        "  my_vocab.set_default_index(0)\n",
        "  return my_vocab"
      ],
      "metadata": {
        "id": "77oCdRyjxLOn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multiclass_vocab = create_vocab(trainset, min_freq = 5)"
      ],
      "metadata": {
        "id": "8XWKpc9VxLRj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(multiclass_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kjuc-CTxLUe",
        "outputId": "c57a0810-c5b8-4a3d-a2db-36ea71f45d2e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84078"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#multiclass_vocab.get_itos()"
      ],
      "metadata": {
        "id": "wzjAUk91bpLn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a lambda function objects that will be used to get the indices of words from vocab\n",
        "text_pipeline = lambda x: [multiclass_vocab[token] for token in str(x).split()]\n",
        "label_pipeline = lambda x: int(x)"
      ],
      "metadata": {
        "id": "BQEBHU0HcY_F"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We know that input to the embedding layers are indices of words from the vocab.\n",
        "The collate_batch() accepts batch of data and gets the indices of text from vocab and returns the same\n",
        "We will include this collate_batch() in collat_fn attribute of DataLoader.\n",
        "So it will create a batch of data containing indices of words and corresponding labels.\n",
        "But for EmbeddingBag we need one more extra parameter, that is offset.\n",
        "offsets determines the starting index position of each bag (sequence) in input.\n",
        "'''\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_text, _label) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return text_list, label_list, offsets"
      ],
      "metadata": {
        "id": "pAykhHQgcZBv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=2\n",
        "check_loader= torch.utils.data.DataLoader(dataset=trainset,\n",
        "                                        batch_size=batch_size,\n",
        "                                        shuffle=True,\n",
        "                                        collate_fn=collate_batch,\n",
        "                                        num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDfacNK3cZEG",
        "outputId": "8796fd22-5581-47fc-9f52-eab34b2927b7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text, label, offsets in check_loader:\n",
        "  print(label, text, offsets)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wSgamP_bpOJ",
        "outputId": "299b3fcb-0a0b-4ce7-c30c-7053909154a0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6, 0]) tensor([    0, 13426,   364,   845,    22,    38,   849, 74397,   851,   955,\n",
            "           77,     0,  1596,    30,   104,   851,   955,    77,     0,  1596,\n",
            "           30,   104,   851,   955,    77,     0,  1596,    30,   104,   851,\n",
            "          955,    77,     0,  1596,    30,   104,   851,  2554,    76, 74397,\n",
            "           78, 13195,  1596,    30,   104,   851,  1727,     0,    30,   104,\n",
            "          851,  1727,     0,    30,   104,   851,     0,    22, 74397,   851,\n",
            "          955,    77,     0,  1596,    30,   104,   851,   955,    77,     0,\n",
            "         1596,    30,   104,   851,   955,    77,     0,  1596,    30,   104,\n",
            "          851,   955,    77,     0,  1596,    30,   104,   851,  2554,    76,\n",
            "        74397,    78,     0,  1596,    30,   104,   851,  1727,     0,    30,\n",
            "          104,   851,  1727,     0,    30,   104,   336,   955,    77,     0,\n",
            "         1596,    27,     0,   955,    77,     0,  1596,    27,     0,   955,\n",
            "           77,     0,  1596,    27,     0,   955,    77,     0,  1596,    27,\n",
            "            0,  2554,    76, 74397,    78, 13195,  1596,    27,     0,  1727,\n",
            "            0,    27,     0,  1727,     0,    27,     0,   851,     0,  3192,\n",
            "         1477,   657,   156,   624,   114, 32145,   330, 10676,  3192,  2256,\n",
            "          663,  5195,  1478,  3192,   149,   114,   663,  2256,  5195,   147,\n",
            "          114,   864,  5195,  3192,  1229,  2256,  3194,  2145,    24,   324,\n",
            "          363]) tensor([  0, 149])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPCustom(nn.Module):\n",
        "  def __init__(self, embed_dim, vocab_size, h_sizes_list, d_prob_list, output_dim, non_linearity, batch_norm):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.h_sizes_list = h_sizes_list\n",
        "    self.d_prob_list = d_prob_list\n",
        "    self.batch_norm = batch_norm\n",
        "    \n",
        "    self.non_linearity = non_linearity\n",
        "\n",
        "    model_layers = []\n",
        "\n",
        "    # embedding_layer\n",
        "    self.embedding = nn.EmbeddingBag(self.vocab_size, self.embed_dim)\n",
        "\n",
        "    input_dim = self.embed_dim\n",
        "    # hidden layers, droput, non_linearity, batchnorm layers\n",
        "    for k, hidden_size in enumerate(self.h_sizes_list):\n",
        "      # hidden_layer\n",
        "      model_layers.append(nn.Linear(input_dim, hidden_size))\n",
        "      # Activation function\n",
        "      model_layers.append(self.non_linearity)\n",
        "      # Dropout Layer\n",
        "      model_layers.append(nn.Dropout(p=self.d_prob_list[k]))\n",
        "      # Batch_Norm Layer\n",
        "      if self.batch_norm:\n",
        "        model_layers.append(nn.BatchNorm1d(hidden_size, momentum = 0.9))\n",
        "      input_dim = hidden_size\n",
        "\n",
        "    # output layer  \n",
        "    if len(self.h_sizes_list)>0:\n",
        "        model_layers.append(nn.Linear(self.h_sizes_list[-1], self.output_dim))\n",
        "    else:\n",
        "        model_layers.append(nn.Linear(self.embed_dim, self.output_dim))\n",
        "\n",
        "    self.module_list = nn.ModuleList(model_layers)\n",
        "    \n",
        "\n",
        "  def forward(self, x, offsets):\n",
        "    out = self.embedding(x, offsets) # batchsize, embedding_dim\n",
        "    for layer in self.module_list:\n",
        "      out = layer(out)\n",
        "    return out\n",
        "\n",
        "    # Note : We do not need to apply softmax as we will be using nn.CrossEntropy Loss"
      ],
      "metadata": {
        "id": "7qK-0ReabpQZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, loss_function, model, optimizer, grad_clipping, max_norm, log_batch, log_interval):\n",
        "\n",
        "  # Training Loop \n",
        "\n",
        "  # initilalize variables as global\n",
        "  # these counts will be updated every epoch\n",
        "  global batch_ct_train\n",
        "\n",
        "  # Initialize train_loss at the he start of the epoch\n",
        "  running_train_loss = 0\n",
        "  running_train_correct = 0\n",
        "  \n",
        "  # put the model in training mode\n",
        "\n",
        "  model.train()\n",
        "  # Iterate on batches from the dataset using train_loader\n",
        "  for input_, targets, offsets in train_loader:\n",
        "    \n",
        "    # move inputs and outputs to GPUs\n",
        "    input_ = input_.to(device)\n",
        "    targets = targets.to(device)\n",
        "    offsets = offsets.to(device)\n",
        "\n",
        "\n",
        "    # Step 1: Forward Pass: Compute model's predictions \n",
        "    output = model(input_, offsets)\n",
        "    \n",
        "    # Step 2: Compute loss\n",
        "    loss = loss_function(output, targets)\n",
        "\n",
        "    # Correct prediction\n",
        "    y_pred = torch.argmax(output, dim = 1)\n",
        "    correct = torch.sum(y_pred == targets)\n",
        "\n",
        "    batch_ct_train += 1\n",
        "\n",
        "    # Step 3: Backward pass -Compute the gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient Clipping\n",
        "    if grad_clipping:\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm, norm_type=2)\n",
        "\n",
        "    # Step 4: Update the parameters\n",
        "    optimizer.step()\n",
        "          \n",
        "    # Add train loss of a batch \n",
        "    running_train_loss += loss.item()\n",
        "\n",
        "    # Add Corect counts of a batch\n",
        "    running_train_correct += correct\n",
        "\n",
        "    # log batch loss and accuracy\n",
        "    if log_batch:\n",
        "      if ((batch_ct_train + 1) % log_interval) == 0:\n",
        "        wandb.log({f\"Train Batch Loss  :\": loss})\n",
        "        wandb.log({f\"Train Batch Acc :\": correct/len(targets)})\n",
        "\n",
        "  \n",
        "  # Calculate mean train loss for the whole dataset for a particular epoch\n",
        "  train_loss = running_train_loss/len(train_loader)\n",
        "\n",
        "  # Calculate accuracy for the whole dataset for a particular epoch\n",
        "  train_acc = running_train_correct/len(train_loader.dataset)\n",
        "  \n",
        "\n",
        "  return train_loss, train_acc"
      ],
      "metadata": {
        "id": "0qjGT1_PbpSr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(valid_loader, loss_function, model, log_batch, log_interval):\n",
        "\n",
        "  # initilalize variables as global\n",
        "  # these counts will be updated every epoch\n",
        "  global batch_ct_valid\n",
        "\n",
        "  # Validation/Test loop\n",
        "  # Initialize valid_loss at the he strat of the epoch\n",
        "  running_val_loss = 0\n",
        "  running_val_correct = 0\n",
        "\n",
        "  # put the model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for input_, targets, offsets in valid_loader:\n",
        "\n",
        "      # move inputs and outputs to GPUs\n",
        "      input_ = input_.to(device)\n",
        "      targets = targets.to(device)\n",
        "      offsets = offsets.to(device)\n",
        "\n",
        "      # Step 1: Forward Pass: Compute model's predictions \n",
        "      output = model(input_, offsets)\n",
        "\n",
        "      # Step 2: Compute loss\n",
        "      loss = loss_function(output, targets)\n",
        "\n",
        "      # Correct Predictions\n",
        "      y_pred = torch.argmax(output, dim = 1)\n",
        "      correct = torch.sum(y_pred == targets)\n",
        "\n",
        "      batch_ct_valid += 1\n",
        "\n",
        "      # Add val loss of a batch \n",
        "      running_val_loss += loss.item()\n",
        "\n",
        "      # Add correct count for each batch\n",
        "      running_val_correct += correct\n",
        "\n",
        "      # log batch loss and accuracy\n",
        "      if log_batch:\n",
        "        if ((batch_ct_valid + 1) % log_interval) == 0:\n",
        "          wandb.log({f\"Valid Batch Loss  :\": loss})\n",
        "          wandb.log({f\"Valid Batch Accuracy :\": correct/len(targets)})\n",
        "\n",
        "    # Calculate mean val loss for the whole dataset for a particular epoch\n",
        "    val_loss = running_val_loss/len(valid_loader)\n",
        "\n",
        "    # Calculate accuracy for the whole dataset for a particular epoch\n",
        "    val_acc = running_val_correct/len(valid_loader.dataset)\n",
        "\n",
        "    # scheduler step\n",
        "    # scheduler.step(valid_loss)\n",
        "    # scheduler.step()\n",
        "    \n",
        "  return val_loss, val_acc"
      ],
      "metadata": {
        "id": "HDaK8phQbpVO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(train_loader, valid_loader, model, optimizer, loss_function, epochs, device, patience, early_stopping,\n",
        "               file_model, save_best_model):\n",
        "    \n",
        "  \"\"\" \n",
        "  Function for training the model and plotting the graph for train & validation loss vs epoch.\n",
        "  Input: iterator for train dataset, initial weights and bias, epochs, learning rate, batch size.\n",
        "  Output: final weights, bias and train loss and validation loss for each epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create lists to store train and val loss at each epoch\n",
        "  train_loss_history = []\n",
        "  valid_loss_history = []\n",
        "  train_acc_history = []\n",
        "  valid_acc_history = []\n",
        "\n",
        "  # initialize variables for early stopping\n",
        "\n",
        "  delta = 0\n",
        "  best_score = None\n",
        "  valid_loss_min = np.Inf\n",
        "  counter_early_stop=0\n",
        "  early_stop=False\n",
        "\n",
        "  # Iterate for the given number of epochs\n",
        "  # Step 5: Repeat steps 1 - 4\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    t0 = datetime.now()\n",
        "\n",
        "    # Get train loss and accuracy for one epoch\n",
        "    train_loss, train_acc = train(train_loader, loss_function, model, optimizer, \n",
        "                                  wandb.config.GRAD_CLIPPING, wandb.config.MAX_NORM,\n",
        "                                  wandb.config.LOG_BATCH, wandb.config.LOG_INTERVAL)\n",
        "    valid_loss, valid_acc   = validate(valid_loader, loss_function, model, \n",
        "                                       wandb.config.LOG_BATCH, wandb.config.LOG_INTERVAL)\n",
        "\n",
        "    dt = datetime.now() - t0\n",
        "\n",
        "    # Save history of the Losses and accuracy\n",
        "    train_loss_history.append(train_loss)\n",
        "    train_acc_history.append(train_acc)\n",
        "\n",
        "    valid_loss_history.append(valid_loss)\n",
        "    valid_acc_history.append(valid_acc)\n",
        "\n",
        "    # Log the train and valid loss to wandb\n",
        "    wandb.log({f\"Train Loss :\": train_loss, \"epoch\": epoch})\n",
        "    wandb.log({f\"Train Acc :\": train_acc, \"epoch\": epoch})\n",
        "\n",
        "    wandb.log({f\"Valid Loss :\": valid_loss, \"epoch\": epoch})\n",
        "    wandb.log({f\"Valid Acc :\": valid_acc, \"epoch\": epoch})\n",
        "\n",
        "    if early_stopping:\n",
        "      score = -valid_loss\n",
        "      if best_score is None:\n",
        "        best_score=score\n",
        "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n",
        "        torch.save(model.state_dict(), file_model)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "      elif score < best_score + delta:\n",
        "        counter_early_stop += 1\n",
        "        print(f'Early stoping counter: {counter_early_stop} out of {patience}')\n",
        "        if counter_early_stop > patience:\n",
        "          early_stop = True\n",
        "\n",
        "\n",
        "      else:\n",
        "        best_score = score\n",
        "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n",
        "        torch.save(model.state_dict(), file_model)\n",
        "        counter_early_stop=0\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "      if early_stop:\n",
        "        print('Early Stopping')\n",
        "        break\n",
        "\n",
        "    elif save_best_model:\n",
        "\n",
        "      score = -valid_loss\n",
        "      if best_score is None:\n",
        "        best_score=score\n",
        "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n",
        "        torch.save(model.state_dict(), file_model)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "      elif score < best_score + delta:\n",
        "        print(f'Validation loss has not decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Not Saving Model...')\n",
        "      \n",
        "      else:\n",
        "        best_score = score\n",
        "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n",
        "        torch.save(model.state_dict(), file_model)\n",
        "        valid_loss_min = valid_loss\n",
        "        \n",
        "    else:\n",
        "        torch.save(model.state_dict(), file_model)\n",
        "    \n",
        "    # Print the train loss and accuracy for given number of epochs, batch size and number of samples\n",
        "    print(f'Epoch : {epoch+1} / {epochs}')\n",
        "    print(f'Time to complete {epoch+1} is {dt}')\n",
        "    # print(f'Learning rate: {scheduler._last_lr[0]}')\n",
        "    print(f'Train Loss: {train_loss : .4f} | Train Accuracy: {train_acc * 100 : .4f}%')\n",
        "    print(f'Valid Loss: {valid_loss : .4f} | Valid Accuracy: {valid_acc * 100 : .4f}%')\n",
        "    print()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  return train_loss_history, train_acc_history, valid_loss_history, valid_acc_history"
      ],
      "metadata": {
        "id": "LJwI0GkIbpXe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acc_pred(data_loader, model, device):\n",
        "    \n",
        "  \"\"\" \n",
        "  Function to get predictions and accuracy for a given data using estimated model\n",
        "  Input: Data iterator, Final estimated weoights, bias\n",
        "  Output: Prections and Accuracy for given dataset\n",
        "  \"\"\"\n",
        "\n",
        "  # Array to store predicted labels\n",
        "  predictions = torch.Tensor() # empty tensor\n",
        "  predictions = predictions.to(device) # move predictions to GPU\n",
        "\n",
        "  # Array to store actual labels\n",
        "  y = torch.Tensor() # empty tensor\n",
        "  y = y.to(device)\n",
        "\n",
        "  # put the model in evaluation mode\n",
        "  model.eval()\n",
        "  \n",
        "  # Iterate over batches from data iterator\n",
        "  with torch.no_grad():\n",
        "    for input_, targets, offsets in data_loader:\n",
        "      \n",
        "      # move inputs and outputs to GPUs\n",
        "      \n",
        "      input_ = input_.to(device)\n",
        "      targets = targets.to(device)\n",
        "      offsets = offsets.to(device)\n",
        "      \n",
        "      # Calculated the predicted labels\n",
        "      output = model(input_, offsets)\n",
        "\n",
        "      # Choose the label with maximum probability\n",
        "      prediction = torch.argmax(output, dim = 1)\n",
        "\n",
        "      # Add the predicted labels to the array\n",
        "      predictions = torch.cat((predictions, prediction)) \n",
        "\n",
        "      # Add the actual labels to the array\n",
        "      y = torch.cat((y, targets)) \n",
        "\n",
        "  # Check for complete dataset if actual and predicted labels are same or not\n",
        "  # Calculate accuracy\n",
        "  acc = (predictions == y).float().mean()\n",
        "\n",
        "  # Return tuple containing predictions and accuracy\n",
        "  return predictions, acc  "
      ],
      "metadata": {
        "id": "OyvRQPwDbpaA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = SimpleNamespace(\n",
        "    EMBED_DIM = 300,\n",
        "    VOCAB_SIZE = len(multiclass_vocab),\n",
        "    OUTPUT_DIM = 10,\n",
        "    HIDDEN_SIZES_LIST = [200, 100], # 100 layers of size 200  [200]*100\n",
        "    DPROB_LIST = [0.5, 0.5],\n",
        "    NON_LINEARITY= nn.ReLU(),\n",
        "    BATCH_NORM = False,\n",
        "    EPOCHS = 20,\n",
        "    \n",
        "    BATCH_SIZE = 256,\n",
        "    LEARNING_RATE = 0.2,\n",
        "    DATASET=\"MULTICLASS\",\n",
        "    ARCHITECTUREe=\"Embed_2_hidden_layers\",\n",
        "    LOG_INTERVAL = 25,\n",
        "    LOG_BATCH = True,\n",
        "    FILE_MODEL = model_folder/'multiclass.pt',\n",
        "    GRAD_CLIPPING = False,\n",
        "    MAX_NORM = 0,\n",
        "    MOMENTUM = 0,\n",
        "    PATIENCE = 5,\n",
        "    EARLY_STOPPING = True,\n",
        "    # SCHEDULER_FACTOR = 0,\n",
        "    # SCHEDULER_PATIENCE = 0,\n",
        "    WEIGHT_DECAY = 0,\n",
        "    SAVE_BEST_MODEL = False,\n",
        "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    )"
      ],
      "metadata": {
        "id": "nSZNVW0kbpcY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new project\n",
        "import random\n",
        "wandb.init(name = 'Embed_2_hidden_layers', project = 'NLP_MLP_Template', config = hyperparameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "BuRRACo1bpe6",
        "outputId": "3cf66943-1206-40b0-d9a9-244e0b3a5eea"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhamshaks1\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221031_012228-1quhmc27</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/hamshaks1/NLP_MLP_Template/runs/1quhmc27\" target=\"_blank\">Embed_2_hidden_layers</a></strong> to <a href=\"https://wandb.ai/hamshaks1/NLP_MLP_Template\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/hamshaks1/NLP_MLP_Template/runs/1quhmc27?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fc9b4264990>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.config = hyperparameters\n",
        "wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gTkB-fubpha",
        "outputId": "ab2b7f19-533c-4f48-c404-2666ff562a06"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "namespace(ARCHITECTUREe='Embed_2_hidden_layers', BATCH_NORM=False, BATCH_SIZE=256, DATASET='MULTICLASS', DEVICE=device(type='cpu'), DPROB_LIST=[0.5, 0.5], EARLY_STOPPING=True, EMBED_DIM=300, EPOCHS=20, FILE_MODEL=PosixPath('/content/drive/MyDrive/NLP_Fall22/HW5/Models/multiclass.pt'), GRAD_CLIPPING=False, HIDDEN_SIZES_LIST=[200, 100], LEARNING_RATE=0.2, LOG_BATCH=True, LOG_INTERVAL=25, MAX_NORM=0, MOMENTUM=0, NON_LINEARITY=ReLU(), OUTPUT_DIM=10, PATIENCE=5, SAVE_BEST_MODEL=False, VOCAB_SIZE=84078, WEIGHT_DECAY=0)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix seed value\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Data Loader\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=wandb.config.BATCH_SIZE, shuffle = True, \n",
        "                                           collate_fn=collate_batch, num_workers = 4)\n",
        "valid_loader = torch.utils.data.DataLoader(validset, batch_size=wandb.config.BATCH_SIZE, shuffle = False, \n",
        "                                           collate_fn=collate_batch,  num_workers = 4)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=wandb.config.BATCH_SIZE,   shuffle = False, \n",
        "                                          collate_fn=collate_batch,  num_workers = 4)\n",
        "\n",
        "# cross entropy loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# model \n",
        "model_multiclass = MLPCustom(wandb.config.EMBED_DIM, \n",
        "                       wandb.config.VOCAB_SIZE, \n",
        "                       wandb.config.HIDDEN_SIZES_LIST, \n",
        "                       wandb.config.DPROB_LIST,\n",
        "                       wandb.config.OUTPUT_DIM, \n",
        "                       wandb.config.NON_LINEARITY,\n",
        "                       wandb.config.BATCH_NORM)\n",
        "\n",
        "model_multiclass.to(wandb.config.DEVICE)\n",
        "\n",
        "def init_weights(m):\n",
        "  if type(m) == nn.Linear:\n",
        "      torch.nn.init.kaiming_normal_(m.weight)\n",
        "      torch.nn.init.zeros_(m.bias)\n",
        "        \n",
        "# apply initialization recursively  to all modules\n",
        "model_multiclass.apply(init_weights)\n",
        "\n",
        "# Intialize stochiastic gradient descent optimizer\n",
        "optimizer = torch.optim.SGD(model_multiclass.parameters(), \n",
        "                             lr = wandb.config.LEARNING_RATE, \n",
        "                             weight_decay=wandb.config.WEIGHT_DECAY)\n",
        "\n",
        "wandb.config.OPTIMIZER = optimizer\n",
        "\n",
        "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor= wandb.config.scheduler_factor, \n",
        "#                              patience=wandb.config.scheduler_patience, verbose=True)\n",
        "\n",
        "#scheduler = StepLR(optimizer, gamma=0.4,step_size=1, verbose=True)"
      ],
      "metadata": {
        "id": "4CX3Xc2Tbpj2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVzGVhAUbpmY",
        "outputId": "9a5ebc12-eaab-46e5-c1a0-69a77ea9cebe"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "namespace(ARCHITECTUREe='Embed_2_hidden_layers', BATCH_NORM=False, BATCH_SIZE=256, DATASET='MULTICLASS', DEVICE=device(type='cpu'), DPROB_LIST=[0.5, 0.5], EARLY_STOPPING=True, EMBED_DIM=300, EPOCHS=20, FILE_MODEL=PosixPath('/content/drive/MyDrive/NLP_Fall22/HW5/Models/multiclass.pt'), GRAD_CLIPPING=False, HIDDEN_SIZES_LIST=[200, 100], LEARNING_RATE=0.2, LOG_BATCH=True, LOG_INTERVAL=25, MAX_NORM=0, MOMENTUM=0, NON_LINEARITY=ReLU(), OPTIMIZER=SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    foreach: None\n",
              "    lr: 0.2\n",
              "    maximize: False\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              "), OUTPUT_DIM=10, PATIENCE=5, SAVE_BEST_MODEL=False, VOCAB_SIZE=84078, WEIGHT_DECAY=0)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "for input_, targets, offsets in train_loader:\n",
        "  \n",
        "  # move inputs and outputs to GPUs\n",
        "  input_ = input_.to(device)\n",
        "  targets = targets.to(device)\n",
        "  offsets = offsets.to(device)\n",
        "  model_multiclass.eval()\n",
        "  # Forward pass\n",
        "  output = model_multiclass(input_, offsets)\n",
        "  loss = loss_function(output, targets)\n",
        "  print(f'Actual loss: {loss}')\n",
        "  break\n",
        "\n",
        "print(f'Expected Theoretical loss: {np.log(2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLFhUfYXbppC",
        "outputId": "ef5d6517-68d5-43d0-d659-62a555fab5ce"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual loss: 2.380561351776123\n",
            "Expected Theoretical loss: 0.6931471805599453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.watch(model_multiclass, log = 'all', log_freq=25, log_graph=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83a7V8zLbprc",
        "outputId": "36c0e509-f4c4-4098-f049-e4d3bc71139d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<wandb.wandb_torch.TorchGraph at 0x7fc9b3cd16d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See live graphs in the notebook.\n",
        "#%%wandb \n",
        "batch_ct_train, batch_ct_valid = 0, 0\n",
        "train_loss_history, train_acc_history, valid_loss_history, valid_acc_history = train_loop(train_loader, \n",
        "                                                                                          valid_loader, \n",
        "                                                                                          model_multiclass, \n",
        "                                                                                          optimizer,\n",
        "                                                                                          loss_function, \n",
        "                                                                                          wandb.config.EPOCHS, \n",
        "                                                                                          wandb.config.DEVICE,\n",
        "                                                                                          wandb.config.PATIENCE, \n",
        "                                                                                          wandb.config.EARLY_STOPPING,\n",
        "                                                                                          wandb.config.FILE_MODEL,\n",
        "                                                                                          wandb.config.SAVE_BEST_MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r0gRpoMbpt7",
        "outputId": "2ab439b1-35c7-4a10-a3ac-a9b3a5f8d125"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss has decreased (inf --> 0.832393). Saving Model...\n",
            "Epoch : 1 / 20\n",
            "Time to complete 1 is 0:01:21.320497\n",
            "Train Loss:  1.4000 | Train Accuracy:  53.4957%\n",
            "Valid Loss:  0.8324 | Valid Accuracy:  73.9305%\n",
            "\n",
            "Validation loss has decreased (0.832393 --> 0.711477). Saving model...\n",
            "Epoch : 2 / 20\n",
            "Time to complete 2 is 0:01:22.717359\n",
            "Train Loss:  0.9576 | Train Accuracy:  69.9613%\n",
            "Valid Loss:  0.7115 | Valid Accuracy:  77.8695%\n",
            "\n",
            "Validation loss has decreased (0.711477 --> 0.658536). Saving model...\n",
            "Epoch : 3 / 20\n",
            "Time to complete 3 is 0:01:20.512033\n",
            "Train Loss:  0.8489 | Train Accuracy:  73.7601%\n",
            "Valid Loss:  0.6585 | Valid Accuracy:  79.7596%\n",
            "\n",
            "Validation loss has decreased (0.658536 --> 0.621304). Saving model...\n",
            "Epoch : 4 / 20\n",
            "Time to complete 4 is 0:01:22.580376\n",
            "Train Loss:  0.7868 | Train Accuracy:  75.5788%\n",
            "Valid Loss:  0.6213 | Valid Accuracy:  80.6491%\n",
            "\n",
            "Validation loss has decreased (0.621304 --> 0.610166). Saving model...\n",
            "Epoch : 5 / 20\n",
            "Time to complete 5 is 0:01:22.406436\n",
            "Train Loss:  0.7425 | Train Accuracy:  77.0413%\n",
            "Valid Loss:  0.6102 | Valid Accuracy:  80.7814%\n",
            "\n",
            "Validation loss has decreased (0.610166 --> 0.574960). Saving model...\n",
            "Epoch : 6 / 20\n",
            "Time to complete 6 is 0:01:20.198350\n",
            "Train Loss:  0.7081 | Train Accuracy:  78.0579%\n",
            "Valid Loss:  0.5750 | Valid Accuracy:  81.6921%\n",
            "\n",
            "Validation loss has decreased (0.574960 --> 0.557625). Saving model...\n",
            "Epoch : 7 / 20\n",
            "Time to complete 7 is 0:01:21.083498\n",
            "Train Loss:  0.6805 | Train Accuracy:  78.9083%\n",
            "Valid Loss:  0.5576 | Valid Accuracy:  82.1792%\n",
            "\n",
            "Validation loss has decreased (0.557625 --> 0.543372). Saving model...\n",
            "Epoch : 8 / 20\n",
            "Time to complete 8 is 0:01:21.435218\n",
            "Train Loss:  0.6571 | Train Accuracy:  79.6045%\n",
            "Valid Loss:  0.5434 | Valid Accuracy:  82.6557%\n",
            "\n",
            "Validation loss has decreased (0.543372 --> 0.531751). Saving model...\n",
            "Epoch : 9 / 20\n",
            "Time to complete 9 is 0:01:22.212717\n",
            "Train Loss:  0.6377 | Train Accuracy:  80.2398%\n",
            "Valid Loss:  0.5318 | Valid Accuracy:  82.9151%\n",
            "\n",
            "Validation loss has decreased (0.531751 --> 0.520791). Saving model...\n",
            "Epoch : 10 / 20\n",
            "Time to complete 10 is 0:01:21.766024\n",
            "Train Loss:  0.6177 | Train Accuracy:  80.7719%\n",
            "Valid Loss:  0.5208 | Valid Accuracy:  83.4922%\n",
            "\n",
            "Validation loss has decreased (0.520791 --> 0.507339). Saving model...\n",
            "Epoch : 11 / 20\n",
            "Time to complete 11 is 0:01:23.120717\n",
            "Train Loss:  0.6031 | Train Accuracy:  81.1564%\n",
            "Valid Loss:  0.5073 | Valid Accuracy:  83.6828%\n",
            "\n",
            "Validation loss has decreased (0.507339 --> 0.499457). Saving model...\n",
            "Epoch : 12 / 20\n",
            "Time to complete 12 is 0:01:22.910100\n",
            "Train Loss:  0.5888 | Train Accuracy:  81.6568%\n",
            "Valid Loss:  0.4995 | Valid Accuracy:  83.9845%\n",
            "\n",
            "Validation loss has decreased (0.499457 --> 0.495523). Saving model...\n",
            "Epoch : 13 / 20\n",
            "Time to complete 13 is 0:01:22.181475\n",
            "Train Loss:  0.5785 | Train Accuracy:  81.9043%\n",
            "Valid Loss:  0.4955 | Valid Accuracy:  84.0587%\n",
            "\n",
            "Validation loss has decreased (0.495523 --> 0.490222). Saving model...\n",
            "Epoch : 14 / 20\n",
            "Time to complete 14 is 0:01:22.727083\n",
            "Train Loss:  0.5659 | Train Accuracy:  82.2994%\n",
            "Valid Loss:  0.4902 | Valid Accuracy:  84.2757%\n",
            "\n",
            "Validation loss has decreased (0.490222 --> 0.478839). Saving model...\n",
            "Epoch : 15 / 20\n",
            "Time to complete 15 is 0:01:22.440496\n",
            "Train Loss:  0.5548 | Train Accuracy:  82.5423%\n",
            "Valid Loss:  0.4788 | Valid Accuracy:  84.6781%\n",
            "\n",
            "Validation loss has decreased (0.478839 --> 0.472452). Saving model...\n",
            "Epoch : 16 / 20\n",
            "Time to complete 16 is 0:01:21.450433\n",
            "Train Loss:  0.5439 | Train Accuracy:  82.9334%\n",
            "Valid Loss:  0.4725 | Valid Accuracy:  84.8899%\n",
            "\n",
            "Validation loss has decreased (0.472452 --> 0.465765). Saving model...\n",
            "Epoch : 17 / 20\n",
            "Time to complete 17 is 0:01:21.826254\n",
            "Train Loss:  0.5361 | Train Accuracy:  83.1379%\n",
            "Valid Loss:  0.4658 | Valid Accuracy:  84.9852%\n",
            "\n",
            "Early stoping counter: 1 out of 5\n",
            "Epoch : 18 / 20\n",
            "Time to complete 18 is 0:01:21.975397\n",
            "Train Loss:  0.5261 | Train Accuracy:  83.6018%\n",
            "Valid Loss:  0.4670 | Valid Accuracy:  85.0805%\n",
            "\n",
            "Validation loss has decreased (0.465765 --> 0.457162). Saving model...\n",
            "Epoch : 19 / 20\n",
            "Time to complete 19 is 0:01:24.594606\n",
            "Train Loss:  0.5212 | Train Accuracy:  83.7454%\n",
            "Valid Loss:  0.4572 | Valid Accuracy:  85.3293%\n",
            "\n",
            "Validation loss has decreased (0.457162 --> 0.456360). Saving model...\n",
            "Epoch : 20 / 20\n",
            "Time to complete 20 is 0:01:22.788437\n",
            "Train Loss:  0.5133 | Train Accuracy:  83.8765%\n",
            "Valid Loss:  0.4564 | Valid Accuracy:  85.4670%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsDP7lYIbpv-",
        "outputId": "560605b8-6d89-4807-f3e2-58903d0e84d2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_nn = MLPCustom(wandb.config.EMBED_DIM, \n",
        "                       wandb.config.VOCAB_SIZE, \n",
        "                       wandb.config.HIDDEN_SIZES_LIST, \n",
        "                       wandb.config.DPROB_LIST,\n",
        "                       wandb.config.OUTPUT_DIM, \n",
        "                       wandb.config.NON_LINEARITY,\n",
        "                       wandb.config.BATCH_NORM)\n",
        "model_nn.to(device)\n",
        "model_nn.load_state_dict(torch.load(wandb.config.FILE_MODEL))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW9BhCwpeYlr",
        "outputId": "d9605d32-0112-40f7-9a26-371c230afe8a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the prediction and accuracy for the test dataseta\n",
        "predictions_test, acc_test = get_acc_pred(test_loader, model_nn, device)\n",
        "predictions_train, acc_train = get_acc_pred(train_loader, model_nn, device)\n",
        "predictions_valid, acc_valid = get_acc_pred(valid_loader, model_nn, device)"
      ],
      "metadata": {
        "id": "GK9-qQzweYoN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Test Accuracy\n",
        "print('Test accuracy', acc_test * 100)\n",
        "print('Train accuracy', acc_train * 100)\n",
        "print('Valid accuracy', acc_valid * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONUiXlwoeYq4",
        "outputId": "03d1aaa1-a7e2-48a1-a0c0-0d5176f5932e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy tensor(84.8952)\n",
            "Train accuracy tensor(87.1438)\n",
            "Valid accuracy tensor(85.4670)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.log({'Best_test_Acc': acc_test})\n",
        "wandb.log({'Best_train_Acc': acc_train})\n",
        "wandb.log({'Best_valid_Acc': acc_valid})"
      ],
      "metadata": {
        "id": "ehzEaeEieYtn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get an array containing actual labels\n",
        "testing_labels = testset.y"
      ],
      "metadata": {
        "id": "XS5dAN50eYwE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(testing_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hgVdna1eYyi",
        "outputId": "ed60ebb9-9096-497e-ce76-9bb93b01ce99"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log a confusion matrix to W&B\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(\n",
        "                        probs = None,\n",
        "                        y_true = testing_labels,\n",
        "                        preds = predictions_test.to('cpu').numpy(),\n",
        "                        class_names =['c#', 'java', 'php', 'javascript', 'android', 'jquery', 'c++', 'python', 'iphone', 'asp.net'])})"
      ],
      "metadata": {
        "id": "IHshts4ceY1B"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "ZsceAP-xeY3d",
        "outputId": "9f7295d4-7952-4758-bf04-417a1166fe33"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Best_test_Acc</td><td>▁</td></tr><tr><td>Best_train_Acc</td><td>▁</td></tr><tr><td>Best_valid_Acc</td><td>▁</td></tr><tr><td>Train Acc :</td><td>▁▅▆▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>Train Batch Acc :</td><td>▁▄▄▄▅▅▆▅▆▆▇▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇▇▇</td></tr><tr><td>Train Batch Loss  :</td><td>█▅▅▄▄▄▃▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂</td></tr><tr><td>Train Loss :</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Valid Acc :</td><td>▁▃▅▅▅▆▆▆▆▇▇▇▇▇██████</td></tr><tr><td>Valid Batch Accuracy :</td><td>▁▂▃▂▄▅▅▆▄▄▅▄▄▅▆▆▆▅▄▄▅▅▆▅▇▅▄█▆▅▅▆▆▆▅▇▇▇█▆</td></tr><tr><td>Valid Batch Loss  :</td><td>█▇▆▇▆▄▄▄▆▅▄▃▄▃▃▂▃▄▄▅▃▄▃▄▂▃▅▂▃▃▄▄▂▃▂▂▂▂▁▃</td></tr><tr><td>Valid Loss :</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best_test_Acc</td><td>0.84895</td></tr><tr><td>Best_train_Acc</td><td>0.87144</td></tr><tr><td>Best_valid_Acc</td><td>0.85467</td></tr><tr><td>Train Acc :</td><td>0.83876</td></tr><tr><td>Train Batch Acc :</td><td>0.86328</td></tr><tr><td>Train Batch Loss  :</td><td>0.51217</td></tr><tr><td>Train Loss :</td><td>0.51328</td></tr><tr><td>Valid Acc :</td><td>0.85467</td></tr><tr><td>Valid Batch Accuracy :</td><td>0.84375</td></tr><tr><td>Valid Batch Loss  :</td><td>0.47612</td></tr><tr><td>Valid Loss :</td><td>0.45636</td></tr><tr><td>epoch</td><td>19</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">Embed_2_hidden_layers</strong>: <a href=\"https://wandb.ai/hamshaks1/NLP_MLP_Template/runs/1quhmc27\" target=\"_blank\">https://wandb.ai/hamshaks1/NLP_MLP_Template/runs/1quhmc27</a><br/>Synced 5 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221031_012228-1quhmc27/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xi6tfwVUvPX4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}