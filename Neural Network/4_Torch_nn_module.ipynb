{"cells":[{"cell_type":"markdown","metadata":{"id":"AXarCLfXYOIk","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["# <font color = 'pickle'>**Lecture Goal**\n","In this lecture, we will understand PyTorch nn. Module. All the modules in Pytorch are implemented as subclass of the torch.nn.Module class. Pytorch uses these modules to perfrom operations on Tensors. We will first understand some importnat modules and then use these in implementing Linear Regression.\n","\n","We will first disuss following modules \n","\n","- nn.Linear()\n","- nn.Sequential()\n","- nn.init()\n","- nn.MSELoss()\n","- torch.optim()\n","- torch.utils.data.DataLoader\n","- torch.utils.data.TensorDataset\n","\n","We will then use these modules to refactor linear regression (HW1)."]},{"cell_type":"markdown","metadata":{"id":"M6mPJ9A0X6DX"},"source":["# <font color = 'pickle'>**Install Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:21:12.016615Z","iopub.status.busy":"2022-09-25T18:21:12.016414Z","iopub.status.idle":"2022-09-25T18:21:12.019809Z","shell.execute_reply":"2022-09-25T18:21:12.019318Z","shell.execute_reply.started":"2022-09-25T18:21:12.016601Z"},"id":"u8KBBtPWUQHO","tags":[]},"outputs":[],"source":["# install torchviz libraries\n","if 'google.colab' in str(get_ipython()):\n","    !pip install torchsummary -qq"]},{"cell_type":"markdown","metadata":{"id":"4kxCqZPzbMQD"},"source":["# <font color = 'pickle'>**Import Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:03:51.543030Z","iopub.status.busy":"2022-09-25T18:03:51.542833Z","iopub.status.idle":"2022-09-25T18:03:51.884790Z","shell.execute_reply":"2022-09-25T18:03:51.884179Z","shell.execute_reply.started":"2022-09-25T18:03:51.543016Z"},"id":"q4ikzNObI7gs"},"outputs":[],"source":["# Importing PyTorch Library\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.nn.functional as F\n","import torchsummary\n","\n","# Importing random library to generate random dataset\n","import random\n","import math"]},{"cell_type":"markdown","metadata":{"id":"V4WIZ1eOsLRN"},"source":["# <font color = 'pickle'>**nn.Module**\n","\n","nn.Module is a base class for all neural network modules in PyTorch. Your models should also subclass this class. This will help us to create a class that\n","holds our weights, biases. `nn.Module` has a\n","number of attributes and methods (such as `.parameters()` and `.zero_grad()`), which we will be using."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:04:47.959326Z","iopub.status.busy":"2022-09-25T18:04:47.959065Z","iopub.status.idle":"2022-09-25T18:04:47.962734Z","shell.execute_reply":"2022-09-25T18:04:47.962393Z","shell.execute_reply.started":"2022-09-25T18:04:47.959311Z"},"id":"HX_wx5JcuIVR"},"outputs":[],"source":["class LinearRegression(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.weights = nn.Parameter(torch.randn(self.output_dim, self.input_dim) / math.sqrt(2))\n","        self.biases = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        return x@self.weights.T + self.biases"]},{"cell_type":"markdown","metadata":{"id":"2MMMR8PQvN94"},"source":["Note that nn.Module objects are used as if they are functions (i.e they are callable), but behind the scenes Pytorch will call the forward method automatically."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:04:59.526725Z","iopub.status.busy":"2022-09-25T18:04:59.526239Z","iopub.status.idle":"2022-09-25T18:04:59.535177Z","shell.execute_reply":"2022-09-25T18:04:59.534690Z","shell.execute_reply.started":"2022-09-25T18:04:59.526709Z"},"executionInfo":{"elapsed":638,"status":"ok","timestamp":1664130350910,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"MdVgz3nnucF1","outputId":"bd443e5b-8852-4b74-e4df-551babc48114"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_tensor shape : torch.Size([3, 2])\n","output_tensor shape:  torch.Size([3, 1])\n"]}],"source":["x = torch.arange(6).view(3, 2).float()\n","\n","# Input Dimension\n","input_dim = 2\n","\n","# Output Dimension\n","output_dim = 1\n","\n","# Since we're now using an object instead of just using a function, we\n","# first have to instantiate our model\n","\n","model = LinearRegression(input_dim, output_dim)\n","\n","# Get the output of linear layer after transformation\n","output = model(x)\n","\n","print('input_tensor shape :', x.shape)\n","print('output_tensor shape: ', output.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:05:03.747724Z","iopub.status.busy":"2022-09-25T18:05:03.747484Z","iopub.status.idle":"2022-09-25T18:05:03.751196Z","shell.execute_reply":"2022-09-25T18:05:03.750844Z","shell.execute_reply.started":"2022-09-25T18:05:03.747709Z"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1664130350911,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"4bWveFdGwHW_","outputId":"9be07345-1670-493c-8920-6dab962f89d5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-1.0733, -0.5606]], requires_grad=True)"]},"metadata":{},"execution_count":6}],"source":["model.weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:05:04.690126Z","iopub.status.busy":"2022-09-25T18:05:04.689888Z","iopub.status.idle":"2022-09-25T18:05:04.693386Z","shell.execute_reply":"2022-09-25T18:05:04.692976Z","shell.execute_reply.started":"2022-09-25T18:05:04.690112Z"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1664130350911,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"fQvDV-IOwTIs","outputId":"226762a4-5704-4b47-d2df-8246cc35f6ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([0.], requires_grad=True)"]},"metadata":{},"execution_count":7}],"source":["model.biases"]},{"cell_type":"markdown","metadata":{"id":"nHAZSWsYdxmB"},"source":["# <font color = 'pickle'>**Linear Module (nn.Linear)**\n"]},{"cell_type":"markdown","metadata":{"id":"es0C8VTzif1a"},"source":["Instead of manually defining and\n","initializing parameter (weights and biases), and calculating `x @ self.weights.T + self.biases`, we wcan use the Pytorch class `nn.Linear`for a\n","linear layer, which does all that for us.\n","\n","This layer takes in dimensions of input and output features and applies the following transformation to the input tensor $x$\n","\n","$y = x w^T + b$ , \n","$w$ and $b$ are the parameters.\n","\n","The syntax for Linear Module is  :\n","`torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)`\n","\n","- in_features – size of each input sample\n","- out_features – size of each output sample\n","\n","Shapes :\n","\n","Input: $(N, *, H_{in})$ <br>\n","\n","here ,  $H_{in} = in\\_features$, ∗ means any number of additional dimensions and N is the batch size (number of observations). <br><br>\n","\n","Output: $(N ,*,  H_{out})$, \n","where all but the last dimension are the same shape as the input and $H_{out} = out\\_features$,\n","\n","\n","Example : \n","  - if input has shape(3, 2) (batch size is 3 and there are two features)\n","  and output = nn.Linear(in_features = 2, out_features =1) \n","  - then output will have the shape (3, 1) (3 observations and 1 feature).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:07:47.055288Z","iopub.status.busy":"2022-09-25T18:07:47.055064Z","iopub.status.idle":"2022-09-25T18:07:47.058486Z","shell.execute_reply":"2022-09-25T18:07:47.057961Z","shell.execute_reply.started":"2022-09-25T18:07:47.055273Z"},"id":"J2irVf6S5PIJ"},"outputs":[],"source":["class LinearRegression(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.linear_layer = nn.Linear(input_dim, output_dim)\n","\n","\n","    def forward(self, x):\n","        return self.linear_layer(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:11:31.878767Z","iopub.status.busy":"2022-09-25T18:11:31.878547Z","iopub.status.idle":"2022-09-25T18:11:31.882735Z","shell.execute_reply":"2022-09-25T18:11:31.882243Z","shell.execute_reply.started":"2022-09-25T18:11:31.878753Z"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1664130351466,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"FkoRciKGifYr","outputId":"907bfc56-7530-46f0-f104-28e589adc372","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["input_tensor shape : torch.Size([3, 2])\n","output_tensor shape:  torch.Size([3, 1])\n"]}],"source":["\n","x = torch.arange(6).view(3, 2).float()\n","\n","# Input Dimension\n","input_dim = 2\n","\n","# Output Dimension\n","output_dim = 1\n","\n","# Initialize first linear layer\n","model = nn.Linear(input_dim, output_dim)\n","# model = LinearRegression(input_dim, output_dim)\n","\n","# Get the output of linear layer after transformation\n","output = model(x)\n","\n","print('input_tensor shape :', x.shape)\n","print('output_tensor shape: ', output.shape)"]},{"cell_type":"markdown","metadata":{"id":"9tla6bCKlXlu"},"source":["We have not specified any initial weights or bias values.  Linear module automatically initializes the weights randomly based on the formulae given below: \n","\n","1. `Weights`: The learnable weights are from standard normal distribution (-${\\sqrt k}$, ${\\sqrt k}$) where k = 1 / input_dim.\n","\n","2. Bias : The learnable bias values are initialized from standard normal distribution (-${\\sqrt k}$, ${\\sqrt k}$) where k = 1 / output_dim.\n","\n","- This initilaization is also known as LeCun initialization (This is default for Linear Layer. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:11:33.078018Z","iopub.status.busy":"2022-09-25T18:11:33.077816Z","iopub.status.idle":"2022-09-25T18:11:33.081225Z","shell.execute_reply":"2022-09-25T18:11:33.080815Z","shell.execute_reply.started":"2022-09-25T18:11:33.078005Z"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1664130352191,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"OuQZrL7IFdSg","outputId":"9b9aae28-ea6f-4824-d0d9-e0cce9c54180","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["weight Parameter containing:\n","tensor([[ 0.0217, -0.6451]], requires_grad=True)\n","bias Parameter containing:\n","tensor([-0.6815], requires_grad=True)\n"]}],"source":["# We can get all the parameters associated with model(linear layer) as follows\n","for name, param in  model.named_parameters():\n","  print(name, param)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:11:33.964945Z","iopub.status.busy":"2022-09-25T18:11:33.964770Z","iopub.status.idle":"2022-09-25T18:11:33.968811Z","shell.execute_reply":"2022-09-25T18:11:33.968394Z","shell.execute_reply.started":"2022-09-25T18:11:33.964931Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1664130352192,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"VfiVv_fh-kSP","outputId":"18ca3fa7-2601-478e-fa45-51c12e4e84b3","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["We can see that PyTorch initializes  weights  in the background\n","\n","W: Parameter containing:\n","tensor([[ 0.0217, -0.6451]], requires_grad=True)\n","b: Parameter containing:\n","tensor([-0.6815], requires_grad=True)\n","Shape of W : torch.Size([1, 2])\n","Shape of b: torch.Size([1])\n"]}],"source":["print('We can see that PyTorch initializes  weights  in the background\\n')\n","print('W:', model.weight)\n","print('b:', model.bias)\n","print('Shape of W :', model.weight.data.shape)\n","print('Shape of b:', model.bias.data.shape)"]},{"cell_type":"markdown","metadata":{"id":"zxLN5mIU-3V-"},"source":["## <font color = 'pickle'>**Summary Linear Layer:**\n","\n","- When we initializes the layer (`layer = nn.Linear(input_dim, output_dim)`), Linear module takes the input and output dimensions as parameters, and automatically initializes the weights randomly.\n","\n","  - PyTorch sets the attribute requires_grad = True for weights and biases.\n","  - Shape of weights is [out_features, in_features]\n","  - Shape of bias is [out_features]\n","\n","- We can then apply this layer to inputs to get our output `(output = layer(input)`\n","  - It then uses randomly initilaized weights and biases to transform inputs. \n","\n","  - Shape of input = [batch_size, in_features]\n","  - output = input (W.T) + b\n","  - shape of output = [batch_size, out_features]\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1ewECT6hqC1sXd-TqXG3K1WZHAKXhY7g7\" width =700 >\n","\n","In the example above, the **output layer** would be `nn.Linear(2, 1)`. In the figure above, we have assumed a batch size of 1.\n"]},{"cell_type":"markdown","metadata":{"id":"UPv8zDkFBtKj"},"source":["# <font color = 'pickle'>**Sequential Module (nn.sequential)**\n","\n","Many times, we want to compose Modules together. `torch.nn.Sequential` provides a good interface to combine modules sequentially where the output of a module (layer) is sequentially fed as an input to the next layer. Consider the following network:\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1rymZGH-Xrp_1ywGAcRJraiuuAd-verg7\" width =700 >\n","\n","\n","In the example above, the **hidden layer** would be `nn.Linear(3, 4)` and the **output layer** would be `nn.Linear(4, 1)`. In the figure above, we have assumed a batch size of 1."]},{"cell_type":"markdown","metadata":{"id":"P3phWDi25PIL"},"source":["## <font color = 'pickle'>**Shallow NN with Custom Class**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:21:28.538529Z","iopub.status.busy":"2022-09-25T18:21:28.538366Z","iopub.status.idle":"2022-09-25T18:21:28.541726Z","shell.execute_reply":"2022-09-25T18:21:28.541260Z","shell.execute_reply.started":"2022-09-25T18:21:28.538516Z"},"tags":[],"id":"u_FVQDUv5PIL"},"outputs":[],"source":["class LinearRegression(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.output_dim = hidden_dim\n","        self.linear_layer1 = nn.Linear(input_dim, hidden_dim)\n","        self.linear_layer2 = nn.Linear(hidden_dim, output_dim)\n","\n","\n","    def forward(self, x):\n","        out1 = self.linear_layer1(x)\n","        out2 = self.linear_layer2(out1)\n","        return out2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:21:28.876204Z","iopub.status.busy":"2022-09-25T18:21:28.876033Z","iopub.status.idle":"2022-09-25T18:21:28.879789Z","shell.execute_reply":"2022-09-25T18:21:28.879347Z","shell.execute_reply.started":"2022-09-25T18:21:28.876191Z"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"5OYVGBrp5PIM","executionInfo":{"status":"ok","timestamp":1664130412962,"user_tz":300,"elapsed":265,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"8722f16d-31d6-4de6-c68e-a12825791094"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_tensor shape : torch.Size([5, 3])\n","output_tensor shape:  torch.Size([5, 1])\n"]}],"source":["# The code below illustrates above eample with batch size of 5\n","input_ =   torch.arange(15).view(5, 3).float()\n","model = LinearRegression(3,4 , 1)\n","output = model(input_)\n","\n","print('input_tensor shape :', input_.shape)\n","print('output_tensor shape: ', output.shape)"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:19:25.506650Z","iopub.status.busy":"2022-09-25T18:19:25.506452Z","iopub.status.idle":"2022-09-25T18:19:25.509214Z","shell.execute_reply":"2022-09-25T18:19:25.508864Z","shell.execute_reply.started":"2022-09-25T18:19:25.506636Z"},"tags":[],"id":"qlVx4pWn5PIM"},"source":["## <font color = 'pickle'>**Shallow NN with nn.Sequential**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:21:29.326177Z","iopub.status.busy":"2022-09-25T18:21:29.326003Z","iopub.status.idle":"2022-09-25T18:21:29.330135Z","shell.execute_reply":"2022-09-25T18:21:29.329735Z","shell.execute_reply.started":"2022-09-25T18:21:29.326163Z"},"executionInfo":{"elapsed":370,"status":"ok","timestamp":1664130424595,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"JyRZ0RjQBJ7R","outputId":"e560e3e8-075f-427c-9717-9e0713d7229b","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["input_tensor shape : torch.Size([5, 3])\n","output_tensor shape:  torch.Size([5, 1])\n"]}],"source":["# The code below illustrates above eample with batch size of 5\n","input_ =   torch.arange(15).view(5, 3).float()\n","hidden_layer = nn.Linear(3, 4)\n","output_layer = nn.Linear(4, 1)\n","model = nn.Sequential(hidden_layer, output_layer)\n","output = model(input_)\n","\n","print('input_tensor shape :', input_.shape)\n","print('output_tensor shape: ', output.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:21:29.713648Z","iopub.status.busy":"2022-09-25T18:21:29.713475Z","iopub.status.idle":"2022-09-25T18:21:29.716447Z","shell.execute_reply":"2022-09-25T18:21:29.716108Z","shell.execute_reply.started":"2022-09-25T18:21:29.713634Z"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1664130427062,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Csq8zhsFqXIO","outputId":"c5e47b6c-be52-431e-b4df-cbb2c7bdadb8","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Linear(in_features=3, out_features=4, bias=True)\n","  (1): Linear(in_features=4, out_features=1, bias=True)\n",")\n"]}],"source":["# print the model\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:21:29.888838Z","iopub.status.busy":"2022-09-25T18:21:29.888663Z","iopub.status.idle":"2022-09-25T18:21:29.897342Z","shell.execute_reply":"2022-09-25T18:21:29.896751Z","shell.execute_reply.started":"2022-09-25T18:21:29.888825Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1664130428421,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"nHZG5RUbpbr4","outputId":"0699932e-2b3e-4e37-b29b-e8eaf812446a","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                 [-1, 5, 4]              16\n","            Linear-2                 [-1, 5, 1]               5\n","================================================================\n","Total params: 21\n","Trainable params: 21\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.00\n","----------------------------------------------------------------\n"]}],"source":["# model summary\n","from torchsummary import summary\n","summary(model, input_size=(5,3) )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1664130430662,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"8Ty7WG2PxD3W","outputId":"e4a7f691-18e4-49ce-95dd-89b93123abb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.weight Parameter containing:\n","tensor([[ 0.0908,  0.5047,  0.2549],\n","        [ 0.1145, -0.1412,  0.1246],\n","        [-0.1499, -0.4533,  0.3863],\n","        [-0.5293, -0.0734,  0.2802]], requires_grad=True)\n","0.bias Parameter containing:\n","tensor([-0.2281, -0.1056,  0.1738, -0.3594], requires_grad=True)\n","1.weight Parameter containing:\n","tensor([[-0.0161,  0.3384, -0.4132, -0.2445]], requires_grad=True)\n","1.bias Parameter containing:\n","tensor([-0.4676], requires_grad=True)\n"]}],"source":["# We can get all the parameters associated with model(linear layer) as follows\n","for name, param in  model.named_parameters():\n","  print(name, param)"]},{"cell_type":"markdown","metadata":{"id":"dhUA3L8cGgAm"},"source":["# <font color = 'pickle'>**Custom initialization nn.init()**\n","Each layer in PyTorch has default initialization. We can chnage that using nn.init() module."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":259,"status":"ok","timestamp":1664130482126,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"8IcCjT2tHAyo","outputId":"0327afef-2720-41ec-8c21-e9c5be19292f"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_tensor shape : torch.Size([5, 3])\n","output_tensor shape:  torch.Size([5, 1])\n"]}],"source":["# The code below illustrates how we can specify default initilaization for each layer\n","input =   torch.arange(15).view(5, 3).float()\n","hidden_layer = nn.Linear(3, 4)\n","\n","torch.nn.init.normal_(hidden_layer.weight, mean = 0, std=0.01)\n","torch.nn.init.zeros_(hidden_layer.bias)\n","\n","output_layer = nn.Linear(4, 1)\n","torch.nn.init.normal_(output_layer.weight, mean = 0, std=0.01)\n","torch.nn.init.zeros_(output_layer.bias)\n","\n","model = nn.Sequential(hidden_layer, output_layer)\n","output = model(input)\n","\n","print('input_tensor shape :', input.shape)\n","print('output_tensor shape: ', output.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":488,"status":"ok","timestamp":1664130507647,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"EEgQtpbVMKO3","outputId":"821fdf90-37f7-4677-bd71-3de0ab4023e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.weight Parameter containing:\n","tensor([[ 0.0062, -0.0051,  0.0024],\n","        [ 0.0231,  0.0032, -0.0079],\n","        [ 0.0030, -0.0010, -0.0090],\n","        [ 0.0061,  0.0146,  0.0084]], requires_grad=True)\n","0.bias Parameter containing:\n","tensor([0., 0., 0., 0.], requires_grad=True)\n","1.weight Parameter containing:\n","tensor([[-0.0101, -0.0068, -0.0169, -0.0020]], requires_grad=True)\n","1.bias Parameter containing:\n","tensor([0.], requires_grad=True)\n"]}],"source":["# We can get all the parameters associated with model as follows\n","for name, param in  model.named_parameters():\n","  print(name, param)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1664130512309,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"61z5cksRNjzr","outputId":"50065848-f776-4e88-f684-cbd72f866e8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Linear(in_features=3, out_features=4, bias=True)\n","Linear(in_features=4, out_features=1, bias=True)\n"]}],"source":["for layer in model.children():\n","  print(layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFxZ01DFNvVV"},"outputs":[],"source":["# If we want to apply same initialization for all the layers we can do that \n","# using for loop\n","for layer in model:\n","  if isinstance(layer, nn.Linear):\n","    torch.nn.init.constant_(layer.weight, 5)\n","    torch.nn.init.zeros_(layer.bias)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1664130554747,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"smQscYjQOnff","outputId":"c99310ca-2183-4e78-ef20-d595c40b4532"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.weight tensor([[5., 5., 5.],\n","        [5., 5., 5.],\n","        [5., 5., 5.],\n","        [5., 5., 5.]])\n","0.bias tensor([0., 0., 0., 0.])\n","1.weight tensor([[5., 5., 5., 5.]])\n","1.bias tensor([0.])\n"]}],"source":["# Check the parameter values\n","for name, param in  model.named_parameters():\n","  print(name, param.data)"]},{"cell_type":"markdown","source":["## <font color = 'pickle'>**Custom initialization using apply function** </font>\n","\n","`apply()` function apply the initialization recursively. In complex models,layers will have sublayers. This will make sure that initialization is applied to sublayers layers as well."],"metadata":{"id":"yT78BJRsB7c4"}},{"cell_type":"code","source":["# Preferred Method\n","def init_weights(layer):\n","  if type(layer) == nn.Linear:\n","    torch.nn.init.normal(layer.weight, mean = 0, std = 0.05)\n","    torch.nn.init.zeros_(layer.bias)"],"metadata":{"id":"SK_my28D6oRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.apply(init_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbaiUlL367o7","executionInfo":{"status":"ok","timestamp":1664130740365,"user_tz":300,"elapsed":264,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"2437981c-8887-47fd-9a2a-316751ac1a45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n","  after removing the cwd from sys.path.\n"]},{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=3, out_features=4, bias=True)\n","  (1): Linear(in_features=4, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["# Check the parameter values\n","for name, param in  model.named_parameters():\n","  print(name, param.data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FN1YjR1-6zkP","executionInfo":{"status":"ok","timestamp":1664130742508,"user_tz":300,"elapsed":264,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"1af2a74d-5224-4698-ccb1-80f30245159c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.weight tensor([[-0.0206, -0.0087, -0.0083],\n","        [-0.0452, -0.0667, -0.0673],\n","        [-0.0507, -0.0312, -0.0406],\n","        [-0.0515,  0.0198,  0.0876]])\n","0.bias tensor([0., 0., 0., 0.])\n","1.weight tensor([[ 0.0029,  0.0317, -0.0181, -0.0537]])\n","1.bias tensor([0.])\n"]}]},{"cell_type":"markdown","metadata":{"id":"L5TQKmUksogN"},"source":["# <font color = 'pickle'>**Mean Squared Error Loss (nn.MSELoss())**\n","\n","PyTorch implements many common loss functions including `MSELoss` and `CrossEntropyLoss`. We will discuss `MSELoss()` in this lecture. We will explore `CrossEntropyLoss` in coming lectures.\n","\n","Supposedly our input and output is as follows:\n","\n","`x = [0, 1, 2, 3, 4]`\n","\n","`y = [1, 3, 5, 7, 9]`\n","\n","But our predicted output comes out with an error with equation `y = 2 * x`\n","\n","`ypred = [0, 2, 4, 6, 8] `\n","\n","Mean Squared Error (MSE) = $\\frac{\\sum_{i=1}^{n} (ypred_i  - y_i)^2} {n}$. Here, n = number of elements.\n","\n","For the above example, loss = 1.0\n","\n","Earlier we have written function to implement MSE. We can use nn.MSE() module from pytorch to calculate loss.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270,"status":"ok","timestamp":1664131100337,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"CcuYiBAGvxtf","outputId":"2ed16fcf-e13c-469c-cff1-b3bacfbe9de9"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.) tensor(1.) tensor(1.)\n"]}],"source":["# Instantiate Mean Squared Error loss function\n","def mse_loss(ypred, y):\n","  \"\"\"\n","  Squared error loss function.\n","  Input: actual labels and predicted labels\n","  Output: squared error loss\n","  \"\"\"\n","  error = ypred - y.view(ypred.shape)\n","  mean_squared_error = error.T@(error)/len(y)\n","  return mean_squared_error\n","\n","loss_nn = nn.MSELoss(reduction='mean')\n","loss_functional = F.mse_loss\n","\n","# when we specify reduction = 'mean' - this will give us mean sqaured loss\n","# if reduction = 'sum' - this will give us total squared loss\n","# reduction = 'mean' is the default\n","\n","# inputs\n","x = torch.Tensor([0, 1, 2, 3, 4])\n","y = torch.Tensor([1, 3, 5, 7, 9])\n","\n","# output\n","ypred = 2 * x\n","\n","# Calculating loss\n","# Loss function will take in 2 inputs: actual labels and predicted labels.\n","loss_manual = mse_loss(y, ypred)\n","loss_nn_module = loss_nn(y, ypred)\n","loss_functional = loss_functional(y, ypred)\n","print(loss_manual, loss_nn_module, loss_functional )"]},{"cell_type":"markdown","source":["# <font color = 'pickle'>**Cross Entropy Loss**"],"metadata":{"id":"Q1Ibnd368WqF"}},{"cell_type":"markdown","source":["**Let us summarize Softmax Regression Model:**\n","\n","- **Input**: Features X, shape: (n x d)\n","  - n : number of examples.\n","  - d : number of features in each example.\n","- **Output**: Labels y = {1, 2....K}, shape: (n x K) \n","\n","- **Parameters**: Weights w, shape: (K x d) and bias b, dimension: (K)\n","\n","- **Forward pass**\n","\n","$$o_k^{(i)}  = \\mathbf{x^{(i)}}\\mathbf{w_k} ^T+b_k$$\n","\n","$$\\hat{p_k}^{(i)} = softmax(o_k^{(i)}) = \\frac{e^{o_k^{(i)}}}{\\sum_{j=1}^{K} e^{o_j^{(i)}}}$$\n","\n","\n","\n","\n","<img src = \"https://drive.google.com/uc?export=view&id=1JZ2cNVX2Cs3v-MhKNcE2jvLjzny9QHW1\" width =600 >\n","\n","- **Cross Entropy Loss Function (assuming a batch size of two)**: \n","\\begin{equation}\n","\\mathcal{L} = -\\frac{1}{m} \\sum_{k=1}^{K} \\sum_{i=1}^{m} \\bigg[y_k^{(i)}log(\\hat{p_k}^{(i)}) \\bigg]\n","\\end{equation}\n","\n","<img src = \"https://drive.google.com/uc?export=view&id=1YFkNGN_x1lETidVDNLaSZ4ndwZv9Wc2T\" width =600 >"],"metadata":{"id":"Dhb1OADo95mY"}},{"cell_type":"markdown","source":["## <font color = 'pickle'>**Create a function for Cross Entropy**"],"metadata":{"id":"TrnWNMvp-qB4"}},{"cell_type":"code","source":["def cross_entropy(outcome, y):\n","  numerator = torch.exp(Output)\n","  denominator = numerator.sum(axis = 1, keepdim=True)\n","  softmax = numerator / denominator\n","  p_hat = softmax\n","  return -torch.log(p_hat[range(len(p_hat)), y]).mean()"],"metadata":{"id":"TJoiL85G9Cte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Output = torch.Tensor([[1.2, -0.8, 0.7, -2.4], [0.1, 0.3, -0.3, 2.4]])\n","y = torch.tensor([0, 1])\n","print(cross_entropy(Output, y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvJWVm4g9jE9","executionInfo":{"status":"ok","timestamp":1664131592647,"user_tz":300,"elapsed":281,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"3dbca5cb-b031-4240-de86-e771c65fcb9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.4626)\n"]}]},{"cell_type":"markdown","source":["## <font color = 'pickle'>**nn.CrossEntropyLoss()**\n","\n","We have to take a log of softmax and then oass it to negative log likelihood loss. nn.CrossEntropyLoss combines these two steps into one step."],"metadata":{"id":"r29Cmtja-dZ3"}},{"cell_type":"code","source":["logsoftmax = nn.LogSoftmax(dim =1)(Output)\n","negative_log_likelihood_loss = nn.NLLLoss()(logsoftmax, y)\n","print(negative_log_likelihood_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JTmTq7uy-2Ow","executionInfo":{"status":"ok","timestamp":1664132277414,"user_tz":300,"elapsed":379,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"ebb3c6c4-0b5c-4e26-f9a7-d10790a90b8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.4626)\n"]}]},{"cell_type":"code","source":["loss = nn.CrossEntropyLoss()"],"metadata":{"id":"vCEgNUFU-O48"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(loss(Output, y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G45eGF8R-VJA","executionInfo":{"status":"ok","timestamp":1664131753690,"user_tz":300,"elapsed":268,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"36270c8e-3521-4bae-bb07-fbedaa11ad0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.4626)\n"]}]},{"cell_type":"markdown","metadata":{"id":"NAcpc-BCPnKG"},"source":["# <font color = 'pickle'>**torch.optim**\n","We can implement number of gradient-based optimization methods using `torch.optim`. **SGD (Stochastic Gradient Descent)** is the most basic of them and **Adam** is one of the most popular. We will use SGD in this notebook and cover other optimizers in a later lecture.\n","\n","An optimizer takes the **model parameters** we want to update (learnable parameters), and the **learning rate**  (and some other hyper-parameters as well).\n","\n","Optimizers do not compute the gradients on their own, we need to call **backward()** on the loss first.\n","\n","We can then use optimizer's **step()** mehod to update the model parameters.\n","\n","Further, we do no not need to zero the gradients one by one. We can invoke the optimizer’s **zero_grad()** method.\n","\n","This does  `zero_()` call on all learnable parametets of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":276,"status":"ok","timestamp":1664132422416,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"pE0NfqSaW-eF","outputId":"1f68b624-fb57-4b4f-d17d-6c210173ca45"},"outputs":[{"output_type":"stream","name":"stdout","text":["model params before weight update: tensor([[ 0.2046,  0.0729, -0.3445]]) tensor([-0.2338])\n","model params after weight update: tensor([[ 0.2684,  0.2642, -0.0895]]) tensor([-0.1700])\n"]}],"source":["# create a simple model\n","model = nn.Linear(3, 1)\n","\n","# create a simple dataset\n","X = torch.tensor([[1., 3., 4.]])\n","y = torch.tensor([[2.]])\n","\n","# create our optimizer\n","optim = torch.optim.SGD(model.parameters(), lr=1e-2)\n","\n","# loss function \n","criterion = nn.MSELoss()\n","\n","y_hat = model(X)\n","\n","print('model params before weight update:', model.weight.data, model.bias.data)\n","\n","# calculate loss\n","loss = criterion(y_hat, y)\n","\n","# reset gradients to zero\n","optim.zero_grad()\n","\n","# calculate gradients\n","loss.backward()\n","\n","# update weights\n","optim.step()\n","\n","\n","print('model params after weight update:', model.weight.data, model.bias.data)\n"]},{"cell_type":"markdown","metadata":{"id":"Ra-i09LpZF5X"},"source":["# <font color = 'pickle'>**Dataset and Dataloader**\n","\n","When we train our model, we typically\n","\n","  - want to process the data in batches \n","  - reshuffle the data at every epoch to reduce model overfitting, \n","  - and use Python’s multiprocessing to speed up data retrieval.\n","\n","Earlier we wrote a function to create an iterator, that will shuffle the data and yield batches of data. However, we can do this much more efficently using **torch.utils.data.DataLoader**, which is an iterator that provides all the above features.\n","\n","The most important argument of DataLoader constructor is dataset, which is a PyTorch Dataset. Pytorch **Dataset** is a regular **Python class** that inherits from the [**Dataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class. \n","\n","If a dataset consists of tensors of lables and features, we can use PyTorch’s [**TensorDataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class to wrap tensors in a Dataset class.\n","\n","If the **dataset is big** (tens of thousands of image/text files, for instance), loading it at once would not be memory efficient. In that case we will need to create  custom dataset class , that load the files\\examples on demand. We will demonstrate how to create a CustomDataset that inherits from PyTorch's Dataset class later. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1664132495879,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Rl2sL5gjbIEL","outputId":"6289575f-9235-417e-ac3d-9aa00abd2673"},"outputs":[{"output_type":"stream","name":"stdout","text":["x:tensor([[0., 1.],\n","        [2., 3.],\n","        [4., 5.],\n","        [6., 7.],\n","        [8., 9.]])\n","\n","y: tensor([[ 4.],\n","        [14.],\n","        [24.],\n","        [34.],\n","        [44.]])\n"]}],"source":["# Generate Dataset\n","x = torch.arange(10).view(5, 2)\n","x = x.type(dtype = torch.float)\n","w = torch.Tensor([2, 3]).view(-1, 1)\n","y = x.mm(w) + 1\n","print(f'x:{x}' )\n","print(f'\\ny: {y}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nm3CYK_2XwVN"},"outputs":[],"source":["# Create Dataset \n","dataset = data.TensorDataset(x, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxtqqOwVa3VN"},"outputs":[],"source":["# Create DataLoader\n","data_iter = data.DataLoader(dataset, batch_size= 2, shuffle= True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1664132505110,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"i5W6JDNubflG","outputId":"293fd41e-bf4c-49f1-f92e-01b229cad79a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1\n","\n","Batch Number 1\n","x:tensor([[6., 7.],\n","        [0., 1.]])\n","y: tensor([[34.],\n","        [ 4.]])\n","\n","Batch Number 2\n","x:tensor([[4., 5.],\n","        [2., 3.]])\n","y: tensor([[24.],\n","        [14.]])\n","\n","Batch Number 3\n","x:tensor([[8., 9.]])\n","y: tensor([[44.]])\n","\n","\n","Epoch 2\n","\n","Batch Number 1\n","x:tensor([[2., 3.],\n","        [8., 9.]])\n","y: tensor([[14.],\n","        [44.]])\n","\n","Batch Number 2\n","x:tensor([[4., 5.],\n","        [0., 1.]])\n","y: tensor([[24.],\n","        [ 4.]])\n","\n","Batch Number 3\n","x:tensor([[6., 7.]])\n","y: tensor([[34.]])\n","\n","\n","Epoch 3\n","\n","Batch Number 1\n","x:tensor([[0., 1.],\n","        [2., 3.]])\n","y: tensor([[ 4.],\n","        [14.]])\n","\n","Batch Number 2\n","x:tensor([[8., 9.],\n","        [6., 7.]])\n","y: tensor([[44.],\n","        [34.]])\n","\n","Batch Number 3\n","x:tensor([[4., 5.]])\n","y: tensor([[24.]])\n","\n"]}],"source":["# We can loop over the DataLoader object to get batch of observations\n","\n","for epoch in range(3):\n","  print(f'\\nEpoch {epoch + 1}\\n')\n","  for i, (x, y) in enumerate(data_iter):\n","    print(f'Batch Number {i+1}')\n","    print(f'x:{x}' )\n","    print(f'y: {y}\\n')"]},{"cell_type":"markdown","metadata":{"id":"lxO8HHxOlM9b"},"source":["We can obseve that in every epoch, an obsetvation is a part of a different batch. This happens as DataLoader shuffles the dataset to create batches."]},{"cell_type":"code","source":[],"metadata":{"id":"etE9SbKb5kW5"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}