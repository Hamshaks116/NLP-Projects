{"cells":[{"cell_type":"markdown","metadata":{"id":"bfqUx6gaBTwq"},"source":["# **Lecture_2_2. Linear Regression from scratch using gradient descent**"]},{"cell_type":"markdown","metadata":{"id":"974rsdI0BksG"},"source":["Linear regression is one of the most popular tools for regression models.\n","\n","Let us consider an example where we have to estimate the price of the house based on its area and age. To actually train a model, we would need a dataset of different house prices along with their area and age. \n","\n","In this dataset, area and age will be know as our `features` using which we will estimate the price. The price or the predicted value will be known as `label`.\n","\n","We can use linear regression model for such a problem. Let's see what actually happens in linear regression and how we can implement it."]},{"cell_type":"markdown","metadata":{"id":"GSj1swM9EWCC"},"source":["## **Linear Regression**\n","\n","\n","\n","\n","In this model, we have some independent variable values x (area and age in case of house price prediction) and a dependent variable y (price of the house) which is linearly dependent on x values. \n","\n","We try to generalize y as a weigthed sum of elemnts in x with some noise on the onservations.\n","\n","$price = weight_{area} * area + weight_{age} * age + b$\n","\n","Here b is the **bias**. The weights determine the influence of each feature on our prediction and the bias tells us what value the predicted price should take when all of the features take value 0. \n","\n","## **Loss Function**\n","\n","We also need to compute loss. We use sum of squared error as our ***loss function***\n","\n","## **Optimization**\n","\n","We also need an **optimization algorithm** so that we can minimize our loss function. We will be using gradient descent."]},{"cell_type":"markdown","metadata":{"id":"MbiPmm5tK47-"},"source":["**Let us summarize above points for linear regression:**\n","\n","**Input**: $\\boldsymbol{X}$ (features)\n","\n","**Output**: $\\mathbf{y}$ (label)\n","\n","**Parameters**: $\\mathbf{w}$, b (weights and bias)\n","\n","\n","The training examples are stored in matrix $\\boldsymbol{X}$ row-wise <br><br>\n","$\\boldsymbol{X} =  \\begin{bmatrix}\n","x_1^{(1)}x_2^{(1)}\\cdot\\cdot\\cdot x_n^{(1)} \\\\ x_1^{(2)}x_2^{(2)}\\cdot\\cdot\\cdot x_n^{(2)}  \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot\\ \\\\ x_1^{(m)}x_2^{(m)}\\cdot\\cdot\\cdot x_n^{(m)} \\end{bmatrix}$ , $ \\mathbf{w} =\\begin{bmatrix}\n","w_1  w_2  \\cdot  \\cdot  \\cdot  w_n\\end{bmatrix}$\n","\n","We can calculale the predicted values of y for all m observations using follwoing:\n","\n","\\begin{equation}\n","\\hat{\\mathbf{y}}  = \\boldsymbol{X} \\mathbf{w^T} + b\n","\\end{equation}\n","where $\\boldsymbol{X} \\mathbf{w^T}$ is matrix multiplication of $\\boldsymbol{X}$ and $\\mathbf{w^T}$ \n","\n","<b>Cost function</b>\n","\\begin{equation}\n","\\mathcal{L} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 \n","\\end{equation}\n","\n","Cost Function (Vector Notation):\n","\\begin{equation}\n","\\mathcal{L}  = \\frac{1}{m}(\\mathbf{\\hat{y}}  -\\mathbf{y})^T (\\mathbf{\\hat{y}}  -\\mathbf{y})\n","\\end{equation}\n","\n","Gradient of thr Loss Function with respect to $\\mathbf{w}$:\n","\\begin{equation}\n","\\frac{2}{m}(\\mathbf{\\hat{y}}  -\\mathbf{y})^T X\n","\\end{equation}\n","\n","Gradient of the Loss Function with respect to $b$:\n","\\begin{equation}\n","\\frac{2}{m}\\mathbf{1_m}^T(\\mathbf{\\hat{y}}  -\\mathbf{y}))\n","\\end{equation}\n","\n","here $\\mathbf{1_m}$ denote the column vector of all ones and length m\n","\n","Gradient Descent Algorithm:\n","\n","The algorithm starts with some “initial guess” for $\\mathbf{w}$ and $b$, and that repeatedly changes $\\mathbf{w}$ and $b$ to make  $\\mathcal{L}$  smaller, until we converge to a value of $\\mathbf{w}$ and $b$ minimizes $\\mathcal{L}$.\n","\n","\\begin{equation}\n","\\mathbf{w^T} := \\mathbf{w^T} - \\frac{2\\alpha} {m}\\boldsymbol{X}^T(\\mathbf{\\hat{y}}  -\\mathbf{y})\n","\\end\\{equation}\n","\n","\n","\\begin{equation}\n","b := b - \\frac{2\\alpha} {m}\\mathbf{1_m}^T(\\mathbf{\\hat{y}}  -\\mathbf{y})\n","\\end\\{equation}\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mdSv1RAqGkbn"},"source":["**We will implement linear regression algorithm from sratch in this notebook using the above formulas.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOWkKIpaNaQa"},"outputs":[],"source":["# Importing PyTorch Library\n","import torch\n","\n","# Importing random library to generate random dataset\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZPys7ECalFv"},"outputs":[],"source":["# To get deterministic results\n","torch.manual_seed(456)\n","random.seed(123)"]},{"cell_type":"markdown","metadata":{"id":"kpeEa2UsCzbA"},"source":["We will be using W&B for visualization."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8560,"status":"ok","timestamp":1661787140116,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"koDMsk5eB0iL","outputId":"ff9e6faa-be9e-4a1a-f208-7a076700da83"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 9.0 MB/s \n","\u001b[?25hCollecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 69.1 MB/s \n","\u001b[?25hCollecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 68.3 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 76.2 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 77.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 80.1 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 72.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 77.8 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=90f940b906ee6333099b4cb6c023f7ec7b71b1b84ef03d31d02cdcbc2bc595e1\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n"]}],"source":["# Install wandb and update it to the latest version\n","!pip install wandb --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"elapsed":4756,"status":"ok","timestamp":1661787147735,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"HkrK5NccB0ZZ","outputId":"01e44439-bc63-461b-baca-4f76bedb53a0"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["# Import wandb\n","import wandb\n","\n","# Login to W&B\n","wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"sTFUkNsGC4H9"},"source":["Initialize a new project and run in wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":973,"status":"ok","timestamp":1661787182812,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"6T0gYkXsB0O0","outputId":"8b073548-f6f9-42b0-ab32-6d88bd848b9b"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhsingh-utd\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20220829_153305-3iba296a</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/hsingh-utd/dl22_l2/runs/3iba296a\" target=\"_blank\">L_2_LR</a></strong> to <a href=\"https://wandb.ai/hsingh-utd/dl22_l2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/hsingh-utd/dl22_l2/runs/3iba296a?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fd0df1ff050>"]},"metadata":{},"execution_count":5}],"source":["# we have specified the id for our run and set resume = None\n","# if we restart the run it will overwrite teh existing run\n","# If you want to resume runs then resume shoule be 'allow\n","wandb.init(name = \"L_2_LR\", project = 'dl22_l2')"]},{"cell_type":"markdown","metadata":{"id":"ZrkSdaYYK3ed"},"source":["## **Generating a Dataset**\n","\n","We will generate a dummy dataset having 1000 observations and 2 features.\n","The observations are sampled from standard normal distribution.\n","\n","Let us have our true parameter values to be w = [3, -4.5] and b = 5.2. \n","\n","`y = Xw.T + b + noise`\n","\n","We will further assume that nose will be normally distributed with mean 0 and standard deviation of 0.01. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWCK1HgYBKuT"},"outputs":[],"source":["def generate_dataset(w, b, num): \n","\n","    \"\"\"\n","    Function to generate a dataset. \n","    Input parameter : \n","    w: weights, \n","    b: bias \n","    num: number of obervations\n","    Output: feature and labels of a dataset\n","    \"\"\"\n","    \n","    # Generate X values from standard normal distribution \n","    X = torch.normal(0, 1, (num, len(w.T)))\n","\n","    # Generate y values: y = Xw + b\n","    y = torch.mm(X, w.T) + b\n","\n","    # Adding noise in labels\n","    \n","    y += torch.normal(0, 0.01, y.shape)\n","\n","    # Returning the dataset generated\n","    return X, y.view((-1, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-K-9_TIO05y"},"outputs":[],"source":["# Initializing actual weight and bias values\n","w_true = torch.Tensor([3, -4.5]).view(1,-1)\n","b_true = 5.2\n","\n","# Calling the generate_dataset function to create a dummy dataset\n","features, labels = generate_dataset(w_true, b_true, 1000)\n","\n","wandb.log({'plots/features': wandb.Histogram(features)})\n","wandb.log({'plots/labels': wandb.Histogram(labels)})"]},{"cell_type":"markdown","metadata":{"id":"xbYjEiosdZpY"},"source":["## **Visualization of the dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArmLQ7f2PbwW"},"outputs":[],"source":["# Importing matplotlib library for visualizing dataset\n","import matplotlib.pyplot as plt\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n","fig.suptitle('Sactter Plots')\n","\n","# We will create a scatter plot between feature and labels\n","ax1.scatter(features[:, 0].detach().numpy(), labels.detach().numpy(),s = 1)\n","ax1.set(xlabel='X1')\n","ax1.set(ylabel='y')\n","ax2.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), s = 1)\n","ax2.set(xlabel='X2')\n","ax2.set(ylabel='y')\n","\n","wandb.log({\"scatter plot \": plt})"]},{"cell_type":"markdown","metadata":{"id":"d3ZgzLGyd5sZ"},"source":["From the above visualization, we can observe a linear relation between features and label values."]},{"cell_type":"markdown","metadata":{"id":"py50Z3VUegX2"},"source":["## **Reading the dataset**\n","\n","For training our model, we will use mini batches from the dataset and use them to update our model.\n","\n","To simplify this process, we will define a function which will shuffle the dataset and access it in mini batches."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6dCBmjFPzZ9"},"outputs":[],"source":["def read_data(batch_size, features, labels):\n","    \"\"\"\n","    Function to access data in mini batches. \n","    Input parameters: batch size, features, labels. \n","    Output: iterator for minibatch data\n","    \"\"\"\n","    \n","    # Number of examples in the datasets\n","    num = len(features)\n","\n","    # Creating a list of indices from 0 to number of examples in the dataset\n","    indices = list(range(num))\n","\n","    # The examples are read at random, in no particular order\n","    random.shuffle(indices)\n","\n","    # Return the features and labels of the minibatch\n","    for i in range(0, num, batch_size):\n","        batch_indices = torch.tensor(indices[i:min(i + batch_size, num)])\n","        yield features[batch_indices], labels[batch_indices]"]},{"cell_type":"markdown","metadata":{"id":"tql9LWY5gKE6"},"source":["Lets create a batch size of 15 and visualize the feature and data values better understanding."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188,"status":"ok","timestamp":1661787264170,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"_EltQ1kVgCOa","outputId":"a4951783-d26c-4fb4-c68f-706dc77e175a"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([15, 2])\n","torch.Size([15, 1])\n"]}],"source":["# Specify value for batch_size\n","batch_size = 15\n","\n","# Call the function read_data and visualize the size of minibatch\n","for X, y in read_data(batch_size, features, labels):\n","    print(X.shape)\n","    print(y.shape)\n","    break"]},{"cell_type":"markdown","metadata":{"id":"1VEzdCYMiSRW"},"source":["## **Linear Regression Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wh15a3SliM9O"},"outputs":[],"source":["def linear_reg(X, w, b):\n","  \"\"\"\n","  Function for linear regression y = Xw + b. \n","  Input parameter: features, weights, bias. \n","  Output: predicted labels\n","  \"\"\"\n","  return torch.mm(X,w.T) + b"]},{"cell_type":"markdown","metadata":{"id":"LSz5VgDKipzE"},"source":["## **Loss Function**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SyBkoaKMinTM"},"outputs":[],"source":["def mse_loss(ypred, y):\n","  \"\"\"\n","  Squared error loss function.\n","  Input: actual labels and predicted labels\n","  Output: squared error loss\n","  \"\"\"\n","  error = ypred - y.view(ypred.shape)\n","  mean_squared_error = error.T.mm(error)/len(y)\n","  return mean_squared_error"]},{"cell_type":"markdown","metadata":{"id":"jRERgZ3njKOc"},"source":["## **Optimization Algorithm**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWaPVEVQjHq2"},"outputs":[],"source":["# weight update step\n","def sgd(params, params_grad, lr):\n","    for param, param_grad in zip(params, params_grad):\n","        param -= lr * param_grad\n","        "]},{"cell_type":"markdown","metadata":{"id":"CimqrDwmk6of"},"source":["## **Model Training**\n","\n","**Model Training** involves five steps: \n","\n","- Step 0: Randomly initialize parameters / weights\n","- Step 1: Compute model's predictions - forward pass\n","- Step 2: Compute loss\n","- Step 3: Compute the gradients\n","- Step 4: Update the parameters\n","- Step 5: Repeat steps 1 - 4\n","\n","Model training is repeating this process over and over, for many **epochs**.\n","\n","An **epoch** is complete when we have used every point once for for computing the loss\n","\n","We will use min-batch gardient descent. In minibatch gradient descent , we take a small batch of the data. We compute the loss for the batch and update the weights. **If there are $n$ batches, then in one epoch weights get updated $n$ times**. \n","\n","***Learning rate*** and ***epochs*** are known as hyperparameters. We have to fine tune the value of hy[erparametes using ceoss-validation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":694,"status":"ok","timestamp":1661787279787,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"TbHTIh3QgqIV","outputId":"cf0b1230-22bb-4d4e-c34f-c838e1053e62"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.0104, -0.0055]])\n"]}],"source":["# Step 0: \n","# Create a tensor weights with values from normal distribution\n","# We are calculating gardients manually\n","# Thus, we will set requires_grad to False\n","\n","w = torch.normal(0, 0.01, size=(1, 2), requires_grad=False)\n","\n","# Create a tensor for bias\n","b = torch.zeros(1, requires_grad=False)\n","print(w)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1661787297850,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Ee_qHnu7lAEw","outputId":"521c2fd1-8d73-4780-d559-7f17cfc7b1b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1, loss 15.022364\n","epoch 2, loss 4.405335\n","epoch 3, loss 1.294533\n","epoch 4, loss 0.377739\n","epoch 5, loss 0.111390\n","epoch 6, loss 0.032954\n","epoch 7, loss 0.009864\n","epoch 8, loss 0.003008\n","epoch 9, loss 0.000970\n","epoch 10, loss 0.000364\n"]}],"source":["# Set the values for learning rate and number of epochs\n","lr = 0.005\n","epochs = 10\n","\n","# Iterate over the whole dataset\n","for epoch in range(epochs):\n","    \n","    # Iterate over mini batch\n","    for X, y in read_data(batch_size, features, labels):\n","\n","        # step 1 :forward pass - compute predictions\n","        ypred = linear_reg(X, w, b)\n","\n","        # step 2: Calculate minibatch loss\n","        batch_loss = mse_loss(ypred, y)\n","        \n","\n","        # step 3: Compute gradient on loss with respect to weights and bias\n","        grad_w = 2 *X.T.mm(ypred-y)/len(y)\n","        grad_b = 2 *(ypred-y).sum()/len(y)\n","\n","        # step 4: Update parameters using their gradient using optimization algorithm\n","        sgd([w.T, b], [grad_w, grad_b], lr)  \n","\n","    \n","    # Calculate and print loss for the complete epoch\n","    train_l = mse_loss(linear_reg(features, w, b), labels)\n","\n","    # We can observe the epoch vs loss curve in W&B\n","    wandb.log({\"/Loss_2_2\": train_l.item()})\n","        \n","    print(f'epoch {epoch + 1}, loss {float(train_l.item()):f}')"]},{"cell_type":"markdown","metadata":{"id":"JAV28C1iCs3L"},"source":["We can observe that with each epoch, our loss is getting reduced hence our linear regression model is able to classify accurately.\n","\n","Now since we generated the dataset ourselves we know the actual values for weights and bias, so we can check the error in both of them."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":380,"status":"ok","timestamp":1661787301902,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"rov0b8L_pMzx","outputId":"7155ee05-90b2-4715-d22c-b323586528ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Error in estimating w: tensor([[ 0.0080,  7.4879],\n","        [-7.4920, -0.0121]])\n","Error in estimating b: tensor([0.0088])\n","estimated value of w: tensor([[ 2.9920, -4.4879]])\n","estimated value of b: tensor([5.1912])\n"]}],"source":["# Printing error in weights and bias\n","print(f'Error in estimating w: {w_true.view(-1,1) - w}')\n","print(f'Error in estimating b: {b_true - b}')\n","print(f'estimated value of w: {w}')\n","print(f'estimated value of b: {b}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206,"referenced_widgets":["fd9fa6f615b44df890a094b7aac82892","a54dc43feee2449896503c7e23a10a6e","274bc8d5f14a4581b2d3925444514e74","419717aa5f3e49028ca7f2c81c7b9252","34d8005d3d2c4c82be0258c7fc75826e","8ceafb9d904c4c34868e46b1c881b7b5","9fd20b52dfca4ea8bce1d554d0d647cd","11408d5b56684d04bfa76ca2ff479ab5"]},"executionInfo":{"elapsed":4768,"status":"ok","timestamp":1661787308993,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"KnJh9XhYiW9s","outputId":"48b79f9b-ddf4-44b9-84d3-c6d91c9eb4f7"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.086 MB of 0.086 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd9fa6f615b44df890a094b7aac82892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>/Loss_2_2</td><td>█▃▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>/Loss_2_2</td><td>0.00036</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">L_2_LR</strong>: <a href=\"https://wandb.ai/hsingh-utd/dl22_l2/runs/3iba296a\" target=\"_blank\">https://wandb.ai/hsingh-utd/dl22_l2/runs/3iba296a</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20220829_153305-3iba296a/logs</code>"]},"metadata":{}}],"source":["wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"6WYKjaT1DiNz"},"source":["We can see that the error is minimal thus, our model is working well."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fd9fa6f615b44df890a094b7aac82892":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_a54dc43feee2449896503c7e23a10a6e","IPY_MODEL_274bc8d5f14a4581b2d3925444514e74"],"layout":"IPY_MODEL_419717aa5f3e49028ca7f2c81c7b9252"}},"a54dc43feee2449896503c7e23a10a6e":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34d8005d3d2c4c82be0258c7fc75826e","placeholder":"​","style":"IPY_MODEL_8ceafb9d904c4c34868e46b1c881b7b5","value":"0.098 MB of 0.098 MB uploaded (0.000 MB deduped)\r"}},"274bc8d5f14a4581b2d3925444514e74":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fd20b52dfca4ea8bce1d554d0d647cd","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11408d5b56684d04bfa76ca2ff479ab5","value":1}},"419717aa5f3e49028ca7f2c81c7b9252":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34d8005d3d2c4c82be0258c7fc75826e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ceafb9d904c4c34868e46b1c881b7b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fd20b52dfca4ea8bce1d554d0d647cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11408d5b56684d04bfa76ca2ff479ab5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}