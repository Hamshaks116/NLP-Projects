{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dN0i_G5cwP2b"},"source":["# <font color = 'pickle'>**Lecture 1.2. Linear Algebra Pytorch**"]},{"cell_type":"markdown","metadata":{"id":"18kaLMD6wo6s"},"source":["We can perform basic algebraic functions like, addition, subtraction, multiplication, division using pytorch.\n","\n","But before moving on to that, we have to get a basic understanding about some common terms such as scalars and vectors.\n"]},{"cell_type":"markdown","metadata":{"id":"g39El4Q6w90p"},"source":["## <font color = 'pickle'>**Scalars**\n","\n","Scalars are values consisting of just one numerical quantity. Let us consider an example of scalers. If we go to some restaurant, our bill would be charged by adding the price of all items (p) that we had along with some service charge. Let us consider the service charge is 5%. So we can represent our total bill (y) as:\n","\n","    y = p + 0.05*p\n","\n","Now, in this equation, 0.05 is a `scalar value`. The placeholders y and p are know as variables as they represent unknown scalar values.\n","\n"]},{"cell_type":"code","metadata":{"id":"NuWxAt4NwL31"},"source":["# Import PyTorch Library\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VnCC-gURshYg"},"source":["We can create a scalar using a 1-d Tensor of size 1 as shown below.\n","\n","Let's create some tensors and perform basic algebraic functions on them. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzN9p6osxM45","executionInfo":{"status":"ok","timestamp":1661146297032,"user_tz":300,"elapsed":361,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"5a3cc543-3b37-44c9-f1e9-c89d9c453369"},"source":["# Creating 2 scalars using tensors\n","t1 = torch.Tensor([10.0])\n","t2 = torch.Tensor([2.0])\n","\n","# Addition : ans = 12\n","print(t1 + t2)\n","\n","#Subtraction : ans = 8\n","print(t1 - t2)\n","\n","# Multiplication : ans = 20\n","print(t1 * t2)\n","\n","# Division : ans = 5\n","print(t1 / t2)\n","\n","# Power function : ans = 10^2 = 100\n","print(t1 ** t2)\n","\n","# Exponentiation\n","print(torch.exp(t1))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([12.])\n","tensor([8.])\n","tensor([20.])\n","tensor([5.])\n","tensor([100.])\n","tensor([22026.4648])\n"]}]},{"cell_type":"markdown","metadata":{"id":"wn6CGyJWtyAB"},"source":["## <font color = 'pickle'>**Vectors**\n","\n","Vectors are basically a list of scalar elements. They are used to represent real world datasets. For example, if we have to represent marks of a student in 5 subjects we can write them in a sequence as [98, 95, 96, 94, 92]. \n","\n","\n","Thus, vectors are needed to represent real-world entities.\n","\n","Precisely, a vector $\\mathbf{x}$ can be written as\n","\n","$$\\mathbf{y} =\\begin{bmatrix}y_{1}  \\\\y_{2}  \\\\ \\vdots  \\\\y_{n}\\end{bmatrix}$$\n","\n","Vectors are a 1-d tensors with size n.\n","\n","Let us create some vectors using tensors and perform some basic operations on them.\n","\n"]},{"cell_type":"code","metadata":{"id":"JtUOUATOtbTU"},"source":["# Creating 2 vectors using tensors\n","v1 = torch.Tensor([2, 5, 7])\n","v2 = torch.Tensor([3, 6, 8])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATmwMINRv0xp","executionInfo":{"status":"ok","timestamp":1661146299577,"user_tz":300,"elapsed":157,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"dfa19c53-6ed0-476f-db35-6302703269e6"},"source":["# Elements can be accessed using index\n","print(v1[2])\n","\n","# Length of the vector\n","print(len(v1))\n","\n","# Shape of the vector\n","print(v1.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(7.)\n","3\n","torch.Size([3])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wahwL-xNwIPM","executionInfo":{"status":"ok","timestamp":1661146303336,"user_tz":300,"elapsed":111,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"5d376dd4-56e4-4d32-926f-44358c25983f"},"source":["# Addition of 2 vectors\n","print(v1 + v2)\n","\n","# Subtraction of 2 vectors\n","print(v2 - v1)\n","\n","# Exponentiation (Element-wise)\n","print(torch.exp(v1))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 5., 11., 15.])\n","tensor([1., 1., 1.])\n","tensor([   7.3891,  148.4132, 1096.6332])\n"]}]},{"cell_type":"markdown","metadata":{"id":"sineKt2Qw2b3"},"source":["## <font color = 'pickle'>**Matrices/Tensors**\n"," \n","Matrices are 2-d arrays with size `n x m`. Here, n: number of rows and m: number of columns.\n","\n","If `m = n`, then the matrix is known as a `square matrix`.\n","\n","Precisely, matrices can be represented as:\n","$$\\mathbf{X}=\\begin{bmatrix} x_{11} & x_{12} & \\cdots & x_{1n} \\\\ x_{21} & x_{22} & \\cdots & x_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{m1} & x_{m2} & \\cdots & x_{mn} \\\\ \\end{bmatrix}$$\n","<br>\n","\n","Tensors are n-dimenional (n-d) arrays with arbitrary number of axes.\n","\n","Scalars, Vectors and Metrices are `0-d, 1-d and 2-d` tensors respectively. To represent color images we need 3-axes or dimensions (Color channels (red, green and blue), height, and width). Hence color images can be represented using 3-d tensors.\n","\n","Let us look at some of the operations on matrices/tensors.\n"]},{"cell_type":"markdown","metadata":{"id":"ttmni2Qj-0h9"},"source":["### <font color = 'pickle'>**Creating Merices**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0UYqt6pwazC","executionInfo":{"status":"ok","timestamp":1661146308583,"user_tz":300,"elapsed":141,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"a070ef65-d582-4986-f696-39af8a63492e"},"source":["# Creating a matrix using tensor\n","\n","# Initializing with random values\n","A = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","\n","# Creating an identity matrix\n","B = torch.eye(3)\n","\n","# Creating matrix having elements in a range\n","C = torch.arange(0,10).reshape(2, 5)\n","\n","# Viewing all the matrices\n","print(A.view(3, 3))\n","print(B.view(3, 3))\n","print(C.view(2, 5))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])\n","tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])\n","tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"LRLVn7CN-6ji"},"source":["### <font color = 'pickle'>**Accessing Elements**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i5lKm8a0565W","executionInfo":{"status":"ok","timestamp":1661146317531,"user_tz":300,"elapsed":156,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"b166ec90-2331-4901-c565-b6da59245787"},"source":["# Accessing elements using index values\n","\n","# To get an element from matrix A which is in row 2 and column 3 i.e 6, we write A[1][2] as indexing starts from 0.\n","A[1][2]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(6.)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"tBr9ML85_Fw-"},"source":["### <font color = 'pickle'>**Copying metrices (tensors)**\n","We can use clone function to copy tensors : A = B.clone().detach()\n","- clone function is recorded in the computation graph. (Computation Graphs are explained in next notebook).\n","- Gradients propagating to the cloned tensor will propagate to the original tensor.\n","- Use detach() to disconnect the computation graph from the cloned tensor.\n","- see other ways to copy tensor here <br>\n"," https://stackoverflow.com/questions/55266154/pytorch-preferred-way-to-copy-a-tensor\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cerSV3w13uTa","executionInfo":{"status":"ok","timestamp":1661146328943,"user_tz":300,"elapsed":143,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"9a027f8a-b8df-43ea-a926-c943f2ed7ba9"},"source":["# Creating 2 matrices\n","\n","# First matrix\n","A = torch.arange(0, 25).reshape(5, 5)\n","\n","# Second matrix : copy of A\n","B = A.clone()\n","\n","print(A)\n","print(B)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14],\n","        [15, 16, 17, 18, 19],\n","        [20, 21, 22, 23, 24]])\n","tensor([[ 0,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14],\n","        [15, 16, 17, 18, 19],\n","        [20, 21, 22, 23, 24]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"bUzXoOeG_rfX"},"source":["### <font color = 'pickle'>**Operations on Metrices** "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0VFFcFWq5ZJZ","executionInfo":{"status":"ok","timestamp":1661146335789,"user_tz":300,"elapsed":109,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"ecc0ba43-d9d1-455c-89a3-09811cdefc84"},"source":["# Addition of 2 matrices\n","A + B"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  2,  4,  6,  8],\n","        [10, 12, 14, 16, 18],\n","        [20, 22, 24, 26, 28],\n","        [30, 32, 34, 36, 38],\n","        [40, 42, 44, 46, 48]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oycCtJiY5jPM","executionInfo":{"status":"ok","timestamp":1661146338398,"user_tz":300,"elapsed":134,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"c5728509-e601-4662-baf0-5b586aba42c0"},"source":["# Subtraction of 2 matrices\n","A - B"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7aua8kx79WOV","executionInfo":{"status":"ok","timestamp":1661146341529,"user_tz":300,"elapsed":119,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"37114e50-97e5-4883-d72d-489574c4a2b6"},"source":["# Elementwise multiplication of two metrices is called Hadamard product\n","A * B"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  0,   1,   4,   9,  16],\n","        [ 25,  36,  49,  64,  81],\n","        [100, 121, 144, 169, 196],\n","        [225, 256, 289, 324, 361],\n","        [400, 441, 484, 529, 576]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISBiKAus-AKu","executionInfo":{"status":"ok","timestamp":1661146346210,"user_tz":300,"elapsed":129,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"cb196554-979e-4c46-8381-2737e939fcca"},"source":["# Each element of matrix can be aded or multiplied by a scalar (broadcasting)\n","# This operation will not change the shape of a matrix or a Tensor\n","a = 2\n","print(a + A)\n","print()\n","print(a * A)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2,  3,  4,  5,  6],\n","        [ 7,  8,  9, 10, 11],\n","        [12, 13, 14, 15, 16],\n","        [17, 18, 19, 20, 21],\n","        [22, 23, 24, 25, 26]])\n","\n","tensor([[ 0,  2,  4,  6,  8],\n","        [10, 12, 14, 16, 18],\n","        [20, 22, 24, 26, 28],\n","        [30, 32, 34, 36, 38],\n","        [40, 42, 44, 46, 48]])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHAObZ7g5nhj","executionInfo":{"status":"ok","timestamp":1661146349936,"user_tz":300,"elapsed":135,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"4e6d7df8-f2a8-491b-b12d-b907a59fa0e2"},"source":["# Transpose of a matrix : Elements of the rows and columns get interchanged a[i][j] becomes a[j][i]\n","# Transpose is a special case of permute \n","A.T"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  5, 10, 15, 20],\n","        [ 1,  6, 11, 16, 21],\n","        [ 2,  7, 12, 17, 22],\n","        [ 3,  8, 13, 18, 23],\n","        [ 4,  9, 14, 19, 24]])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"d-GLwJNV7eyN"},"source":["<font color = 'pickle'>**Symmetric Matrix : A special type of square matrix which is equal to its transpose.** "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9OPb3EO7eY7","executionInfo":{"status":"ok","timestamp":1661146354843,"user_tz":300,"elapsed":144,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"96f8688b-5f78-4538-f175-d16b46ac6044"},"source":["# Check for symmetric matrix\n","X = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n","\n","X == X.T"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[True, True, True],\n","        [True, True, True],\n","        [True, True, True]])"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"TFamcFCH8Rid"},"source":["Since X is equal to it's transpose, X is a symmetric matrix."]},{"cell_type":"markdown","metadata":{"id":"-eDfPe9dAwNe"},"source":["We can also construct binary tensor using logical statements. Let's take two matrices A and B for example. If A and B are equal at some position, then A == B will be true and if A and B are not equal at some position, then that position will be false."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6iY7aweABhg6","executionInfo":{"status":"ok","timestamp":1661146402749,"user_tz":300,"elapsed":116,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"6aed9490-3424-4bf5-e024-8adf25eb7f08"},"source":["A = torch.zeros(3,3)\n","B = torch.eye(3,3)\n","A == B"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[False,  True,  True],\n","        [ True, False,  True],\n","        [ True,  True, False]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"UeOLV3qj7DTS"},"source":["## <font color = 'pickle'>**Reduction**\n","\n","We can calculate the sum of all elemnets of a vector or a matrix of any shape. This can be done using the ***sum*** function.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RleGFr-96dnx","executionInfo":{"status":"ok","timestamp":1661146413734,"user_tz":300,"elapsed":138,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"e560641c-505b-48d4-b834-6ea845431b24"},"source":["# Creating a vector\n","x = torch.arange(5)\n","print(x)\n","\n","# This will do summation of all the elements of the vector : 0 + 1 + 2 + 3 + 4 = 10\n","print(x.sum())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4])\n","tensor(10)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YC7ffhAU9bnO","executionInfo":{"status":"ok","timestamp":1661146419446,"user_tz":300,"elapsed":120,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"8822bc8f-63ac-449a-e23a-586ed31637d5"},"source":["# Creating a matrix\n","X = torch.arange(0, 10).reshape(2, 5)\n","print(X)\n","\n","# This will do summation of all the elements of the matrix\n","print(X.sum())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n","tensor(45)\n"]}]},{"cell_type":"markdown","metadata":{"id":"IRQX5pa6-r6V"},"source":["We can also calculate the mean or average of all elements in a vector or a matrix by dividing the sum of elements by no. of elements. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WINQaPla-Y3E","executionInfo":{"status":"ok","timestamp":1661146436819,"user_tz":300,"elapsed":126,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"c2eca99a-5a9f-405d-c582-1a8f5906c714"},"source":["# Calculating mean / average\n","\n","# While creating a tensor we can also specify the data type of the elements using parameter \"dtype\"\n","\n","v = torch.arange(5, dtype = float) \n","\n","print(v)\n","\n","# Sum of all elements\n","print(v.sum())\n","\n","# Number of elements\n","print(v.numel())\n","\n","# Mean\n","print(v.sum() / v.numel())\n","print(v.mean())             # Same as above"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 1., 2., 3., 4.], dtype=torch.float64)\n","tensor(10., dtype=torch.float64)\n","5\n","tensor(2., dtype=torch.float64)\n","tensor(2., dtype=torch.float64)\n"]}]},{"cell_type":"markdown","metadata":{"id":"k6Ti1nNrBP4f"},"source":["By default, invoking the sum/mean finction on a tensor will give us a scaler (reduces the tensor along all its axes)\n","\n","We can also calculate sum, along the rows or columns by specifying the value of parameter \"axis\".\n","\n","axis = 0 will calculate sum along the rows while axis = 1 will calculate sum along the columns.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifrBSeGm_P6Z","executionInfo":{"status":"ok","timestamp":1661146448082,"user_tz":300,"elapsed":115,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"bbef65e2-1ec6-4c84-9aa8-44695c269a7a"},"source":["# Creating a matrix A\n","A = torch.arange(0, 15, dtype = float).reshape(5, 3)\n","A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.],\n","        [ 9., 10., 11.],\n","        [12., 13., 14.]], dtype=torch.float64)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MSk9Q9uyQHL7","executionInfo":{"status":"ok","timestamp":1661146505017,"user_tz":300,"elapsed":146,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"d724c294-872e-4633-929f-c9a58f623eb2"},"source":["# Sum of elements row-wise\n","# Since we are taking sum along axis = 0, the input tensor reduces along axis 0\n","# The shape of A was ([5,3])\n","# After invoking sum along axis = 0, the shape reduces to ([3])\n","print(f'Shape before rediction{A.shape}')\n","A.sum(axis = 0), A.sum(axis=0).shape"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape before redictiontorch.Size([5, 3])\n"]},{"output_type":"execute_result","data":{"text/plain":["(tensor([30., 35., 40.], dtype=torch.float64), torch.Size([3]))"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRsJKUOcQG7a","executionInfo":{"status":"ok","timestamp":1661146508689,"user_tz":300,"elapsed":130,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"fd378e96-57dd-4082-c2c3-c4145126ac39"},"source":["# Sum of elements column-wise\n","# Since we are taking sum along axis = 1, the input tensor reduces along axis 1\n","# The shape of A was ([5,3])\n","# After invoking sum along axis = 1, the shape redices to ([5])\n","A.sum(axis = 1), A.sum(axis = 1).shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 3., 12., 21., 30., 39.], dtype=torch.float64), torch.Size([5]))"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"Pt8xjJ8w8sba"},"source":["## <font color = 'pickle'>**Non-Reduction Sum**"]},{"cell_type":"markdown","metadata":{"id":"YEp87iCYDX3h"},"source":["As seem in above examples, invoking sum() or mean() will reduce number of dimensions. We can keep number of axis unchanged by passing argument keepdims = True."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"421spORkPFph","executionInfo":{"status":"error","timestamp":1661146637967,"user_tz":300,"elapsed":447,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"892e8e30-f591-4c9d-a2cb-4d5fafa4663a"},"source":["# however if we do not pass the argument keepdim=true, \n","# the two tensors are not broadcastabel and we will get an error\n","print(A/A.sum(axis=1, keepdim=False))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-41483e6d143d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# however if we do not pass the argument keepdim=true,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# the two tensors are not broadcastabel and we will get an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGZhGYCLIM0S","executionInfo":{"status":"ok","timestamp":1661146773232,"user_tz":300,"elapsed":185,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"986088c3-f265-4da9-e554-a0b4fb57ae4d"},"source":["# When we pass argument keepdim=True, the shape will now be ([5,1]. The output has 2-dimensions\n","# if we do not pass the argument keepdim=True, the shape will be ([5]). The output has one-dimension\n","sum_A_0 = A.sum(axis=1, keepdim=True)\n","print(sum_A_0.shape)\n","print(sum_A_0)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 1])\n","tensor([[ 3.],\n","        [12.],\n","        [21.],\n","        [30.],\n","        [39.]], dtype=torch.float64)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQQoa2RlJn-J","executionInfo":{"status":"ok","timestamp":1661146791244,"user_tz":300,"elapsed":175,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"68f18740-abfa-4f77-a215-7215c6875dbe"},"source":["# Let us now try operation : A/(sum(A, axis=0))\n","print(A/A.sum(axis=1, keepdim=True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.0000, 0.3333, 0.6667],\n","        [0.2500, 0.3333, 0.4167],\n","        [0.2857, 0.3333, 0.3810],\n","        [0.3000, 0.3333, 0.3667],\n","        [0.3077, 0.3333, 0.3590]], dtype=torch.float64)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LexuntQ6KMY","executionInfo":{"status":"ok","timestamp":1661146808782,"user_tz":300,"elapsed":178,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"2b846a8a-1793-482a-c67e-3e8dfca9c4eb"},"source":["A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.],\n","        [ 9., 10., 11.],\n","        [12., 13., 14.]], dtype=torch.float64)"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyENwjkxDSkP","executionInfo":{"status":"ok","timestamp":1661146813806,"user_tz":300,"elapsed":139,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"2886922d-c706-42e0-9223-bdc6ea9dcd87"},"source":["# Cumulative sum of elements along rows\n","A.cumsum(axis = 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.],\n","        [ 3.,  5.,  7.],\n","        [ 9., 12., 15.],\n","        [18., 22., 26.],\n","        [30., 35., 40.]], dtype=torch.float64)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8OYWH9PMAt4g","executionInfo":{"status":"ok","timestamp":1661146828448,"user_tz":300,"elapsed":147,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"738237fa-3c91-49e6-d5b3-7d27724a928c"},"source":["# Cumulative sum of elements along columns\n","A.cumsum(axis = 1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  3.],\n","        [ 3.,  7., 12.],\n","        [ 6., 13., 21.],\n","        [ 9., 19., 30.],\n","        [12., 25., 39.]], dtype=torch.float64)"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"Mhr-NiekRA7Y"},"source":["## <font color = 'pickle'>**Dot Products**\n","\n","Dot product of 2 vectors x and y, represented as (x.T)(y) is given by the summation of product of elements at the same position. \n","\n","If we have 2 vectors x: [1, 2, 3, 4] and y: [1, 1, 2, 1]\n","\n","(x.y) will be 1x1 + 2x1 + 3x2 + 4x1 = 13"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KD4_9ZUxQ2Hp","executionInfo":{"status":"ok","timestamp":1661146832388,"user_tz":300,"elapsed":148,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"14a80eea-e781-4736-c46a-0811d9c0bdb6"},"source":["# Initializing 2 tensors\n","x = torch.Tensor([1, 2, 3, 4])\n","y = torch.Tensor([1, 1, 2, 1])\n","\n","# Performing Dot product\n","torch.dot(x, y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(13.)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NRdtGrvSGCj","executionInfo":{"status":"ok","timestamp":1661146835348,"user_tz":300,"elapsed":151,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"b6fb5baf-d952-42aa-9dfb-af6f74fe556e"},"source":["# Dot Product is equal to sum of products at the same position, thus the expression below will give similar result\n","torch.sum(x * y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(13.)"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"Ys0KKwAFTHpk"},"source":["## <font color = 'pickle'>**Matrix Multiplication**\n","\n","Matrix multiplication is a binary operation on 2 matrices which gives us a matrix which is the product of the 2 matrices. \n","\n","If we are given 2 matrices $A$ of shape $(m * n)$ and $B$ of shape $(q * p)$, we can perform matrix multiplication only when $n = q$ and the resultant product matrix will have shape $(m * p)$. \n","\n","Suppose we are given 2 matrices $A (m * n)$ and $B (n * p)$: \n","\n","$$\\mathbf{A}=\\begin{bmatrix}\n"," a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n"," a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n","\\end{bmatrix},\\quad\n","\\mathbf{B}=\\begin{bmatrix}\n"," b_{11} & b_{12} & \\cdots & b_{1p} \\\\\n"," b_{21} & b_{22} & \\cdots & b_{2p} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," b_{n1} & b_{n2} & \\cdots & b_{np} \\\\\n","\\end{bmatrix}$$\n","\n","Then after performing matrix multiplication, the resultant matrix C = AB will be:\n","\n","$$\\mathbf{C}=\\begin{bmatrix}\n"," c_{11} & c_{12} & \\cdots & c_{1p} \\\\\n"," c_{21} & c_{22} & \\cdots & c_{2p} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," c_{mp} & c_{mp} & \\cdots & c_{mp} \\\\\n","\\end{bmatrix}$$\n","\n","Here, $c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + ... a_{in}b_{b_nj} = \\sum_{k = 1}^n a_{ik}b_{kj}$\n","\n","for, $i = 1,....m$ and $j = 1,...p$\n","\n","\n","Thus, each element of C, $c_{ij}$ is obtained by dot product of $i^{th}$ row of $A$ and $j^{th}$ column of $B$. \n","\n","We can perform matrix multiplication in the following way using PyTorch:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAGYwiI4SykF","executionInfo":{"status":"ok","timestamp":1661146841277,"user_tz":300,"elapsed":117,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"57044e7e-a924-4553-ee44-3dbc1ec003d6"},"source":["# Initializing 2 matrices\n","A = torch.arange(0, 10, dtype=float).reshape(2, 5)\n","B = torch.ones(5, 2, dtype=float)\n","\n","# Matrix-Matrix Multiplication using mm function of PyTorch \n","torch.mm(A, B)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[10., 10.],\n","        [35., 35.]], dtype=torch.float64)"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"VKqv2FqpQt45"},"source":["### <font color = 'pickle'>**Matrix - Vector Multiplication**\n","\n","This a subset of normal matrix multiplication. In this case, one of the two input tensors is a one dimensional tensor.\n","\n","Suppose we have a matrix $A$ of shape $(m * n)$ and a vector $v$ of shape $(n)$:\n","\n","$$\\mathbf{A}=\\begin{bmatrix}\n"," a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n"," a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n","\\end{bmatrix},\\quad\n","\\mathbf{v}=\\begin{bmatrix}\n"," v_{1} \\\\\n"," v_{2} \\\\\n","\\vdots \\\\\n"," v_{n} \\\\\n","\\end{bmatrix}$$\n","\n","In this case, we will consider vector to be of shape $(n * 1)$ and will perform normal matrix multiplication.\n","\n","The resultant matrix $R$ will be of shape $(m * 1)$ given as:\n","\n","$$\\mathbf{R}=\\begin{bmatrix}\n"," a_{11}v_1 + a_{12}v_2 + \\cdots + a_{1n}v_n \\\\\n"," a_{21}v_1 + a_{22}v_2 + \\cdots + a_{2n}v_n\\\\\n","\\vdots \\\\\n"," a_{m1}v_1 + a_{m2}v_2 + \\cdots + a_{mn}v_n\\\\\n","\\end{bmatrix}$$\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4uskrYB8Tx_Z","executionInfo":{"status":"ok","timestamp":1661146845510,"user_tz":300,"elapsed":144,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"d0abc149-5757-488e-85d1-12a2ceeeae7a"},"source":["# Initializing a matrix and a vector\n","A = torch.arange(0, 9).reshape(3, 3)\n","v = torch.arange(3)\n","\n","# Matrix-Vector Multiplication using mv function of PyTorch\n","torch.mv(A, v)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 5, 14, 23])"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"aA-QivRwB6UU"},"source":["## **Norms**\n","\n","Norm is a function on a vector that tells us about the size of the vector.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tWL75N8oEkUu"},"source":["### <font color = 'pickle'>**p - Norm**\n","\n","The most commonly used norms are clubbed under p-norms or (lâ‚š-norms) family, where p is any number greater than or equal to 1.\n","\n","p-Norm of a vector X is given by:\n","\n","$$\\|\\mathbf{x}\\|_p = (x_1^p + x_2^p + x_3^p + \\cdots + x_n^p)^{1/p}$$\n","\n","It could be simplified as: \n","\n","$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$"]},{"cell_type":"markdown","metadata":{"id":"45cjDeTqWznP"},"source":["### <font color = 'pickle'>**$L_1$ norm**\n","\n","$L_1$ norm is also known as the Manhattan Distance as it measures the actual distance between 2 points.\n","\n","$L_1$ norm is given by putting $p = 1$ in p - norm as:\n","\n","$$\\|\\mathbf{x}\\|_1 = {\\sum_{i=1}^n \\left|x_i \\right|}$$\n","\n","If we have vector $v = [2, 3]$, then $L_1$ norm will be:\n","\n","$$\\|\\mathbf{v}\\|_1 = (2^1 + 3^1)^1$$\n","\n","$$\\|\\mathbf{v}\\|_1 = 5$$\n","\n"]},{"cell_type":"code","metadata":{"id":"Ktt_0e3u64-w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661146851149,"user_tz":300,"elapsed":118,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"6ecfebcc-4f03-4162-f819-3ba6c488350c"},"source":["# Initializing a tensor (vector)\n","t = torch.tensor([-3.0, 4.0])\n","\n","# Calculating l1 norm\n","n = torch.norm(t, p=1)\n","print(n)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(7.)\n"]}]},{"cell_type":"markdown","metadata":{"id":"1N3tN-vDWzjg"},"source":["### <font color = 'pickle'>**$L_2$ norm**\n","\n","$L_2$ norm is also known as the Euclidean Distance as it measures the shortest distance between 2 points. \n","\n","$L_2$ norm is given by:\n","\n","$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$\n","\n","\n","If we have vector $v = [2, 3]$, then $L_2$ norm will be:\n","\n","$$\\|\\mathbf{v}\\|_2 = (2^2 + 3^2)^{1/2}$$\n","\n","$$\\|\\mathbf{v}\\|_2 = \\sqrt {13}$$\n","\n","In deep learning, we will usually work with squared $L_2$ norm. \n","\n","We can calculate $L_2$ norm using the norm() function of PyTorch as given below:\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTSzbQMQGbDe","executionInfo":{"status":"ok","timestamp":1661146856790,"user_tz":300,"elapsed":131,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"d98f35d8-d7bb-4eda-823c-bad62c2260b9"},"source":["# Initializing a tensor (matrix)\n","t = torch.tensor([3.0, 4])\n","\n","# Calculating l2 norm\n","n = torch.norm(t)\n","print(n)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(5.)\n"]}]},{"cell_type":"markdown","metadata":{"id":"V7yKFh3z7JJA"},"source":["In deep learning we always try to solve optimization problems:\n","\n","- Maximize the probability assigned to observed data.\n","- Minimize the distance between predicted and actual values.\n","\n","We aim to maximize the distance between similar items and minimize the distance between dissimilar items (euclidean distance using $L_2$ norm). Thus, norms are frequently used in deep learning algorithms."]}]}