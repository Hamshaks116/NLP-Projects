{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gaGkLWaZZv45"},"source":["# <font color = 'pickle'>**Lecture 1.1. Intoduction to PyTorch Tensors**\n"]},{"cell_type":"markdown","metadata":{"id":"IHWosHogaREo"},"source":["# <font color = 'pickle'>**Tensors**\n","\n","- Tensors are the basic building blocks of any deep learning network.\n","\n","- They are used to represent all the different types of data be it images, sound files, text data etc.\n","\n","- Tensors are **order N-matrix**.\n","\n","\n","If N=1, tensor will basically be a **vector**.\n","If N=2, tensor will be a **2-d matrix**.\n","\n","Why Tensors and not NumPy arrays?\n","\n","- NumPy only supports CPU computation.\n","- Tensor class supports automatic differentiation."]},{"cell_type":"markdown","metadata":{"id":"EJREGcIPcIm2"},"source":["**Let us start by importing PyTorch library and understand some of the basic functions on tensors.**"]},{"cell_type":"markdown","metadata":{"id":"sa1p0E6Sduea"},"source":["## <font color = 'pickle'>**Importing PyTorch Library** "]},{"cell_type":"code","metadata":{"id":"chOMYVyUVNof"},"source":["import torch\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HCmCJhfsdZXJ"},"source":["## <font color = 'pickle'>**Declaring a Tensor**\n","\n","*torch.Tensor* is a multi-dimensional matrix containing elements of the same data type. \n","\n","We can create a Tensor of n dimensions using PyTorch in the following way:\n","    \n","    name_of_tensor = torch.Tensor(s1, s2, s3....sn)"]},{"cell_type":"code","metadata":{"id":"eMbcj9zEX2BH"},"source":["t = torch.Tensor(1, 2, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yMuURbgYf4A_"},"source":["Here, we are creating a tensor \"t\" with **3 dimensions** and size 1 x 2 x 4."]},{"cell_type":"markdown","metadata":{"id":"P8QRYASxgXFF"},"source":["## <font color = 'pickle'>**Basic functions on Tensors**"]},{"cell_type":"code","metadata":{"id":"EGgM8Fb_Y1AA"},"source":["# We can get size of each dimension of tensor using size() and shape method\n","print(f'size of tensor using size() method: {t.size()}')\n","print(f'size of tensor using shape attribute: {t.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"moSgralGhQCL"},"source":["# we can pass argument to size() to get size of a particular dimension\n","# for example t.size(2) - will give us size of second dimesnion\n","# indexing in pytorch starts with zero, so zero represents first dimension, one represets second dimension\n","# and so on\n","t.size(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qSOkXeYmg_u7"},"source":["# Printing dimensions of tensor using dim() method\n","t.dim()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEvnaW30lVU1"},"source":["# Printing the values of tensor\n","# We have not initialized our tensor yet, so it will contain random values.\n","print(t)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZ1hXgxDlNUG"},"source":["# Printing number of elements of tensor using numel() method\n","torch.numel(t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ChBqobQdis7T"},"source":["## <font color = 'pickle'>**Initializing a Tensor**"]},{"cell_type":"code","metadata":{"id":"agJbamRzhFh8"},"source":["# Initializing a 1-d vector\n","# initialize tensor using tensor() method\n","vec1 = torch.tensor([1, 2, 5, 7, 8, 9])\n","\n","# Printing size and dimensions of 1-d vector v\n","print(f'Size: {vec1.size()}')\n","print(f'Dimension: {vec1.dim()}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"21GpqSxV0Urm"},"source":["# initialize tensor using Tensor() method \n","vec2 = torch.Tensor([1, 2, 5, 7, 8, 9])\n","\n","# Printing size and dimensions of 1-d vector v\n","print(f'Size: {vec2.size()}')\n","print(f'Dimension: {vec2.dim()}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"psL2qWf4h1p7"},"source":["# Initializing a 2-d matrix\n","vec2d = torch.tensor([[1, 2, 5], [7, 8, 9]])\n","\n","# Printing size and dimensions of 1-d vector v\n","print(f'Size: {vec2d.size()}')\n","print(f'Dimension: {vec2d.dim()}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v_4VmMUwl826"},"source":["## <font color = 'pickle'>**Accessing elements of a Tensor**\n","\n","We can access individual elements of a Tensor using **index values**. Indexing always **starts from 0**.\n","\n","For example if the tensor is: `[10, 12, 31, 34]` \n","\n","Index of 10 is 0, index of 12 is 1 and so on."]},{"cell_type":"code","metadata":{"id":"G6nbzmQxkkIp"},"source":["t1 = torch.tensor([[1, 2, 5], [7, 8, 9]])\n","\n","# Printing all elements\n","print(t1)\n","\n","# Get the first row\n","print(t1[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the first element of the second row\n","print(t1[1][0])"],"metadata":{"id":"03U-6P0JFbse"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDgRsn4wuoVA"},"source":["## <font color = 'pickle'>**Accessing a Sub-Tensor**"]},{"cell_type":"code","metadata":{"id":"CwK4E8muunYb"},"source":["t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","\n","# Specify [from: to : step)  \n","# by default step is 1\n","# here from is inclusive but to is not \n","\n","# Get a subarray [9, 13, 21, 45]\n","print(t1[2:6]) # index 2 (i.e 9) is inclusive but index 6 (i.e. 67) is not and step size is 1\n","\n","# Get a subarray [9, 21]\n","print(t1[2:6:2]) # step size is 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"knosW3KhFZA0"},"source":["# subarrays created using slicing and indexing do not create a copy, \n","# modifying the subarray modifies the original tensor as well\n","\n","t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","t2 = t1[2:6].clone()\n","print('array and subarray before modifying subarray')\n","print(t1)\n","print(t2)\n","\n","# modify subarray\n","\n","t2[0] = 100\n","print('\\narray and subarray after modifying subarray')\n","print(t1)\n","print(t2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6iUsJ8uGhWG"},"source":["As we can see above, modifying subarray, modifes the original array as well"]},{"cell_type":"code","source":["t1 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n","t1"],"metadata":{"id":"qV1DADJLGKcJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the sub array [[6,7], [10,11]]\n","t1[1:3,1:3]"],"metadata":{"id":"0R7kNTmtGmIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xX3tyHVrpN4s"},"source":["## <font color = 'pickle'>**Some commonly used Tensors**"]},{"cell_type":"markdown","metadata":{"id":"wpm1RohNpzKv"},"source":["####<font color = 'pickle'>**1) Tensor containing all zeros/ all ones/ or any value**"]},{"cell_type":"code","metadata":{"id":"O8FzuQrEoB2T"},"source":["# Tensor containing all zeros, size = 10\n","z1 = torch.zeros(5)      \n","\n","# Tensor containing all zeros, size = 2 X 2 X 3\n","z2 = torch.zeros(2 ,2, 3)\n","\n","print(z1)\n","print(z2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JMbGq7ceqba0"},"source":["# Tensor containing all ones, size = 7\n","z1 = torch.ones(7)      \n","\n","# Tensor containing all ones, size = 1 X 2 X 3\n","z2 = torch.ones(1, 2, 3)\n","\n","print(z1)\n","print(z2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVREaBSAZK7w"},"source":["# We can also use torch.full(size, fill_value) to create a tensor filled with any value\n","# Tensor containing all fives, size = 1 X 2 X 3\n","\n","z3 = torch.full(size = (1, 2, 3), fill_value = 5)\n","print(z3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HxOWMc03qszf"},"source":["#### <font color = 'pickle'>**2) Tensor with elements in a particular range**\n","Suppose we need a tensor with values `1, 2, 3, 4.....n. ` \n","\n","We can simply specify the range and tensor will automatically get filled with these values."]},{"cell_type":"code","metadata":{"id":"Ydycx6TMqowD"},"source":["# Creating a tensor with integers from 1 to 5 with space 1: [1, 2, 3, 4, 5]\n","# syntax arange(start, end, step) - create tensor with values in the interval [start, end). \n","# start is inclusive , end is not i.e. start <= values < end\n","tr1 = torch.arange(1, 6) \n","print(tr1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9n03rD_sFdS"},"source":["# Creating a tensor with integers from 0 to 10 with space 2 using \"step\" parameter: [0, 2, 4, 6, 8, 10]\n","tr2 = torch.arange(0, 11, step=2)\n","print(tr2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4TOw7gUZ-He"},"source":["We can also use `torch.linspace()` to generate evenly spaced values between two numbers"]},{"cell_type":"code","metadata":{"id":"-9iboyquHrmW"},"source":["# Generate 10 evenly spaced values between 0 and 1 (both inclusive)\n","t1 = torch.linspace(0, 1, 10)\n","print(t1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sVqmTY7IzBmT"},"source":["#### <font color = 'pickle'>**3) Tensor with elements from probability distribution**\n","\n","We can use the randn function to get elements from standard normal probabilty distribution i.e. normal dustribution with mean = 0 and variance = 1. If we want to select elements from normal ditsribution with different mean and variance then we should use torch.normal"]},{"cell_type":"code","metadata":{"id":"N9dnwRY1zAoO"},"source":["torch.manual_seed(0) # for reproducabilty so that we get same results everytime we run this cell\n","# Sample 500,000 values from standard normal distribution (mean = 0 , variance = 1)\n","t1 = torch.randn(500000) \n","\n","# Sample 500,000 values from normal distribution (mean = 5 , std = 2)\n","t2 = torch.normal(mean = 5, std = 2, size = (500000,))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nty188mkR6Yx"},"source":["print('Mean and std of tensor using torch.randn')\n","print(torch.mean(t1))\n","print(torch.std(t1))\n","\n","print('\\nMean and std of tensor using torch.normal')\n","print(torch.mean(t2))\n","print(torch.std(t2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyiCCPVZVg8H"},"source":["torch.manual_seed(0) # for reproducabilty so that we get same results everytime we run this cell\n","t1 = torch.randn(5, 2) # we sampled 10 values from standard noemal distribution. (5, 2) is the shape.\n","t1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxiimV9UY7zu"},"source":["We can also sample from other distributions like torch.rand, torch.randint etc."]},{"cell_type":"markdown","metadata":{"id":"OIeaFS6SV5lS"},"source":["### <font color = 'pickle'>**4) Empty Tensor**\n","We can create uninitialized  tensors using torch.empty.\n","<br> Note : as seen earlier we can use torch.Tensor(shape) without passing any elements to create uninitialized  tensor. For code clarity, it is better to create uninitialized  tensors using torch.empty"]},{"cell_type":"code","metadata":{"id":"n7Z40rBVY3oA"},"source":["# create empty tensor of shape (2, 4)\n","empty_tensor = torch.empty(2, 4)\n","empty_tensor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UiNZ0HioJdXF"},"source":["### <font color = 'pickle'>**5) Commonly used tensors based on shape of other tensors**"]},{"cell_type":"markdown","metadata":{"id":"iJEmlIsUIuYu"},"source":["We can also use `torch.zeros_like(input)`, `torch.ones_like(input)`, `torch.full_like(input)` and `torch.empty_like(input)` to create tensors based on the shape of other tensors\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"f4J69YwmJz75"},"source":["input_tensor = torch.arange(6).view(2, 3)\n","input_tensor.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"avYjEMStKF5g"},"source":["print(torch.ones_like(input_tensor))\n","print(torch.zeros_like(input_tensor))\n","print(torch.full_like(input_tensor, 5))\n","print(torch.empty_like(input_tensor))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsMg-Jujsm2W"},"source":["###<font color = 'pickle'> **6) Identity Matrix**\n","\n","Identity matrix is a matrix which has 1's along the diagnal and zeros everywhere else."]},{"cell_type":"code","metadata":{"id":"mM4w1jmDsOwR"},"source":["# Identity matrix of size 3\n","id_matrix = torch.eye(3)\n","print(id_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PqCSmvwtBp4"},"source":["# Identity matrix of size 5\n","id_matrix = torch.eye(5)\n","print(id_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NsU8FU-9s3kl"},"source":["## <font color = 'pickle'>**Changing the shape of tensors**"]},{"cell_type":"markdown","metadata":{"id":"4Dw-lJ1Lu3fT"},"source":["### <font color = 'pickle'>**1) Reshape**\n","\n","If we want to change the shape of our tensor, without affecting the elements present, we can use the ***reshape*** function."]},{"cell_type":"code","metadata":{"id":"SlxcKezfxMVK"},"source":["# Initializing a tensor with 10 elements from 0 to 9\n","t = torch.arange(10)\n","print(t)\n","\n","# Changing the shape of tensor t from 1x10 to 2x5\n","tr = t.reshape(2, 5)\n","print(tr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"00BapNUKxlMQ"},"source":["If we have to specify just 1 dimension in reshape function and want the function to calculate the second dimension itself, we can write `-1` in place of second dimension. \n","\n","For 2 rows, we will write `reshape(2,-1)`\n","\n","For 5 columns, we will write `reshape(-1,5)`. "]},{"cell_type":"code","metadata":{"id":"9CYpAZaAxkxc"},"source":["# Changing the shape of tensor t from 1 row to 2 rows\n","tr1 = t.reshape(2, -1)\n","print(tr1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6qaekDtPp9u"},"source":["# Changing the shape of tensor t from 10 columns to 5 columns\n","tr2 = t.reshape(-1, 5)\n","print(tr2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AWCEvFyfu7Xl"},"source":["### <font color = 'pickle'>**2) View**\n","\n","We can allow a tensor to be a view of an existing tensor. It performs the same operation as reshape. The only difference is that View will not create a copy and will allow us to perform fast and memory efficient computations whereas reshape may or may not share the same memory. There's a good discussion of the differences [here](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch).\n","\n","Line from above link \" *Another difference is that reshape() can operate on both contiguous and non-contiguous tensor while view() can only operate on contiguous tensor* \"\n","\n","[Definition of contiguous](https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092)\n"]},{"cell_type":"code","metadata":{"id":"WVrQF7xw25-f"},"source":["# Initializing a tensor with 10 elements from 0 to 9\n","t = torch.arange(10)\n","print(t)\n","# Changing the shape of tensor t from 1x10 to 2x5\n","t.view(2, 5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WOZJuWP84T7N"},"source":["Views can reflect changes from the base tensor."]},{"cell_type":"code","metadata":{"id":"Yfl2ob_w3X88"},"source":["# Create a view of tensor t\n","tr = t.view(2, 5)\n","\n","# Before change in base tensor\n","print(f'before changing the base tensor\\n{tr}')\n","\n","# Modifying element of base tensor\n","t[0] = 67\n","\n","# After change in base tensor\n","print(f'\\nafter changing the base tensor\\n {tr}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQUVywTktsOf"},"source":["# we can use -1 with view as well. \n","t = torch.rand((4, 5))\n","t1 = t.view(2, -1)\n","print(t1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvbN4qgNQFEs"},"source":["# we can also flatten the tensor (convert the tensor to one dimensional tensor) by using view(-1)\n","# this gives the same result as method flatten()\n","t2 = t.view(-1)\n","t3 = t.flatten()\n","print(t2.shape)\n","print(t3.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCacRHTqv-SN"},"source":["###<font color = 'pickle'> **3) Adding and removing dimensions of size 1**\n","- Insert a dimension of size 1 at a specific location (location specified by dim) using `torch.unsqueeze(dim)`\n","- Remove a dimension of size 1 at a specific location (location specified by dim) using `torch.squeeze(dim)`\n","- Remove all dimensions of size 1 using `torch.squeeze()`\n","- Insert dimenion of size 1 using `None `keyword\n","- Remove dimenion of size 1 using `0 `keyword"]},{"cell_type":"code","metadata":{"id":"bjzYf6InwH6_"},"source":["# Initialize an tensor\n","t1 = torch.ones(2, 2)\n","print(t1)\n","t1.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0NYoZm0wqRF"},"source":["# add dimension of size 1 in the beginning using unsqueeze method and argument dim = 0\n","t1 = t1.unsqueeze(dim=0)\n","print(t1)\n","t1.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2F1MRj3xBYX"},"source":["# add dimesnion of size 1 at the end usin unsqueeze method and dim = 3\n","t1 = t1.unsqueeze(dim=3)\n","print(t1)\n","print(t1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_9kJKcjxffh"},"source":["# We can add new dimesnion at any place\n","t1 = torch.arange(20).view(2, 2, 5)\n","print(t1.shape)\n","t1 = t1.unsqueeze(dim=1)\n","print(t1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"clT8xFNkx_GL"},"source":["# we can also use None keyword to add dimension of size 1 at multiple locations\n","t1 = t1[:, :, :, None, :, None]\n","print(t1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVJMCxpPYgxP"},"source":["# Remove a dimension of size 1 at a specific location using torch.squeeze(dim)\n","t1 = t1.squeeze(dim=1)\n","print(t1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NedAKkzJYnhU"},"source":["# Remove a dimension of size 1 at a specific location using 0 keyword\n","t1 = t1[:, :, 0]\n","print(t1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r7fHrZ-TY2qN"},"source":["# Removing all dimensions of size 1 using torch.squeeze()\n","t1 = t1.squeeze()\n","print(t1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SewxjuW7O6Y"},"source":["### <font color = 'pickle'>**4) Adopting shape of other tensors**\n","We can use view_as(input) to adopt shape of other tensors"]},{"cell_type":"code","metadata":{"id":"rkxKGSH57eBt"},"source":["a = torch.arange(10).view(2, 5)\n","# create a tensor b filled with ones (10 elements) and has same shape as b\n","b = torch.ones(10).view_as(a)\n","print(a.shape)\n","print(b.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMtrejzioBMJ"},"source":["### <font color = 'pickle'>**5) Permute**\n","\n","Permute function rearranges the original tensor according to the desired ordering and returns a new multidimensional rotated tensor. \n","\n","The size of the returned tensor remains the same as that of the original.\n","\n","Let us consider an example:\n","\n","If the size of a tensor is (2, 3, 4),\n","\n","- First size is 2\n","- Second size is 3\n","- Third size is 4\n","\n","Now, in case of permute we will just change the ordering of the sizes. Thus if we write permute(0, 2, 1) the new tensor will have:\n","\n","- First size is 2 (1st size of previous)\n","- Second size is 4 (3rd size of previous)\n","- Third size is 3 (2nd size of previous)\n","\n","Pytorch's function permute() only permutes or in other words shuffles the order of the axes of a tensor whereas view() reshapes the tensor by reducing/expanding the size of each dimension.\n"]},{"cell_type":"code","metadata":{"id":"1r4bB2APt7oR"},"source":["# Initilaize a tensor and print it's size and elements\n","torch.manual_seed(0)\n","t1 = torch.randint(0, 10, size =(2, 4))\n","print(t1.size())\n","print(t1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1.storage()"],"metadata":{"id":"33olxUBS7dcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1.is_contiguous()"],"metadata":{"id":"2FbSfoYX7qw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1.stride()"],"metadata":{"id":"hW-TQkXr7yKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OC0qXNZNVz3_"},"source":["# Permute the tensor and print it's size and elements \n","t1_p = t1.permute(1, 0)\n","print(t1_p.size())\n","print()\n","print(t1_p)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1_p.storage()"],"metadata":{"id":"JmcbvT6t8E3h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1_p.stride()"],"metadata":{"id":"aoDcYejx8VkU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1_p.is_contiguous()"],"metadata":{"id":"QWm1K3dq8YFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1_p.view(2, 4)"],"metadata":{"id":"D4xvyGKg8t0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1_p.reshape(2, 4)"],"metadata":{"id":"EZFobTRY86RF"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBaHNUYro71U"},"source":["# Initilaize a tensor and print it's size and elements\n","torch.manual_seed(0)\n","t2 = torch.rand(2, 3, 4)\n","print(t2.size())\n","print(f'\\n{t2}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wCufxBysSUta"},"source":["# Permute the tensor and print it's size and elements - use permute (0, 2, 1)\n","t2_p = t2.permute(0, 2, 1)\n","print(t2_p.size())\n","print(f'\\n{t2_p}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDbnALFR2ut_"},"source":["# difference between permute and view\n","x = torch.arange(3*2).view(2, 3)\n","print(x)\n","# create a view (3, 2)\n","print(x.view(3, 2))\n","# permute axis(1, 0)\n","print(x.permute(1, 0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f-HexBDq4PIi"},"source":["Question and answer taken from following reference: <br>\n","https://discuss.pytorch.org/t/different-between-permute-transpose-view-which-should-i-use/32916\n","\n","- (1) If I have a feature size of BxCxHxW, I want to reshape it to BxCxHW. Which one is a good option?\n","- (2) If I have a feature size of BxCxHxW, I want to change it to BxCxWxH . Which one is a good option?\n","- (3) If I have a feature size of BxCxH, I want to change it to BxCxHx1 . Which one is a good option?\n","\n","Solution:\n","- permute changes the order of dimensions aka axes, so 2 would be a use case. Transpose is a special case of permute, use it with 2d tensors.\n","- view can combine and split axes, so 1 and 3 can use view,\n","- note that view can fail for noncontiguous layouts (e.g. crop a picture using indexing), in these cases reshape will do the right thing,\n","- for adding dimensions of size 1 (case 3), there also are unsqueeze and indexing with None.\n"]},{"cell_type":"markdown","metadata":{"id":"nhLHAqPLix2O"},"source":["## <font color = 'pickle'>**Operations on tensors of same size**\n","We can call element-wise operations on any two tensors of the same shape."]},{"cell_type":"code","metadata":{"id":"fXtbVUp8IAUZ"},"source":["x = torch.tensor([1.0, 2, 4, 8])\n","y = torch.tensor([2, 2, 2, 2])\n","x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"khxj8DIV5Eic"},"source":["## <font color = 'pickle'>**Broadcasting - Operations on tensors of different  size**\n","\n","Broadcasting describes how a tensor has to be treated during arithematic operation. If we have tensors of different sizes, we can broadcast the smaller array across the larger one so that they can have comaptible shapes.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H9Vet8dAPu5O"},"source":["### <font color = 'pickle'>**1) Understanding how broadcasting works**\n","\n","* The following image describes how a tensor of 2 dimensional tensor will be added to a 1 dimensional tensor \n","<img src=\"https://drive.google.com/uc?export=view&id=1QG2GO1owGpyXbcugJFVFGb4o_buV4s3j\" width=\"500\"/>"]},{"cell_type":"code","metadata":{"id":"bR4BtZeAPu5Q"},"source":["# create tensor\n","t2 = torch.tensor([[12, 16, 14], [13, 17, 13], [14, 18, 12]])\n","t1 = torch.tensor([1, 2, 3])\n","print(t2.shape)\n","print(t1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BN7SCwtQPu5R"},"source":["t1 has one dimension and t2 has 2- dimensions. Pytorch will first prepend 1 to the dimension of t1 so that it also has same number of dimensions."]},{"cell_type":"code","metadata":{"id":"aC7ihRDUPu5S"},"source":["# we can use None keyword to add redundant dimension\n","t1_mod = t1[None,] \n","print(t1_mod)\n","print(t1_mod.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJ93wIdaPu5S"},"source":["Now it will stretch the tensor with smaller dimension so that it has the same shape as the tensor with higher dimensions."]},{"cell_type":"code","metadata":{"id":"OPPAl3o2Pu5T"},"source":["# here we are stretching the tensor along first dimension\n","# we will repeat the same row thrice to generate tensor of size (3, 3)\n","t1_mod_rep = t1_mod.repeat(3, 1) # The number of times to repeat this tensor along each dimension\n","t1_mod_rep"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"se8ZvnApPu5U"},"source":["print(t1_mod_rep + t2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-H5uc89Pu5V"},"source":["# we can check that it gives us the same result if we simply add t1 and t2\n","# so broadcasting is an efficient way of performing operations on tensors of unequal sizes\n","print(t1 + t2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zyhE_dEUQwLk"},"source":["### <font color = 'pickle'>**2) Rules for Broadcasting**</font>\n","Broadcasting can only happen if the two tensors are broadcastable. Conditions for broadcasting:\n","\n","- Each tensor has at least one dimension.\n","\n","- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n","\n","Examples:\n","\n","1. `t1(5, 8, 10) t2(5, 8, 10)`\n","Same size -> Broadcasting possible.\n","2. `t1((0,)) t2(5, 8, 10)`\n","t1 doesn't have atleast one dimension -> Broadcasting not possible. \n","3. `t1(5, 8, 10, 1) t2(8, 1, 1)` Broadcasting possible. Reasons:\n","  - 1st trailing position : both have size 1\n","  - 2nd trailing position : t2 has size 1\n","  - 3rd trailing position : both have size 8\n","  - 4th training position: t2 size doesn't exist but t2 has atleast 1 dimension."]},{"cell_type":"code","metadata":{"id":"x1BuDjiW4MNx"},"source":["# Broadcasting\n","t1 = torch.empty(5, 8, 10, 1)\n","t2 = torch.empty(8, 1, 1)\n","(t1 + t2).size()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjZwjy9d8wbD"},"source":["The dimensions after broadcasting will be:\n","\n","- If the number of dimensions are\n"," not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n","\n","- Then, for each dimension size, the resulting dimension size is the max of the sizes along that dimension."]},{"cell_type":"code","metadata":{"id":"AqqPPTU78v83"},"source":["# Another example for broadcasting\n","t1 = torch.empty(1)\n","t2 = torch.empty(3, 1, 7)\n","(t1 + t2).size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02HFhB1S9QfX"},"source":["# Example where broadcasting is not possible\n","t1 = torch.empty(5, 8, 10, 1)\n","t2 = torch.empty(   3, 1, 1)\n","(t1 + t2).size()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wru9LKgg9f5G"},"source":["Here, at third trailing position sizes are not equal and none of them is 1, thus broadcasting is not possible."]},{"cell_type":"markdown","metadata":{"id":"Y6zZfHhsu92j"},"source":["## <font color = 'pickle'>**Conversion to other Python Objects**"]},{"cell_type":"code","metadata":{"id":"bHmHIXZB-Aw7"},"source":["# Initializing a tensor\n","t = torch.arange(10)\n","\n","# Converting tensor t to numpy array using numpy() mehod\n","arr = t.numpy()\n","\n","# Converting numpy array to tensor T using tensor() method\n","T = torch.tensor(arr)\n","\n","# Printing data type of arr and T\n","type(arr), type(T)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K8ot3be10F1e"},"source":["We can also use torch.from_numpy() and torch.as_tensor() to convert numpy array to PyTorch Tensor. However, with these methods, the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.Tensor() function always makes a copy."]},{"cell_type":"code","metadata":{"id":"ZxfeLiNH1dB4"},"source":["my_ndarray = np.arange(10)\n","t_from_numpy = torch.from_numpy(my_ndarray)\n","t_as_tensor = torch.as_tensor(my_ndarray)\n","t_Tensor = torch.tensor(my_ndarray)\n","\n","print(f\"tensor craeted using torch.from_numpy before changing np array: {t_from_numpy}\")\n","print(f\"tensor craeted using torch.as_tensor before changing np array : {t_as_tensor}\")\n","print(f\"tensor craeted using torch.Tensor before changing np array    : {t_Tensor}\")\n","\n","# change numpy array\n","my_ndarray[2] = 1000\n","\n","print()\n","print(f\"tensor craeted using torch.from_numpy after changing np array: {t_from_numpy}\")\n","print(f\"tensor craeted using torch.as_tensor after changing np array : {t_as_tensor}\")\n","print(f\"tensor craeted using torch.Tensor after changing np array    : {t_Tensor}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-ti5Hum-kCF"},"source":["# Initializing a size-1 tensor\n","t = torch.tensor([10.5])\n","\n","# Printing tensor\n","print(t)\n","\n","# Accessing element of tensor using item function\n","# item returns the value of the tensor as python number\n","# works only for tensors with single element\n","\n","print(t.item())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pmZk-J0TzwE0"},"source":["## <font color = 'pickle'>**Changing datatype of Tensors**\n","When creating tensor we can pass the dtype as an argument. We can also change the datatype of tensors using to() and type() mehods. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"]},{"cell_type":"code","metadata":{"id":"PoYt1PzhzuQw"},"source":["x = torch.tensor([8, 9, -3], dtype=torch.int)\n","\n","# we can use type() method or to() method to change the datatype\n","print(f'Old: {x.dtype}')\n","\n","# change the datatype to int64 using type() method\n","x = x.type(dtype=torch.int64)\n","print(f'New: {x.dtype}')\n","\n","# change the datatype to int32 using type() method\n","x= x.to(dtype=torch.int32)\n","print(f'Newer: {x.dtype}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wMkz0iL56Va7"},"source":["## <font color = 'pickle'>**Concatenating Tensors**\n","\n","We can use `torch.cat((tensors_to_concatenate), dim)` to concatenate tensors.\n","\n","The tensors must have the same shape (except in the concatenating dimension)."]},{"cell_type":"code","metadata":{"id":"TwWw4dEphsw_"},"source":["# we can use torch\n","x1 = torch.randint(low=0, high=10, size = (2, 5))\n","x2 = torch.ones(4, 5)\n","x3 =  torch.zeros(2, 3)\n","\n","# The tensors must have the same shape (except in the concatenating dimension)\n","# x1 and x2 have the same shape except for dim = 0, hence we can conactenate these along dim = 0\n","# x1 and x3 have the same shape except for dim = 1, hence we can conactenate these along dim = 1\n","# we cannot concatenate x2 and x3 along any dimension\n","\n","x1_x2 = torch.cat((x1, x2), dim = 0) \n","x1_x3 = torch.cat((x1, x3), dim = 1)\n","print(f'shape of x1_x2 is {x1_x2.shape}')\n","print(f'shape of x2_x3 is {x1_x3.shape}')\n","print(f'\\nx1_x2\\n:{x1_x2}')\n","print(f'\\nx1_x3\\n:{x1_x3}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l1ugpAlJ6Lks"},"source":["## <font color = 'pickle'>**Saving Memory - inplace operations**"]},{"cell_type":"markdown","metadata":{"id":"Yhk6tvq_6THn"},"source":["In-place operation are operations that change the content of a given Tensor without making a copy. \n","\n","Operations that have a `_` suffix are in-place. For example: `.add_()`. Operations like += or *= are also inplace operations.\n","\n","We can also perform in-place opaeration usng the notation `Z[:] = <expression>`.\n","\n","As in-place operations do not make a copy, they can save memory. However, we need to use them carefully. They can be problematic when computing derivatives because of an immediate loss of history. We will learn about derivatives and computation graphs in coming lectures."]},{"cell_type":"markdown","metadata":{"id":"X-nm1NrfbvIU"},"source":["### <font color = 'pickle'>**1) Checking gpu**"]},{"cell_type":"code","metadata":{"id":"1TFzHz0Thbjt"},"source":["# check if gpu is availaible\n","device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_drUk69u5ky"},"source":["# create a tensor\n","X = torch.tensor([1, 2, 3, 4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qiYTr-LTvAeT"},"source":["# check the device attribute of the tensor\n","X.device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTU-niOavIgd"},"source":["# move the tensor to gpu\n","X.to(device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TahnwIQOvWZV"},"source":["# it is more efficient to create the tensor on gpu directly\n","Y = torch.tensor([1, 2, 3], device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YWQv_dEwviH7"},"source":["# check the device attribute of the tensor\n","Y.device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mJyoahoFNVtP"},"source":["### <font color = 'pickle'>**2) Memory allocation of in-place operations**"]},{"cell_type":"code","metadata":{"id":"pqdfZVuhw5PG"},"source":["# create tensor\n","t1 = torch.randn(10000, 10000, device = 'cpu')\n","\n","# move tensor to gpu\n","t1 = t1.to(device)\n","print(t1.device)\n","\n","# we can use id() function to get memory location of tensor\n","print(f'initial memory location of tensor t1 is : {id(t1)}')\n","\n","x = t1\n","print(f'initial memory location of x is : {id(x)}')\n","\n","# Waits for everything to finish running\n","torch.cuda.synchronize()\n","\n","# initial memory allocated\n","start_memory = torch.cuda.memory_allocated()\n","\n","# inplace operation\n","# t1 = t1 + 0.1\n","# t1.add(0.1)\n","t1 +=  0.1\n","t1.add_(0.1)\n","# since the operation was inplace when we update t1 it will update x as well\n","print(x == t1)\n","\n","print(f'final memory location of tensor t1 is: {id(t1)}')\n","print(f'final location of x is : {id(x)}')\n","\n","# totall memory allocated after function call\n","end_memory = torch.cuda.memory_allocated()\n","\n","# memory allocated because of function call\n","memory_allocated = end_memory - start_memory\n","print(memory_allocated/1024**2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ti7xX-TSLp_3"},"source":["From the above example wecan see that both x and t1 has same memory location. When we ue in-place operation on t1, it also updates x"]},{"cell_type":"markdown","metadata":{"id":"yyQSz0iiNsgw"},"source":["### <font color = 'pickle'>**3) Memory allocation of out-of-place operations**"]},{"cell_type":"code","metadata":{"id":"d3NPUZxTxcmL"},"source":["# create tensor\n","t2 = torch.randn(10000, 10000, device = 'cpu')\n","\n","# move tensor to gpu\n","t2 = t2.to(device)\n","print(t2.device)\n","\n","# we can use id() function to get memory location of tensor\n","print(f'initial memory location of tensor t2 {id(t2)}')\n","\n","y = t2\n","print(f'final memory location of y is : {id(y)}')\n","\n","# Waits for everything to finish running\n","torch.cuda.synchronize()\n","\n","# initial memory allocated\n","start_memory = torch.cuda.memory_allocated()\n","\n","# out-place opertaions\n","t2 = t2 + 0.1\n","\n","# since the operation was not inplace when we update t2 it will not update y\n","print(y == t2)\n","\n","# we can use id() function to get memory location of tensor\n","print(f'final memory location of tensor t2 {id(t2)}')\n","print(f'final memory location of y is : {id(y)}')\n","\n","# totall memory allocated after function call\n","end_memory = torch.cuda.memory_allocated()\n","\n","# memory allocated because of function call\n","memory_allocated = end_memory - start_memory\n","print(memory_allocated/1024**2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kli0fOuMGCM"},"source":["From the above example we can see that initially both y and t2 has same memory location. After running t2 = t2 + 0.1, we will find that id(t2) points to a different location. That is because Python first evaluates t2 + 0.1, allocating new memory for the result and then makes t2 point to this new location in memory. Since we have not done in-place operation, updating t2 does not effect y. y still points to the same memory location."]},{"cell_type":"markdown","metadata":{"id":"24OjlU8RnSie"},"source":["## <font color = 'pickle'>**Masks using binary tensors** "]},{"cell_type":"code","metadata":{"id":"CxyRZVxGPgt_"},"source":["# create a tensor which has probailities of events\n","prob = torch.tensor([0.7, 0.4, 0.6, 0.2, 0.8, 0.1])\n","\n","# Binary tensors\n","print(prob > 0.5)\n","print(prob <= 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwpKs1DRQd7J"},"source":["# create output tensor where output = 1 if prob >0.5 and 0 otherwise\n","# craete an empty output tensor of same shape as prob\n","output = torch.empty_like(prob)\n","\n","# update output tensor using the binary mask\n","output[prob > 0.5] = 1\n","output[prob <= 0.5] = 0\n","print(output)"],"execution_count":null,"outputs":[]}]}